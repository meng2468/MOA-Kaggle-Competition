{\rtf1\ansi\ansicpg1252\cocoartf2513
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fnil\fcharset0 Menlo-Regular;\f1\fnil\fcharset134 PingFangSC-Regular;\f2\fnil\fcharset0 Menlo-Bold;
}
{\colortbl;\red255\green255\blue255;\red255\green255\blue255;\red30\green31\blue41;\red107\green117\blue255;
\red63\green172\blue45;\red103\green255\blue155;\red70\green158\blue255;\red218\green120\blue42;\red252\green121\blue114;
\red93\green255\blue135;\red151\green247\blue255;\red219\green175\blue255;\red196\green60\blue43;\red253\green141\blue130;
}
{\*\expandedcolortbl;;\csgenericrgb\c100000\c100000\c100000;\csgenericrgb\c11766\c12215\c15978;\csgenericrgb\c41961\c45882\c100000;
\cssrgb\c29431\c71398\c23139;\cssrgb\c45330\c100000\c67088;\cssrgb\c32974\c69190\c100000;\cssrgb\c88897\c54535\c21258;\cssrgb\c100000\c55928\c52219;
\cssrgb\c40970\c100000\c60078;\cssrgb\c64585\c97114\c100000;\cssrgb\c88837\c75388\c100000;\cssrgb\c81904\c31857\c22026;\cssrgb\c100000\c63189\c58174;
}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f0\fs24 \cf2 \cb3 \CocoaLigature0 Last login: Wed Nov  4 19:52:26 on ttys001\
baconbaker@MacBook-Pro-2 ~ % cd Downloads \
baconbaker@MacBook-Pro-2 Downloads % ls\
0.AIOpsBooks\
0246 
\f1 \'c3\'cf\'cf\'e9\'c8\'f0
\f0 .pdf\
03-1.mp3\
03-2.mp3\
03-3.mp3\
1-fc-schweinfurt-gegen-fc-schalke-04-heute-live-im-tv-und-livestream.html\
10045.biHppSwIwf37qVdL.answer0\
10207.vRuPwBqYCOuj637g.answer0\
10527.htm\
117097.epub\
1200px-Octicons-mark-github.svg.png\
13720_18513_bundle_archive.zip\
2-SVM-final.pdf\
2006990067_ZY_16000685432845a53560b8-ea5b-4b67-885b-ecd0e224eac1.pdf\
2006990067_ZY_16006730824742996942539-da3b-4766-a164-a6047c6aaaad.pdf\
2020-11-05 10-04-51.mp4\
2020-w2.2-class.pdf\
2020-w5-1-recurrence.pdf\
2020-w5-2.pdf\
20200717110544_TK-Angaben zur Untersuchung.pdf\
2020280441.pdf\
2020_W3.2_pigeonHole.pdf\
2020_linearP_II.ppt\
2020_w4-2Recurrnece_1-2.pdf\
2020_w4-2Recurrnece_1.pdf\
2020\'93
\f1 \'ba\'ba\'d3\'ef\'c7\'c5
\f0 \'94
\f1 \'c8\'ab\'c7\'f2\'cd\'e2\'b9\'fa\'c8\'cb\'ba\'ba\'d3\'ef\'b4\'f3\'bb\'e1
\f0  
\f1 \'d6\'d0\'ce\'c4\'d3\'b0\'ca\'d3\'d7\'f7\'c6\'b7\'d4\'c6\'c5\'e4\'d2\'f4\'b1\'c8\'c8\'fc\'b1\'a8\'c3\'fb\'b1\'ed
\f0  2.docx\
2020\'93
\f1 \'ba\'ba\'d3\'ef\'c7\'c5
\f0 \'94
\f1 \'c8\'ab\'c7\'f2\'cd\'e2\'b9\'fa\'c8\'cb\'ba\'ba\'d3\'ef\'b4\'f3\'bb\'e1
\f0  
\f1 \'d6\'d0\'ce\'c4\'d3\'b0\'ca\'d3\'d7\'f7\'c6\'b7\'d4\'c6\'c5\'e4\'d2\'f4\'b1\'c8\'c8\'fc\'b1\'a8\'c3\'fb\'b1\'ed
\f0 .docx\
3-sized-db-generation\
3rd_textbook_849206991.pdf\
4 a.rar\
4 b.rar\
4-Boosting-annotated2.pdf\
4-Boosting.pdf\
5-deep learning-final.pdf\
6-Clustering.pdf\
7097-Article Text-10326-1-10-20200526.pdf\
90.Day.Fiance.S07E01.720p.WEBRip.x264-KOMPOST[rartv]-[rarbg.to].torrent\
90.Day.Fiance.S07E01.I.Want.to.Kiss.You.1080p.HULU.WEBRip.AAC2.0.H264-NTb[rartv]-[rarbg.to].torrent\
90.Day.Fiance.S07E02.WEBRip.x264-ION10-[rarbg.to].torrent\
90.Day.Fiance.S07E03.What.Am.I.Worth.to.You.1080p.HULU.WEBRip.AAC2.0.H264-NTb[rartv]-[rarbg.to].torrent\
9587.FgfhQXuMuhslUFT1.answer0\
9964.Oo5aUNjWW2y8KN8Y.answer0\
ANM-data\
ANM-data.zip\
ANMproject\
Additional_Tools_for_Xcode_11.dmg\
Anaconda3-2020.07-MacOSX-x86_64.sh\
Application_Form.pdf\
ApplyForm_DUS2200713AL5453107.pdf\
BSc_Certificate_Transcript_compressed.pdf\
Bad.Education.2019.1080p.WEB.H264-SECRECY-[rarbg.to].torrent\
Bad.Education.2019.1080p.WEB.H264-SECRECY.mkv.download\
Billions.S01.1080p.AMZN.WEBRip.DD5.1.x264-NTb[rartv]-[rarbg.to].torrent\
Black.Dynamite.2009.1080p.BluRay.H264.AAC-RARBG-[rarbg.to].torrent\
Blockly.mp4\
BlueStacksInstaller_4.230.10.2820_8fb8c436e99ef2569158b215ac8ecd7a_QW1vbmcgVXM=.dmg\
Borat.Subsequent.Moviefilm.2020.1080p.WEB.H264-NAISU-[rarbg.to].torrent\
Boya Chinese_ Advanced level I 
\f1 \'b2\'a9\'d1\'c5\'ba\'ba\'d3\'ef
\f0 _ 
\f1 \'b8\'df\'bc\'b6
\f0  
\f1 \'b7\'c9\'cf\'e8\'c6\'aa
\f0  I.  ( PDFDrive.com ).pdf\
Boya Chinese_ Advanced level III
\f1 \'b2\'a9\'d1\'c5\'ba\'ba\'d3\'ef
\f0 _ 
\f1 \'b8\'df\'bc\'b6
\f0  
\f1 \'b7\'c9\'cf\'e8\'c6\'aa
\f0  III.  ( PDFDrive.com ).pdf\
Brave.New.World.US.S01.1080p.WEB.H264-GHOSTS[rartv]-[rarbg.to].torrent\
CA20-midterm-online-exam-guide-v4.docx\
Cao2016_Article_ARobustDataScalingAlgorithmToI.pdf\
Capone.2020.1080p.AMZN.WEBRip.DDP5.1.x264-NTG-[rarbg.to].torrent\
Chatbot.mp4\
CourseOverview(EN).pdf\
Crouching.Tiger.Hidden.Dragon.2000.REMASTERED.CHINESE.1080p.BluRay.H264.AAC-VXT-[rarbg.to].torrent\
DDBS-Design
\f1 \'a3\'a8
\f0 EN
\f1 \'a3\'a9
\f0 .pdf\
DDBS-Introduction(EN).pdf\
DDBS-Tentative-Schedule
\f1 \'a3\'a8
\f0 2020-9
\f1 \'a3\'a9
\f0 .pdf\
DUS2200713AL5453107.qztp\
DataScienceForBusiness.pdf\
Deep Learning - Goodfellow, Bengio.epub\
Deep Learning - Goodfellow, Bengio.mobi\
Desolate Era - I Eat Tomatoes.epub\
Docker.dmg.download\
Does_label_smoothing_mitigate_label_noise.pdf\
Dungeon Defense - Volume 04.epub\
Dungeon Defense - Volume 04.epub.download\
Dungeon Defense - Volume 5.epub\
Ellen 
\f1 \'d7\'f7\'d2\'b5
\f0 .docx\
Enter.The.Void.2009.1080p.BluRay.H264.AAC-RARBG-[rarbg.to].torrent\
Eternal.Love.S01.CHINESE.720p.NF.WEBRip.DDP2.0.x264-ExREN[rartv]-[rarbg.to].torrent\
Eternal.Love.S01.CHINESE.WEBRip.x264-ION10-[rarbg.to].torrent\
Eurovision.Song.Contest.The.Story.of.Fire.Saga.2020.1080p.WEB.H264-SECRECY-[rarbg.to].torrent\
Fehlende Meldung zur Sozialversicherung.pdf\
FileZilla_3.50.0_macosx-x86.app.tar.bz2\
First.Cow.2019.1080p.WEB-DL.DD5.1.H264-FGT-[rarbg.to].torrent\
Government Operations in China
\f1 \'bd\'cc\'d1\'a7\'b4\'f3\'b8\'d9
\f0 .doc\
Greyhound.2020.1080p.ATVP.WEB-DL.DDP5.1.Atmos.x264-MZABI-[rarbg.to].torrent\
HSK 1-6 2012.apkg\
HSK workbook 5
\f1 \'c9\'cf
\f0 .pdf\
HSK-4-Full-Vocabulary-.xlsx\
HSK1
\f1 \'bf\'bc\'ca\'d4\'b4\'f3\'b8\'d9
\f0 .pdf\
HSK2
\f1 \'bf\'bc\'ca\'d4\'b4\'f3\'b8\'d9
\f0 .pdf\
HSK3
\f1 \'bf\'bc\'ca\'d4\'b4\'f3\'b8\'d9
\f0 .pdf\
HSK4 WB a.pdf\
HSK4 WB b.pdf\
HSK4 
\f1 \'c1\'b7\'cf\'b0\'b2\'e1\'cc\'fd\'c1\'a6\'ce\'c4\'b1\'be\'bc\'b0\'b4\'f0\'b0\'b8
\f0 .docx\
HSK4
\f1 \'c4\'a3\'c4\'e2
\f0 .pdf\
HSK4
\f1 \'bf\'bc\'ca\'d4
\f0 .pdf\
HSK4
\f1 \'bf\'bc\'ca\'d4\'b4\'f3\'b8\'d9
\f0 .pdf\
HSK4
\f1 \'bf\'ce\'b1\'be\'c1\'b7\'cf\'b0\'b4\'f0\'b0\'b8
\f0 .pdf\
HSK5-Book-A
\f1 \'b4\'ca\'bb\'e3\'b1\'ed
\f0 .pdf\
HSK5-B
\f1 \'b4\'ca\'bb\'e3
\f0 .pdf\
HSK_4_Chinese_to_English.apkg\
HSK_Vocabulary_List.xls\
HSK
\f1 \'b1\'ea\'d7\'bc\'bd\'cc\'b3\'cc
\f0   5  
\f1 \'c9\'cf
\f0 _14014590.pdf\
HSK
\f1 \'b1\'ea\'d7\'bc\'bd\'cc\'b3\'cc
\f0 4
\f1 \'cf\'c2
\f0 .pdf\
HSK
\f1 \'b1\'ea\'d7\'bc\'bd\'cc\'b3\'cc
\f0 5
\f1 \'c9\'cf
\f0 .pdf\
HSK
\f1 \'b1\'ea\'d7\'bc\'bd\'cc\'b3\'cc
\f0 5
\f1 \'a3\'a8\'c9\'cf\'a3\'a9\'c1\'b7\'cf\'b0\'b2\'e1\'c2\'bc\'d2\'f4\'ce\'c4\'bc\'fe
\f0 .rar\
HW-OJ-1-Sample-Solutions.docx\
HW-OJ-3-Sample-Solutions.docx\
HW4-Sample-Solutions.docx\
Hamilton.2020.1080p.WEB.h264-WATCHER-[rarbg.to].torrent\
Hanping_Chinese_HSK_1-6 (1).apkg\
Hanping_Chinese_HSK_1-6.apkg\
IMG_0168.jpg\
IMG_0407.HEIC\
IMG_0731.HEIC\
Install League of Legends euw.app\
Install League of Legends euw.zip\
Invincible - Shen Jian - Ongoing To 1395.prc\
Ip.Man.4.The.Finale.2019.1080p.BluRay.x264-WUTANG-[rarbg.to].torrent\
Irresistible.2020.1080p.AMZN.WEBRip.DDP5.1.x264-NTG-[rarbg.to].torrent\
Its.Always.Sunny.In.Philadelphia.S05.BDRip.x264-ION10-[rarbg.to].torrent\
Its.Always.Sunny.In.Philadelphia.S06.BDRip.x264-ION10-[rarbg.to].torrent\
Its.Always.Sunny.in.Philadelphia.S01.WEB-DL.AAC2.0.H264-BTN[rartv]-[rarbg.to].torrent\
Its.Always.Sunny.in.Philadelphia.S02.WEB-DL.AAC2.0.H264-BTN[rartv]-[rarbg.to].torrent\
Its.Always.Sunny.in.Philadelphia.S03.WEB-DL.AAC2.0.H264-BTN[rartv]-[rarbg.to].torrent\
Its.Always.Sunny.in.Philadelphia.S04.WEB-DL.AAC2.0.H264-BTN[rartv]-[rarbg.to].torrent\
Its.Always.Sunny.in.Philadelphia.S07.1080p.BluRay.x264-RCSW[rartv]-[rarbg.to].torrent\
Journey.to.China.The.Mystery.of.Iron.Mask.2019.1080p.BluRay.x264-PFa-[rarbg.to].torrent\
Keka-1.1.30.dmg\
Kontaktpersonenliste.xlsx\
Kuendigungsschreiben-Fitnessstudio-Vorlage-Mustertext.docx\
Last.Week.Tonight.with.John.Oliver.S07E23.August.30.2020.1080p.AMZN.WEBRip.DDP2.0.x264-monkee[rartv]-[rarbg.to].torrent\
Last.Week.Tonight.with.John.Oliver.S07E23.August.30.2020.1080p.HMAX.WEBRip.DD2.0.x264-monkee[rartv]-[rarbg.to].torrent\
Lazy Dungeon Master - Volume 1.epub\
LectureCoverage\
Li Xiaoqi, Ren Xuemei, Xu Jingning. 
\f1 \'c0\'ee\'cf\'fe\'e7\'f7\'a3\'ac\'c8\'ce\'d1\'a9\'c3\'b7\'a3\'ac\'d0\'ec\'be\'a7\'c4\'fd
\f0   - Boya Chinese. Elementary II (second edition) 
\f1 \'b2\'a9\'d1\'c5\'ba\'ba\'d3\'ef
\f0 \'b7
\f1 \'b3\'f5\'bc\'b6\'c6\'f0\'b2\'bd\'c6\'aa
\f0  II.   - libgen.lc.pdf.download\
Li Xiaoqi, Ren Xuemei, Xu Jingning. 
\f1 \'c0\'ee\'cf\'fe\'e7\'f7\'a3\'ac\'c8\'ce\'d1\'a9\'c3\'b7\'a3\'ac\'d0\'ec\'be\'a7\'c4\'fd
\f0   - Boya Chinese. Elementary II (second edition) 
\f1 \'b2\'a9\'d1\'c5\'ba\'ba\'d3\'ef
\f0 \'b7
\f1 \'b3\'f5\'bc\'b6\'c6\'f0\'b2\'bd\'c6\'aa
\f0  II. .pdf.download\
Masters.in.Forbidden.City.2016.CHINESE.1080p.BluRay.H264.AAC-VXT-[rarbg.to].torrent\
Muster_Anwesenheitsliste_Corona_DaTeNSCHuTZ.docx\
New Horizon in Chinese Philosophy
\f1 \'bd\'cc\'d1\'a7\'b4\'f3\'b8\'d9
\f0 .doc\
Orientation Activities Schedule31-4.docx\
Orientation Handbook for New Postgraduate Students\
Orientation Handbook for New Postgraduate Students.zip.download\
Protokoll eines aoCC vom  04.09.2020.pdf\
RS9900704319 (1).pdf\
RS9900704319.pdf\
Rechnung-nc-1322035.pdf\
Rechnung-nc-1402805.pdf\
Rick.and.Morty.S04.1080p.AMZN.WEBRip.DDP5.1.x264-CtrlHD[rartv]-[rarbg.to].torrent\
SHKD-754 
\f1 \'c5\'ae\'bd\'cc\'8e\'9f\'a5\'ec\'a5\'a4\'a5\'d7\'a4\'bd\'a4\'ec\'a4\'c7\'a4\'e2\'c9\'fa\'cd\'bd\'a4\'f2\'90\'db\'a4\'b7\'a4\'c6\'a4\'a4\'a4\'eb
\f0  
\f1 \'b9\'e2\'be\'ae\'a4\'d2\'a4\'ab\'a4\'ea
\f0  (1).torrent\
SHKD-754 
\f1 \'c5\'ae\'bd\'cc\'8e\'9f\'a5\'ec\'a5\'a4\'a5\'d7\'a4\'bd\'a4\'ec\'a4\'c7\'a4\'e2\'c9\'fa\'cd\'bd\'a4\'f2\'90\'db\'a4\'b7\'a4\'c6\'a4\'a4\'a4\'eb
\f0  
\f1 \'b9\'e2\'be\'ae\'a4\'d2\'a4\'ab\'a4\'ea
\f0 .torrent\
Sea.of.Shadows.2019.1080p.HULU.WEBRip.DDP5.1.x264-FC-[rarbg.to].torrent\
Sino_German_Tongji_E_12Juli2020.pdf\
SiteReliabilityEngineering.pdf\
South.Park.S24E00.The.Pandemic.Special.REPACK.1080p.WEB.h264-BAE[rartv]-[rarbg.to].torrent\
StatHunt (1).mp4\
StatHunt.mp4\
Superman.Man.of.Tomorrow.2020.1080p.WEBRip.x264-RARBG-[rarbg.to].torrent\
Superman.Red.Son.2020.1080p.WEBRip.DD5.1.x264-CM-[rarbg.to].torrent\
Ted.Lasso.S01E01.1080p.WEB.H264-OATH[rartv]-[rarbg.to].torrent\
Ted.Lasso.S01E02.1080p.WEB.h264-TRUMP[rartv]-[rarbg.to].torrent\
Ted.Lasso.S01E03.1080p.WEB.H264-OATH[rartv]-[rarbg.to].torrent\
Ted.Lasso.S01E04.1080p.WEB.H264-BLACKHAT[rartv]-[rarbg.to].torrent\
Ted.Lasso.S01E05.1080p.ATVP.WEB-DL.DDP5.1.Atmos.x264-NOGRP[rartv]-[rarbg.to].torrent\
Ted.Lasso.S01E06.1080p.ATVP.WEB-DL.DDP5.1.Atmos.x264-NOGRP[rartv]-[rarbg.to].torrent\
Ted.Lasso.S01E07.1080p.ATVP.WEB-DL.DDP5.1.Atmos.x264-NOGRP[rartv]-[rarbg.to].torrent\
Ted.Lasso.S01E08.1080p.WEB.H264-VIDEOHOLE[rartv]-[rarbg.to].torrent\
Ted.Lasso.S01E09.1080p.WEB.H264-VIDEOHOLE[rartv]-[rarbg.to].torrent\
Ted.Lasso.S01E10.1080p.WEB.H264-CAKES[rartv]-[rarbg.to].torrent\
The Experimental Log of the Crazy Lich - Chapter 001-100\
The Experimental Log of the Crazy Lich - Chapter 001-100.zip\
The Experimental Log of the Crazy Lich - Chapter 101-200 [Gravity Tales][Hasseno].epub\
The Experimental Log of the Crazy Lich - Chapter 201-300 [Gravity Tales][Hasseno].epub\
The.Green.Man.1956.1080p.BluRay.x264-GHOULS-[rarbg.to].torrent\
The.Imagineering.Story.S01.1080p.DSNP.WEBRip.DDP5.1.x264-PETRiFiED[rartv]-[rarbg.to].torrent\
The.King.of.Staten.Island.2020.1080p.WEBRip.x264-RARBG-[rarbg.to].torrent\
The.Lighthouse.2019.1080p.BluRay.x264-GECKOS-[rarbg.to].torrent\
The.Mandalorian.S01.1080p.DSNP.WEBRip.DDP5.1.Atmos.x264-SKGTV[rartv]-[rarbg.to].torrent\
The.Office.US.S07E04.Sex.Ed.1080p.BluRay.x264-DEiMOS.mkv.download\
The.Office.US.S07E05.The.Sting.1080p.BluRay.x264-DEiMOS.mkv\
The.Office.US.S07E05.The.Sting.1080p.BluRay.x264-DEiMOS.mkv.download\
The.Office.US.S07E06.Costume.Contest.1080p.BluRay.x264-DEiMOS.mkv\
The.Outpost.2020.1080p.WEBRip.AAC5.1.x264-CM-[rarbg.to].torrent\
The.Ruling.Class.1972.1080p.HMAX.WEBRip.DD2.0.x264-QOQ-[rarbg.to].torrent\
The.Wailing.2016.KOREAN.1080p.BluRay.H264.AAC-VXT-[rarbg.to].torrent\
The.Wild.Goose.Lake.2019.CHINESE.1080p.BluRay.H264.AAC-VXT-[rarbg.to].torrent\
VooVMeeting_1410000198_1.7.1.510.publish.dmg\
Westworld.S03.1080p.AMZN.WEBRip.DDP5.1.x264-NTb[rartv]-[rarbg.to].torrent\
Wohnungsgeberbest\'e4tigung_Internet.pdf\
Wu Dong Qian Kun - Volume 01 - Great Yan Empire.epub\
Zhumu.pkg\
Zoom.pkg\
[tuhao66.com]
\f1 \'c9\'a8\'b6\'be
\f0 2
\f1 \'a3\'ba\'cc\'ec\'b5\'d8\'b6\'d4\'be\'f6
\f0 bd1080p
\f1 \'b8\'df\'c7\'e5\'b9\'fa\'d4\'c1\'cb\'ab\'d3\'ef\'d6\'d0\'d7\'d6
\f0 .mkv.torrent\
[www.asianovel.com]_-_Akuyaku_Reijo_Ni_Koi_Wo_Shite.epub\
[www.asianovel.com]_-_Chikyuu_Tenseisha_no_Koroshikata__Chapter_0_-_Prologue_-_Chapter_75.epub\
[www.asianovel.com]_-_Even_Posing_as_a_Hero_is_Easy___Why__Cause_I___m_a_God_____Chapter_0_-_Prologue_-_Chapter_102.epub\
[www.asianovel.com]_-_Himekishi_ga_Classmate____Isekai_Cheat_de_Dorei_ka_Harem_.epub\
[www.asianovel.com]_-_I_Reincarnated_For_Nothing__Chapter_0_-_Prologue_-_Chapter_201 (1).epub\
[www.asianovel.com]_-_I_Reincarnated_For_Nothing__Chapter_0_-_Prologue_-_Chapter_201.epub\
[www.asianovel.com]_-_I_am_the_Monarch__Chapter_0_-_Prologue_-_Chapter_199.epub\
[www.asianovel.com]_-_Martial_King___s_Retired_Life__Chapter_0_-_Prologue_-_Vol.4_Chapter_35.epub\
[www.asianovel.com]_-_Martial_King___s_Retired_Life__Vol.4_Chapter_36_-_Vol.7_Chapter_32.epub\
[www.asianovel.com]_-_Revolution_of_the_8th_Class_Mage__Chapter_1_-_Chapter_185.epub\
[www.asianovel.com]_-_The_Experimental_Log_of_the_Crazy_Lich__Chapter_1_-_Chapter_200.epub\
[www.asianovel.com]_-_The_Lazy_King__Chapter_1_-_Chapter_16.epub\
[www.asianovel.com]_-_The_Silly_Alchemist__Chapter_1_-_Chapter_200.epub\
[www.asianovel.com]_-_The_Silly_Alchemist__Chapter_201_-_Chapter_400 (1).epub\
[www.asianovel.com]_-_The_Silly_Alchemist__Chapter_201_-_Chapter_400.epub\
_var_tmp_tmp_generated_BishopPatternRecognitionAndMachineLearning_Bishop+-+Pattern+Recognition+and+Machine+Learning.mobi\
a9a\
assignment2\
authors-version.pdf\
baomingbiao.rar\
beat_bench.py\
botpress-v12_10_2-darwin-x64\
botpress-v12_7_1-darwin-x64.zip\
calibre-4.22.0.dmg\
citations.html\
csg.mobi\
d7ce7f9d8c80f99af103ed3cf4ad5953.torrent\
db-generation.rar\
download (1).html\
download (2).html\
download.html\
duesseldorf-city-guide-germany.jpg\
e41c-5f79d5028f26dc3ee765e451b2d369e69cb3bfee.torrent\
en_windows_10_consumer_edition_version_1809_updated_sept_2018_x64_dvd_491ea967.iso\
files-2.html\
files-3.html\
files-4.html\
files.html\
final-2.docx\
final2018.pdf\
final_project_grading_rubric.pdf\
gmt20200618-104303_club-i10_1920x1080.mp4\
goose.jpg\
hssb.mobi\
i-reincarnated-for-nothing.epub\
i10-goodbye-checklist.pdf\
insurance.csv\
iterative-stratification-master\
kuendigung-mietvertrag-muster.doc\
l.html\
lish-moa\
merlin_157387785_e74e3d11-82d4-4c5a-95dd-f30f6b1f1dde-superJumbo.jpg\
npc-town-building-game.epub\
ppt-week1.pptx\
refs\
reverend-insanity.epub\
soukos_robots.jpg\
titanic\
training_data\
view_video (1).html\
view_video.html\
vlc-3.0.11.dmg\
vlc-output.ts\
w104.mp4\
w104.mp4.download\
warlock-of-the-magus-world.mobi\
zhumu_0.mp4\
zhumu_0.mp4.download\
zhumu_4.mp4\
zhumu_4.mp4-2.download\
zhumu_4.mp4.download\
\uc0\u8706 \u8721 \u8710 \u8710 \u8804 \'91\'d2\u8721 _\'c3\'cf\u8804 \'9c\'d5\'a1\u8706 \u960 _shushu8.com.mobi\

\f1 \'a1\'b6
\f0 HSK
\f1 \'b1\'ea\'d7\'bc\'bd\'cc\'b3\'cc\'c1\'b7\'cf\'b0\'b2\'e1
\f0 4
\f1 \'cf\'c2\'a1\'b7\'cc\'fd\'c1\'a6\'ce\'c4\'b1\'be\'bc\'b0\'b2\'ce\'bf\'bc\'b4\'f0\'b0\'b8
\f0 .doc\

\f1 \'a1\'b6\'ce\'de\'b5\'d0\'cc\'ec\'cf\'c2\'a3\'ad\'cd\'f8\'d3\'ce\'b0\'e6\'a1\'b7
\f0 _qinkan.net.mobi\

\f1 \'a1\'b6\'c1\'f7\'d0\'c7\'ba\'fb\'b5\'fb\'bd\'a3\'a1\'b7
\f0 _qinkan.net.mobi\

\f1 \'d2\'bb\'ba\'cd\'b2\'bb
\f0 .pdf\

\f1 \'b9\'fa\'bc\'ca\'d1\'a7\'c9\'fa\'d4\'da\'cf\'df\'c8\'eb\'d1\'a7\'b1\'a8\'b5\'bd
\f0 .pdf\

\f1 \'c6\'c1\'c4\'bb\'bf\'ec\'d5\'d5
\f0  2020-08-31 19.55.59.png\

\f1 \'c6\'c1\'c4\'bb\'bf\'ec\'d5\'d5
\f0  2020-08-31 19.56.10.png\

\f1 \'b6\'b7\'c6\'c6\'b2\'d4\'f1\'b7
\f0 .mobi\

\f1 \'d5\'fd\'ca\'bd\'d1\'a7\'ce\'bb\'d6\'a4\'b2\'b9\'bd\'bb\'c9\'f9\'c3\'f7
\f0  Statement for submitting the official Degree Certificate.pdf\

\f1 \'c7\'e5\'bb\'aa\'b4\'f3\'d1\'a7\'b9\'fa\'bc\'ca\'d1\'d0\'be\'bf\'c9\'fa\'d4\'da\'cf\'df\'c8\'eb\'d1\'a7\'d6\'aa\'c7\'e9\'cd\'ac\'d2\'e2\'ca\'e9
\f0 -
\f1 \'d6\'d0\'d3\'a2\'ce\'c4
\f0 (1).docx\

\f1 \'c7\'e5\'bb\'aa\'b4\'f3\'d1\'a7\'d1\'d0\'be\'bf\'c9\'fa\'bf\'ce\'b1\'ed
\f0 .xls\

\f1 \'b5\'da\'d2\'bb\'bf\'ce\'d0\'b4\'d7\'d6
\f0 .pdf\

\f1 \'b0\'ac\'c8\'f0\'bf\'cb\'d7\'f7\'d2\'b5
\f0 .pdf\

\f1 \'bf\'ce\'ce\'c4
\f0 12 - 
\f1 \'d7\'f7\'d2\'b5
\f0 .docx\

\f1 \'c1\'bf\'b4\'ca
\f0 .pdf\
baconbaker@MacBook-Pro-2 Downloads % md5 2020-11-05\\ 10-04-51.mp4 \
MD5 (2020-11-05 10-04-51.mp4) = 80efe3f8593e2fdd78ff69211aa0948b\
baconbaker@MacBook-Pro-2 Downloads % md5 comb_exam.mp4 \
MD5 (comb_exam.mp4) = 436b89cab94fce93fc68fa92fec3b7ea\
baconbaker@MacBook-Pro-2 Downloads % cd..\
zsh: command not found: cd..\
baconbaker@MacBook-Pro-2 Downloads % cd ..\
baconbaker@MacBook-Pro-2 ~ % cd Documents \
baconbaker@MacBook-Pro-2 Documents % ls\
Anwesenheitsbogen_Corona.docx\
Application_Form.pdf\
Bachelor_Certificate.pdf\
Chinese\
Consent_Online_Enrollment.pdf\
Copy of 20 fall Chinese language list(1).xls\
Ebooks\
League of Legends\
Misc\
Orientation Activities Schedule7-11.docx\
Programming\
Statement_Degree_Submission.pdf\
Studium\
Thore Auslandssemster Bewerbung Entwurf.pages\
Zoom\
hwrk1.1.pdf\
is_enroll_form.pdf\
sr_agreement.pdf\
zhumu\
baconbaker@MacBook-Pro-2 Documents % cd Studium/C\\&A \
baconbaker@MacBook-Pro-2 C&A % ls\
Algo				Lectures\
Exam				Notes\
Homework			Programming\
IntroductoryCombinatorics.pdf\
baconbaker@MacBook-Pro-2 C&A % cd Exam \
baconbaker@MacBook-Pro-2 Exam % ls\
2020-11-05 12-15-31.mp4\
baconbaker@MacBook-Pro-2 Exam % dm5 2020-11-05\\ 12-15-31.mp4 \
zsh: command not found: dm5\
baconbaker@MacBook-Pro-2 Exam % md5 2020-11-05\\ 12-15-31.mp4 \
MD5 (2020-11-05 12-15-31.mp4) = b6ae6e5e21364327ce381fb715b25f09\
baconbaker@MacBook-Pro-2 Exam % clear\
\
baconbaker@MacBook-Pro-2 Exam % md5 2020280441_MalteMeng.mp4 \
MD5 (2020280441_MalteMeng.mp4) = cb93163a54c1c024af719068fa8d7dc3\
baconbaker@MacBook-Pro-2 Exam % md5 2020280441_
\f1 \'c3\'cf\'cf\'e9\'c8\'f0
\f0 .mp4 \
MD5 (2020280441_
\f1 \'c3\'cf\'cf\'e9\'c8\'f0
\f0 .mp4) = cb93163a54c1c024af719068fa8d7dc3\
baconbaker@MacBook-Pro-2 Exam % md5 2020280441_2_
\f1 \'c3\'cf\'cf\'e9\'c8\'f0
\f0 .mp4\
MD5 (2020280441_2_
\f1 \'c3\'cf\'cf\'e9\'c8\'f0
\f0 .mp4) = 20bb1f6aeae29a2b46aeda853816bb91\
baconbaker@MacBook-Pro-2 Exam % ls\
2020-11-05 12-15-31.mp4		2020280441_2_
\f1 \'c3\'cf\'cf\'e9\'c8\'f0
\f0 .mp4\
2020-11-05 12-29-12.mp4		2020280441_
\f1 \'c3\'cf\'cf\'e9\'c8\'f0
\f0 .mp4\
baconbaker@MacBook-Pro-2 Exam % cd ..\
baconbaker@MacBook-Pro-2 C&A % cd ..\
baconbaker@MacBook-Pro-2 Studium % ls\
AML		Adv_Chinese	DDBS		Misc\
ANM		C&A		Meng (B.Sc.)	Research\
baconbaker@MacBook-Pro-2 Studium % cd AML \
baconbaker@MacBook-Pro-2 AML % ls\
Homework		Papers			python-cheatsheets.pdf\
Literature		Project\
Notes			Slides\
baconbaker@MacBook-Pro-2 AML % ls\
Homework		Papers			python-cheatsheets.pdf\
Literature		Project\
Notes			Slides\
baconbaker@MacBook-Pro-2 AML % cd Project/moa-pipeline \
baconbaker@MacBook-Pro-2 moa-pipeline % ls\
README.md	input		pipeline-remake	processing\
__init__.py	input_reader.py	predict		run.py\
ensemble	models		processed-input\
baconbaker@MacBook-Pro-2 moa-pipeline % cd pipeline-remake \
baconbaker@MacBook-Pro-2 pipeline-remake % ls\
README.md	experimentation	models\
data		logs		processing\
baconbaker@MacBook-Pro-2 pipeline-remake % ls\
README.md	data		experimentation	logs		models		processing\
baconbaker@MacBook-Pro-2 pipeline-remake % conda activate ml\
l
\f2\b \cf3 \cb4 %
\f0\b0 \cf2 \cb3                                                                                                                                                      (ml) baconbaker@MacBook-Pro-2 pipeline-remake % ls\
README.md	data		experimentation	logs		models		processing\
(ml) baconbaker@MacBook-Pro-2 pipeline-remake % cd experimentation \
(ml) baconbaker@MacBook-Pro-2 experimentation % ls\
__init__.py		__pycache__		crossvalidation.py	evaluate.py		trial.csv		tuning.py\
(ml) baconbaker@MacBook-Pro-2 experimentation % cd ..\
(ml) baconbaker@MacBook-Pro-2 pipeline-remake % cd logs \
(ml) baconbaker@MacBook-Pro-2 logs % ls\
experiment_results.csv	tuning\
(ml) baconbaker@MacBook-Pro-2 logs % ls\
experiment_results.csv	tuning\
(ml) baconbaker@MacBook-Pro-2 logs % ipython\
Python 3.8.5 (default, Sep  4 2020, 02:22:02) \
Type 'copyright', 'credits' or 'license' for more information\
IPython 7.18.1 -- An enhanced Interactive Python. Type '?' for help.\
\
\cf5 In [
\f2\b \cf6 1
\f0\b0 \cf5 ]: 
\f2\b import
\f0\b0 \cf2  
\f2\b \cf7 pandas
\f0\b0 \cf2  
\f2\b \cf5 as
\f0\b0 \cf2  
\f2\b \cf7 pd
\f0\b0 \cf2 \
\
\cf5 In [
\f2\b \cf6 2
\f0\b0 \cf5 ]: \cf2 res = pd.read_csv(\cf8 './logs/experiment_results.csv'\cf2 )\
\cf9 ---------------------------------------------------------------------------\cf2 \
\cf9 FileNotFoundError\cf2                          Traceback (most recent call last)\
\cf10 <ipython-input-2-99fb93474527>\cf2  in \cf11 <module>\cf2 \
\cf10 ----> 1\cf9  \cf2 res \cf12 =\cf2  pd\cf12 .\cf2 read_csv\cf12 ('./logs/experiment_results.csv')\cf2 \
\
\cf10 ~/anaconda3/envs/ml/lib/python3.8/site-packages/pandas/io/parsers.py\cf2  in \cf11 read_csv\cf12 (filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\cf2 \

\f2\b \cf10     684
\f0\b0 \cf2      )\

\f2\b \cf10     685
\f0\b0 \cf2  \
\cf10 --> 686\cf9      \cf10 return\cf2  _read\cf12 (\cf2 filepath_or_buffer\cf12 ,\cf2  kwds\cf12 )\cf2 \

\f2\b \cf10     687
\f0\b0 \cf2  \

\f2\b \cf10     688
\f0\b0 \cf2  \
\
\cf10 ~/anaconda3/envs/ml/lib/python3.8/site-packages/pandas/io/parsers.py\cf2  in \cf11 _read\cf12 (filepath_or_buffer, kwds)\cf2 \

\f2\b \cf10     450
\f0\b0 \cf2  \

\f2\b \cf10     451
\f0\b0 \cf2      \cf9 # Create the parser.\cf2 \
\cf10 --> 452\cf9      \cf2 parser \cf12 =\cf2  TextFileReader\cf12 (\cf2 fp_or_buf\cf12 ,\cf2  \cf12 **\cf2 kwds\cf12 )\cf2 \

\f2\b \cf10     453
\f0\b0 \cf2  \

\f2\b \cf10     454
\f0\b0 \cf2      \cf10 if\cf2  chunksize \cf10 or\cf2  iterator\cf12 :\cf2 \
\
\cf10 ~/anaconda3/envs/ml/lib/python3.8/site-packages/pandas/io/parsers.py\cf2  in \cf11 __init__\cf12 (self, f, engine, **kwds)\cf2 \

\f2\b \cf10     944
\f0\b0 \cf2              self\cf12 .\cf2 options\cf12 ["has_index_names"]\cf2  \cf12 =\cf2  kwds\cf12 ["has_index_names"]\cf2 \

\f2\b \cf10     945
\f0\b0 \cf2  \
\cf10 --> 946\cf9          \cf2 self\cf12 .\cf2 _make_engine\cf12 (\cf2 self\cf12 .\cf2 engine\cf12 )\cf2 \

\f2\b \cf10     947
\f0\b0 \cf2  \

\f2\b \cf10     948
\f0\b0 \cf2      \cf10 def\cf2  close\cf12 (\cf2 self\cf12 ):\cf2 \
\
\cf10 ~/anaconda3/envs/ml/lib/python3.8/site-packages/pandas/io/parsers.py\cf2  in \cf11 _make_engine\cf12 (self, engine)\cf2 \

\f2\b \cf10    1176
\f0\b0 \cf2      \cf10 def\cf2  _make_engine\cf12 (\cf2 self\cf12 ,\cf2  engine\cf12 ="c"):\cf2 \

\f2\b \cf10    1177
\f0\b0 \cf2          \cf10 if\cf2  engine \cf12 ==\cf2  \cf12 "c":\cf2 \
\cf10 -> 1178\cf9              \cf2 self\cf12 .\cf2 _engine \cf12 =\cf2  CParserWrapper\cf12 (\cf2 self\cf12 .\cf2 f\cf12 ,\cf2  \cf12 **\cf2 self\cf12 .\cf2 options\cf12 )\cf2 \

\f2\b \cf10    1179
\f0\b0 \cf2          \cf10 else\cf12 :\cf2 \

\f2\b \cf10    1180
\f0\b0 \cf2              \cf10 if\cf2  engine \cf12 ==\cf2  \cf12 "python":\cf2 \
\
\cf10 ~/anaconda3/envs/ml/lib/python3.8/site-packages/pandas/io/parsers.py\cf2  in \cf11 __init__\cf12 (self, src, **kwds)\cf2 \

\f2\b \cf10    2006
\f0\b0 \cf2          kwds\cf12 ["usecols"]\cf2  \cf12 =\cf2  self\cf12 .\cf2 usecols\

\f2\b \cf10    2007
\f0\b0 \cf2  \
\cf10 -> 2008\cf9          \cf2 self\cf12 .\cf2 _reader \cf12 =\cf2  parsers\cf12 .\cf2 TextReader\cf12 (\cf2 src\cf12 ,\cf2  \cf12 **\cf2 kwds\cf12 )\cf2 \

\f2\b \cf10    2009
\f0\b0 \cf2          self\cf12 .\cf2 unnamed_cols \cf12 =\cf2  self\cf12 .\cf2 _reader\cf12 .\cf2 unnamed_cols\

\f2\b \cf10    2010
\f0\b0 \cf2  \
\
\cf10 pandas/_libs/parsers.pyx\cf2  in \cf11 pandas._libs.parsers.TextReader.__cinit__\cf12 ()\cf2 \
\
\cf10 pandas/_libs/parsers.pyx\cf2  in \cf11 pandas._libs.parsers.TextReader._setup_parser_source\cf12 ()\cf2 \
\
\cf9 FileNotFoundError\cf2 : [Errno 2] No such file or directory: './logs/experiment_results.csv'\
\
\cf5 In [
\f2\b \cf6 3
\f0\b0 \cf5 ]: \cf2 exit\
(ml) baconbaker@MacBook-Pro-2 logs % ls\
experiment_results.csv	tuning\
(ml) baconbaker@MacBook-Pro-2 logs % ipython   \
Python 3.8.5 (default, Sep  4 2020, 02:22:02) \
Type 'copyright', 'credits' or 'license' for more information\
IPython 7.18.1 -- An enhanced Interactive Python. Type '?' for help.\
\
\cf5 In [
\f2\b \cf6 1
\f0\b0 \cf5 ]: 
\f2\b import
\f0\b0 \cf2  
\f2\b \cf7 pandas
\f0\b0 \cf2  
\f2\b \cf5 as
\f0\b0 \cf2  
\f2\b \cf7 pd
\f0\b0 \cf2 \
\
\cf5 In [
\f2\b \cf6 2
\f0\b0 \cf5 ]: \cf2 \
\
\cf5 In [
\f2\b \cf6 2
\f0\b0 \cf5 ]: \cf2 df = pd.read_csv(\cf8 'experiment_results.csv'\cf2 )\
\
\cf5 In [
\f2\b \cf6 3
\f0\b0 \cf5 ]: \cf2 df = df.sort_values(by=\cf8 'loss'\cf2 )\
\
\cf5 In [
\f2\b \cf6 4
\f0\b0 \cf5 ]: \cf2 df.head(\cf5 40\cf2 )\
\cf13 Out[
\f2\b \cf14 4
\f0\b0 \cf13 ]: \cf2 \
                                     feature_csv                            target_csv   dropout  ...      loss       auc      extra_inf\
34      ../processing/gauss_pca2/v1.8g100c40.csv       ../processing/feature_eng_y.csv  0.150000  ...  0.016638  0.834692            NaN\
33      ../processing/gauss_pca2/v1.2g100c60.csv       ../processing/feature_eng_y.csv  0.150000  ...  0.016824  0.831320            NaN\
54  ../processing/real_gauss_pca/v0.8g450c55.csv       ../processing/feature_eng_y.csv  0.200000  ...  0.016842  0.830199  submit:.01915\
24         ../processing/gauss_pca/v2g100c25.csv       ../processing/feature_eng_y.csv  0.200000  ...  0.016851  0.833900            NaN\
25         ../processing/gauss_pca/v2g100c50.csv       ../processing/feature_eng_y.csv  0.200000  ...  0.016852  0.831860            NaN\
64  ../processing/real_gauss_pca/v0.8g450c55.csv       ../processing/feature_eng_y.csv  0.054030  ...  0.016869  0.792295            NaN\
56  ../processing/real_gauss_pca/v0.8g500c60.csv       ../processing/feature_eng_y.csv  0.200000  ...  0.016871  0.831992            NaN\
26         ../processing/gauss_pca/v2g100c75.csv       ../processing/feature_eng_y.csv  0.200000  ...  0.016872  0.833674            NaN\
60  ../processing/real_gauss_pca/v0.8g500c60.csv       ../processing/feature_eng_y.csv  0.200000  ...  0.016875  0.830392            NaN\
55  ../processing/real_gauss_pca/v0.8g400c60.csv       ../processing/feature_eng_y.csv  0.200000  ...  0.016883  0.830349            NaN\
63  ../processing/real_gauss_pca/v0.8g450c55.csv       ../processing/feature_eng_y.csv  0.062674  ...  0.016883  0.799635            NaN\
1           ../processing/feature_eng_temp_x.csv  ../processing/feature_eng_temp_y.csv  0.209301  ...  0.016887  0.825870            NaN\
28         ../processing/gauss_pca/v2g150c50.csv       ../processing/feature_eng_y.csv  0.200000  ...  0.016888  0.833379            NaN\
57  ../processing/real_gauss_pca/v0.8g600c60.csv       ../processing/feature_eng_y.csv  0.200000  ...  0.016889  0.829842            NaN\
32         ../processing/gauss_pca/v2g200c75.csv       ../processing/feature_eng_y.csv  0.200000  ...  0.016892  0.833343            NaN\
16         ../processing/gauss_pca/v1g100c50.csv       ../processing/feature_eng_y.csv  0.200000  ...  0.016894  0.829795            NaN\
18         ../processing/gauss_pca/v1g150c25.csv       ../processing/feature_eng_y.csv  0.200000  ...  0.016899  0.829574            NaN\
35  ../processing/real_gauss_pca/v0.4g120c60.csv       ../processing/feature_eng_y.csv  0.200000  ...  0.016901  0.828209            NaN\
7        ../processing/gauss_pca/v0.5g100c50.csv       ../processing/feature_eng_y.csv  0.200000  ...  0.016904  0.829087            NaN\
6        ../processing/gauss_pca/v0.5g100c25.csv       ../processing/feature_eng_y.csv  0.200000  ...  0.016905  0.829050            NaN\
27         ../processing/gauss_pca/v2g150c25.csv       ../processing/feature_eng_y.csv  0.200000  ...  0.016910  0.833890            NaN\
17         ../processing/gauss_pca/v1g100c75.csv       ../processing/feature_eng_y.csv  0.200000  ...  0.016911  0.831216            NaN\
15         ../processing/gauss_pca/v1g100c25.csv       ../processing/feature_eng_y.csv  0.200000  ...  0.016914  0.831472            NaN\
12       ../processing/gauss_pca/v0.5g200c25.csv       ../processing/feature_eng_y.csv  0.200000  ...  0.016923  0.827892            NaN\
11       ../processing/gauss_pca/v0.5g150c75.csv       ../processing/feature_eng_y.csv  0.200000  ...  0.016931  0.827619            NaN\
29         ../processing/gauss_pca/v2g150c75.csv       ../processing/feature_eng_y.csv  0.200000  ...  0.016936  0.832634            NaN\
31         ../processing/gauss_pca/v2g200c50.csv       ../processing/feature_eng_y.csv  0.200000  ...  0.016948  0.833584            NaN\
61  ../processing/real_gauss_pca/v0.8g600c60.csv       ../processing/feature_eng_y.csv  0.200000  ...  0.016956  0.836000  submit:.01918\
5      ../processing/feature_eng_gpca10050_x.csv       ../processing/feature_eng_y.csv  0.200000  ...  0.016964  0.831736            NaN\
42  ../processing/real_gauss_pca/v0.4g100c50.csv       ../processing/feature_eng_y.csv  0.200000  ...  0.016966  0.828911            NaN\
9        ../processing/gauss_pca/v0.5g150c25.csv       ../processing/feature_eng_y.csv  0.200000  ...  0.016968  0.827984            NaN\
10       ../processing/gauss_pca/v0.5g150c50.csv       ../processing/feature_eng_y.csv  0.200000  ...  0.016969  0.829201            NaN\
13       ../processing/gauss_pca/v0.5g200c50.csv       ../processing/feature_eng_y.csv  0.200000  ...  0.016970  0.825402            NaN\
46  ../processing/real_gauss_pca/v0.5g100c50.csv       ../processing/feature_eng_y.csv  0.200000  ...  0.016980  0.829077            NaN\
40  ../processing/real_gauss_pca/v0.8g500c60.csv       ../processing/feature_eng_y.csv  0.200000  ...  0.016986  0.830514            NaN\
19         ../processing/gauss_pca/v1g150c50.csv       ../processing/feature_eng_y.csv  0.200000  ...  0.016987  0.829036            NaN\
20         ../processing/gauss_pca/v1g150c75.csv       ../processing/feature_eng_y.csv  0.200000  ...  0.016999  0.829471            NaN\
47  ../processing/real_gauss_pca/v0.5g100c60.csv       ../processing/feature_eng_y.csv  0.200000  ...  0.017001  0.830715            NaN\
41  ../processing/real_gauss_pca/v0.8g600c60.csv       ../processing/feature_eng_y.csv  0.200000  ...  0.017001  0.828335            NaN\
30         ../processing/gauss_pca/v2g200c25.csv       ../processing/feature_eng_y.csv  0.200000  ...  0.017004  0.834230            NaN\
\
[40 rows x 9 columns]\
\
\cf5 In [
\f2\b \cf6 5
\f0\b0 \cf5 ]: \cf2 df.head(\cf5 10\cf2 )\
\cf13 Out[
\f2\b \cf14 5
\f0\b0 \cf13 ]: \cf2 \
                                     feature_csv                       target_csv  dropout  learning_rate  batch_size  label_smoothing      loss       auc      extra_inf\
34      ../processing/gauss_pca2/v1.8g100c40.csv  ../processing/feature_eng_y.csv  0.15000       0.006500         500         0.000000  0.016638  0.834692            NaN\
33      ../processing/gauss_pca2/v1.2g100c60.csv  ../processing/feature_eng_y.csv  0.15000       0.006500         500         0.000000  0.016824  0.831320            NaN\
54  ../processing/real_gauss_pca/v0.8g450c55.csv  ../processing/feature_eng_y.csv  0.20000       0.003809        1200         0.000000  0.016842  0.830199  submit:.01915\
24         ../processing/gauss_pca/v2g100c25.csv  ../processing/feature_eng_y.csv  0.20000       0.003500         500         0.000000  0.016851  0.833900            NaN\
25         ../processing/gauss_pca/v2g100c50.csv  ../processing/feature_eng_y.csv  0.20000       0.003500         500         0.000000  0.016852  0.831860            NaN\
64  ../processing/real_gauss_pca/v0.8g450c55.csv  ../processing/feature_eng_y.csv  0.05403       0.001386         200         0.002577  0.016869  0.792295            NaN\
56  ../processing/real_gauss_pca/v0.8g500c60.csv  ../processing/feature_eng_y.csv  0.20000       0.003809        1200         0.000000  0.016871  0.831992            NaN\
26         ../processing/gauss_pca/v2g100c75.csv  ../processing/feature_eng_y.csv  0.20000       0.003500         500         0.000000  0.016872  0.833674            NaN\
60  ../processing/real_gauss_pca/v0.8g500c60.csv  ../processing/feature_eng_y.csv  0.20000       0.003909        1200         0.000020  0.016875  0.830392            NaN\
55  ../processing/real_gauss_pca/v0.8g400c60.csv  ../processing/feature_eng_y.csv  0.20000       0.003809        1200         0.000000  0.016883  0.830349            NaN\
\
\cf5 In [
\f2\b \cf6 6
\f0\b0 \cf5 ]: \cf2 df.head(\cf5 20\cf2 )\
\cf13 Out[
\f2\b \cf14 6
\f0\b0 \cf13 ]: \cf2 \
                                     feature_csv                            target_csv   dropout  learning_rate  batch_size  label_smoothing      loss       auc      extra_inf\
34      ../processing/gauss_pca2/v1.8g100c40.csv       ../processing/feature_eng_y.csv  0.150000       0.006500         500         0.000000  0.016638  0.834692            NaN\
33      ../processing/gauss_pca2/v1.2g100c60.csv       ../processing/feature_eng_y.csv  0.150000       0.006500         500         0.000000  0.016824  0.831320            NaN\
54  ../processing/real_gauss_pca/v0.8g450c55.csv       ../processing/feature_eng_y.csv  0.200000       0.003809        1200         0.000000  0.016842  0.830199  submit:.01915\
24         ../processing/gauss_pca/v2g100c25.csv       ../processing/feature_eng_y.csv  0.200000       0.003500         500         0.000000  0.016851  0.833900            NaN\
25         ../processing/gauss_pca/v2g100c50.csv       ../processing/feature_eng_y.csv  0.200000       0.003500         500         0.000000  0.016852  0.831860            NaN\
64  ../processing/real_gauss_pca/v0.8g450c55.csv       ../processing/feature_eng_y.csv  0.054030       0.001386         200         0.002577  0.016869  0.792295            NaN\
56  ../processing/real_gauss_pca/v0.8g500c60.csv       ../processing/feature_eng_y.csv  0.200000       0.003809        1200         0.000000  0.016871  0.831992            NaN\
26         ../processing/gauss_pca/v2g100c75.csv       ../processing/feature_eng_y.csv  0.200000       0.003500         500         0.000000  0.016872  0.833674            NaN\
60  ../processing/real_gauss_pca/v0.8g500c60.csv       ../processing/feature_eng_y.csv  0.200000       0.003909        1200         0.000020  0.016875  0.830392            NaN\
55  ../processing/real_gauss_pca/v0.8g400c60.csv       ../processing/feature_eng_y.csv  0.200000       0.003809        1200         0.000000  0.016883  0.830349            NaN\
63  ../processing/real_gauss_pca/v0.8g450c55.csv       ../processing/feature_eng_y.csv  0.062674       0.001386         200         0.004251  0.016883  0.799635            NaN\
1           ../processing/feature_eng_temp_x.csv  ../processing/feature_eng_temp_y.csv  0.209301       0.003306         900         0.000000  0.016887  0.825870            NaN\
28         ../processing/gauss_pca/v2g150c50.csv       ../processing/feature_eng_y.csv  0.200000       0.003500         500         0.000000  0.016888  0.833379            NaN\
57  ../processing/real_gauss_pca/v0.8g600c60.csv       ../processing/feature_eng_y.csv  0.200000       0.003809        1200         0.000000  0.016889  0.829842            NaN\
32         ../processing/gauss_pca/v2g200c75.csv       ../processing/feature_eng_y.csv  0.200000       0.003500         500         0.000000  0.016892  0.833343            NaN\
16         ../processing/gauss_pca/v1g100c50.csv       ../processing/feature_eng_y.csv  0.200000       0.003500         500         0.000000  0.016894  0.829795            NaN\
18         ../processing/gauss_pca/v1g150c25.csv       ../processing/feature_eng_y.csv  0.200000       0.003500         500         0.000000  0.016899  0.829574            NaN\
35  ../processing/real_gauss_pca/v0.4g120c60.csv       ../processing/feature_eng_y.csv  0.200000       0.003000         900         0.000000  0.016901  0.828209            NaN\
7        ../processing/gauss_pca/v0.5g100c50.csv       ../processing/feature_eng_y.csv  0.200000       0.003500         500         0.000000  0.016904  0.829087            NaN\
6        ../processing/gauss_pca/v0.5g100c25.csv       ../processing/feature_eng_y.csv  0.200000       0.003500         500         0.000000  0.016905  0.829050            NaN\
\
\cf5 In [
\f2\b \cf6 7
\f0\b0 \cf5 ]: \cf2 exit\
(ml) baconbaker@MacBook-Pro-2 logs % ls\
experiment_results.csv	tuning\
(ml) baconbaker@MacBook-Pro-2 logs % cd ..\
(ml) baconbaker@MacBook-Pro-2 pipeline-remake % ls\
README.md	data		experimentation	logs		models		processing\
(ml) baconbaker@MacBook-Pro-2 pipeline-remake % cd experimentation \
(ml) baconbaker@MacBook-Pro-2 experimentation % ls\
__init__.py		__pycache__		crossvalidation.py	evaluate.py		trial.csv		tuning.py\
(ml) baconbaker@MacBook-Pro-2 experimentation % python tuning.py  \
\cf10 [I 2020-11-05 16:13:44,120]\cf2  A new study created in memory with name: no-name-9abefc64-c43f-434c-9e5c-0e133b29193f\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/trial/_trial.py:745: RuntimeWarning: Inconsistent parameter values for distribution with name "label_smoothing"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: \{'low': 1, 'high': 10, 'step': 1\}\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
^CTraceback (most recent call last):\
  File "tuning.py", line 67, in <module>\
    param_tuning()\
  File "tuning.py", line 62, in param_tuning\
    study.optimize(tuning_objective, n_trials=150)\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/study.py", line 338, in optimize\
    self._optimize_sequential(\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/study.py", line 747, in _optimize_sequential\
    self._run_trial_and_callbacks(func, catch, callbacks, gc_after_trial)\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/study.py", line 776, in _run_trial_and_callbacks\
    trial = self._run_trial(func, catch, gc_after_trial)\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/study.py", line 799, in _run_trial\
    result = func(trial)\
  File "tuning.py", line 37, in tuning_objective\
    datasets = get_strat_folds(df_x, df_y, 5)\
  File "/Users/baconbaker/Documents/Studium/AML/Project/moa-pipeline/pipeline-remake/experimentation/crossvalidation.py", line 13, in get_strat_folds\
    for train_index, test_index in mlskf.split(features, targets):\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/model_selection/_split.py", line 336, in split\
    for train, test in super().split(X, y, groups):\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/model_selection/_split.py", line 80, in split\
    for test_index in self._iter_test_masks(X, y, groups):\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/iterstrat/ml_stratifiers.py", line 183, in _iter_test_masks\
    test_folds = self._make_test_folds(X, y)\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/iterstrat/ml_stratifiers.py", line 178, in _make_test_folds\
    test_folds = IterativeStratification(labels=y, r=r, random_state=rng)\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/iterstrat/ml_stratifiers.py", line 56, in IterativeStratification\
    num_labels = labels[labels_not_processed_mask].sum(axis=0)\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/numpy/core/_methods.py", line 38, in _sum\
    return umr_sum(a, axis, dtype, out, keepdims, initial, where)\
KeyboardInterrupt\
\
(ml) baconbaker@MacBook-Pro-2 experimentation % python tuning.py \
\cf10 [I 2020-11-05 16:14:28,053]\cf2  A new study created in memory with name: no-name-5d21368d-c2d7-45cd-852e-5052e658d5ff\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/trial/_trial.py:745: RuntimeWarning: Inconsistent parameter values for distribution with name "label_smoothing"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: \{'low': 1, 'high': 10, 'step': 1\}\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
2020-11-05 16:14:34.812250: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\
2020-11-05 16:14:34.828020: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fa887a7c0e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\
2020-11-05 16:14:34.828046: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\
Epoch 1/40\
88/88 [==============================] - 1s 13ms/step - loss: 1.0237 - val_loss: 0.5824\
Epoch 2/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.3435 - val_loss: 0.1361\
Epoch 3/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0804 - val_loss: 0.0502\
Epoch 4/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0428 - val_loss: 0.0339\
Epoch 5/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0333 - val_loss: 0.0295\
Epoch 6/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0291 - val_loss: 0.0270\
Epoch 7/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0266 - val_loss: 0.0255\
Epoch 8/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0253 - val_loss: 0.0245\
Epoch 9/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0243 - val_loss: 0.0238\
Epoch 10/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0236 - val_loss: 0.0232\
Epoch 11/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0231 - val_loss: 0.0228\
Epoch 12/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0227 - val_loss: 0.0224\
Epoch 13/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0224 - val_loss: 0.0222\
Epoch 14/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0222 - val_loss: 0.0220\
Epoch 15/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0219 - val_loss: 0.0218\
Epoch 16/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0218 - val_loss: 0.0217\
Epoch 17/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0216 - val_loss: 0.0216\
Epoch 18/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0215 - val_loss: 0.0214\
Epoch 19/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0215 - val_loss: 0.0213\
Epoch 20/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0214 - val_loss: 0.0213\
Epoch 21/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0213 - val_loss: 0.0212\
Epoch 22/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0212 - val_loss: 0.0211\
Epoch 23/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0212 - val_loss: 0.0211\
Epoch 24/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0211 - val_loss: 0.0210\
Epoch 25/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0210 - val_loss: 0.0210\
Epoch 26/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0209 - val_loss: 0.0209\
Epoch 27/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0209 - val_loss: 0.0208\
Epoch 28/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0208 - val_loss: 0.0208\
Epoch 29/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0209 - val_loss: 0.0208\
Epoch 30/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0208 - val_loss: 0.0208\
Epoch 31/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0208 - val_loss: 0.0208\
Epoch 32/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0208 - val_loss: 0.0208\
Epoch 33/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0208 - val_loss: 0.0208\
Epoch 34/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0208 - val_loss: 0.0208\
Epoch 35/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0208 - val_loss: 0.0208\
Epoch 36/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0208 - val_loss: 0.0208\
Epoch 37/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0208 - val_loss: 0.0208\
Epoch 38/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0208 - val_loss: 0.0208\
Epoch 39/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0208 - val_loss: 0.0208\
Fold 1: 0.020799514 loss, 0.74889153 auc\
Epoch 1/40\
88/88 [==============================] - 1s 12ms/step - loss: 1.1164 - val_loss: 0.6669\
Epoch 2/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.5223 - val_loss: 0.2688\
Epoch 3/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.1108 - val_loss: 0.0593\
Epoch 4/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0444 - val_loss: 0.0375\
Epoch 5/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0329 - val_loss: 0.0309\
Epoch 6/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0292 - val_loss: 0.0279\
Epoch 7/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0269 - val_loss: 0.0263\
Epoch 8/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0256 - val_loss: 0.0262\
Epoch 9/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0248 - val_loss: 0.0270\
Epoch 10/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0243 - val_loss: 0.0271\
Epoch 11/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0240 - val_loss: 0.0263\
Fold 2: 0.025357243 loss, 0.68766195 auc\
Epoch 1/40\
88/88 [==============================] - 1s 12ms/step - loss: 0.9773 - val_loss: 0.6360\
Epoch 2/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.4761 - val_loss: 0.2182\
Epoch 3/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.1201 - val_loss: 0.0639\
Epoch 4/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0491 - val_loss: 0.0400\
Epoch 5/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0356 - val_loss: 0.0323\
Epoch 6/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0305 - val_loss: 0.0287\
Epoch 7/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0278 - val_loss: 0.0267\
Epoch 8/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0261 - val_loss: 0.0253\
Epoch 9/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0250 - val_loss: 0.0245\
Epoch 10/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0243 - val_loss: 0.0238\
Epoch 11/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0238 - val_loss: 0.0234\
Epoch 12/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0233 - val_loss: 0.0230\
Epoch 13/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0231 - val_loss: 0.0228\
Epoch 14/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0227 - val_loss: 0.0225\
Epoch 15/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0225 - val_loss: 0.0222\
Epoch 16/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0223 - val_loss: 0.0220\
Epoch 17/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0222 - val_loss: 0.0219\
Epoch 18/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0220 - val_loss: 0.0217\
Epoch 19/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0218 - val_loss: 0.0215\
Epoch 20/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0216 - val_loss: 0.0214\
Epoch 21/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0216 - val_loss: 0.0213\
Epoch 22/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0214 - val_loss: 0.0212\
Epoch 23/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0213 - val_loss: 0.0211\
Epoch 24/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0212 - val_loss: 0.0210\
Epoch 25/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0212 - val_loss: 0.0210\
Epoch 26/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0211 - val_loss: 0.0209\
Epoch 27/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0210 - val_loss: 0.0208\
Epoch 28/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0210 - val_loss: 0.0207\
Epoch 29/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0209 - val_loss: 0.0207\
Epoch 30/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0209 - val_loss: 0.0207\
Epoch 31/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0209 - val_loss: 0.0206\
Epoch 32/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0208 - val_loss: 0.0206\
Epoch 33/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0208 - val_loss: 0.0206\
Epoch 34/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0208 - val_loss: 0.0206\
Epoch 35/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0208 - val_loss: 0.0206\
Epoch 36/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0208 - val_loss: 0.0206\
Epoch 37/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0208 - val_loss: 0.0206\
Epoch 38/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0208 - val_loss: 0.0206\
Epoch 39/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0208 - val_loss: 0.0206\
Epoch 40/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0208 - val_loss: 0.0206\
Fold 3: 0.020582994 loss, 0.7452928 auc\
Epoch 1/40\
88/88 [==============================] - 1s 13ms/step - loss: 1.0077 - val_loss: 0.5196\
Epoch 2/40\
88/88 [==============================] - 0s 4ms/step - loss: 0.3069 - val_loss: 0.1213\
Epoch 3/40\
88/88 [==============================] - 0s 4ms/step - loss: 0.0808 - val_loss: 0.0527\
Epoch 4/40\
88/88 [==============================] - 0s 4ms/step - loss: 0.0440 - val_loss: 0.0383\
Epoch 5/40\
88/88 [==============================] - 0s 4ms/step - loss: 0.0340 - val_loss: 0.0320\
Epoch 6/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0298 - val_loss: 0.0291\
Epoch 7/40\
88/88 [==============================] - 0s 4ms/step - loss: 0.0275 - val_loss: 0.0272\
Epoch 8/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0259 - val_loss: 0.0260\
Epoch 9/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0251 - val_loss: 0.0250\
Epoch 10/40\
88/88 [==============================] - 0s 4ms/step - loss: 0.0243 - val_loss: 0.0245\
Epoch 11/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0238 - val_loss: 0.0240\
Epoch 12/40\
88/88 [==============================] - 0s 4ms/step - loss: 0.0234 - val_loss: 0.0250\
Epoch 13/40\
88/88 [==============================] - 0s 4ms/step - loss: 0.0231 - val_loss: 0.0229\
Epoch 14/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0229 - val_loss: 0.0239\
Epoch 15/40\
88/88 [==============================] - 0s 4ms/step - loss: 0.0225 - val_loss: 0.0237\
Epoch 16/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0224 - val_loss: 0.0238\
Fold 4: 0.023477985 loss, 0.7096122 auc\
Epoch 1/40\
88/88 [==============================] - 1s 13ms/step - loss: 1.0654 - val_loss: 0.5132\
Epoch 2/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.3223 - val_loss: 0.1014\
Epoch 3/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0809 - val_loss: 0.0410\
Epoch 4/40\
88/88 [==============================] - 0s 4ms/step - loss: 0.0430 - val_loss: 0.0345\
Epoch 5/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0336 - val_loss: 0.0305\
Epoch 6/40\
88/88 [==============================] - 0s 4ms/step - loss: 0.0296 - val_loss: 0.0279\
Epoch 7/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0273 - val_loss: 0.0263\
Epoch 8/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0260 - val_loss: 0.0252\
Epoch 9/40\
88/88 [==============================] - 0s 4ms/step - loss: 0.0250 - val_loss: 0.0247\
Epoch 10/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0244 - val_loss: 0.0240\
Epoch 11/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0239 - val_loss: 0.0235\
Epoch 12/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0235 - val_loss: 0.0232\
Epoch 13/40\
88/88 [==============================] - 0s 4ms/step - loss: 0.0231 - val_loss: 0.0231\
Epoch 14/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0229 - val_loss: 0.0226\
Epoch 15/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0227 - val_loss: 0.0226\
Epoch 16/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0224 - val_loss: 0.0223\
Epoch 17/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0223 - val_loss: 0.0222\
Epoch 18/40\
88/88 [==============================] - 0s 4ms/step - loss: 0.0221 - val_loss: 0.0220\
Epoch 19/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0220 - val_loss: 0.0219\
Epoch 20/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0219 - val_loss: 0.0218\
Epoch 21/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0218 - val_loss: 0.0218\
Epoch 22/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0217 - val_loss: 0.0216\
Epoch 23/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0217 - val_loss: 0.0216\
Epoch 24/40\
88/88 [==============================] - 0s 4ms/step - loss: 0.0216 - val_loss: 0.0215\
Epoch 25/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0215 - val_loss: 0.0214\
Epoch 26/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0215 - val_loss: 0.0214\
Epoch 27/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0214 - val_loss: 0.0214\
Epoch 28/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0214 - val_loss: 0.0214\
Epoch 29/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0214 - val_loss: 0.0214\
Epoch 30/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0214 - val_loss: 0.0214\
Epoch 31/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0214 - val_loss: 0.0214\
Epoch 32/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0214 - val_loss: 0.0214\
Epoch 33/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0214 - val_loss: 0.0214\
Epoch 34/40\
88/88 [==============================] - 0s 4ms/step - loss: 0.0214 - val_loss: 0.0214\
Epoch 35/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0214 - val_loss: 0.0214\
Epoch 36/40\
88/88 [==============================] - 0s 4ms/step - loss: 0.0214 - val_loss: 0.0214\
Fold 5: 0.02142217 loss, 0.7417389 auc\
[0.020799514, 0.025357243, 0.020582994, 0.023477985, 0.02142217] [0.74889153, 0.68766195, 0.7452928, 0.7096122, 0.7417389]\
\cf10 [I 2020-11-05 16:15:46,217]\cf2  Trial 0 finished with value: 0.022327981144189834 and parameters: \{'label_smoothing': 5\}. Best is trial 0 with value: 0.022327981144189834.\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/trial/_trial.py:745: RuntimeWarning: Inconsistent parameter values for distribution with name "label_smoothing"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: \{'low': 1, 'high': 10, 'step': 1\}\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 1s 13ms/step - loss: 0.9644 - val_loss: 0.5324\
Epoch 2/40\
88/88 [==============================] - 0s 4ms/step - loss: 0.3151 - val_loss: 0.1093\
Epoch 3/40\
88/88 [==============================] - 0s 4ms/step - loss: 0.0742 - val_loss: 0.0479\
Epoch 4/40\
88/88 [==============================] - 0s 4ms/step - loss: 0.0412 - val_loss: 0.0349\
Epoch 5/40\
88/88 [==============================] - 0s 4ms/step - loss: 0.0324 - val_loss: 0.0299\
Epoch 6/40\
88/88 [==============================] - 0s 3ms/step - loss: 0.0286 - val_loss: 0.0274\
Epoch 7/40\
88/88 [==============================] - 0s 5ms/step - loss: 0.0266 - val_loss: 0.0259\
Epoch 8/40\
88/88 [==============================] - 0s 4ms/step - loss: 0.0254 - val_loss: 0.0249\
Epoch 9/40\
88/88 [==============================] - 0s 4ms/step - loss: 0.0246 - val_loss: 0.0242\
Epoch 10/40\
88/88 [==============================] - 0s 4ms/step - loss: 0.0240 - val_loss: 0.0237\
Epoch 11/40\
88/88 [==============================] - 0s 4ms/step - loss: 0.0235 - val_loss: 0.0232\
Epoch 12/40\
88/88 [==============================] - 0s 4ms/step - loss: 0.0230 - val_loss: 0.0228\
Epoch 13/40\
88/88 [==============================] - 0s 5ms/step - loss: 0.0227 - val_loss: 0.0225\
Epoch 14/40\
88/88 [==============================] - 0s 4ms/step - loss: 0.0224 - val_loss: 0.0223\
Epoch 15/40\
88/88 [==============================] - 0s 4ms/step - loss: 0.0222 - val_loss: 0.0221\
Epoch 16/40\
88/88 [==============================] - 0s 4ms/step - loss: 0.0221 - val_loss: 0.0219\
Epoch 17/40\
88/88 [==============================] - 0s 4ms/step - loss: 0.0219 - val_loss: 0.0218\
Epoch 18/40\
88/88 [==============================] - 0s 4ms/step - loss: 0.0218 - val_loss: 0.0217\
Epoch 19/40\
88/88 [==============================] - 0s 4ms/step - loss: 0.0217 - val_loss: 0.0216\
Epoch 20/40\
88/88 [==============================] - 0s 4ms/step - loss: 0.0215 - val_loss: 0.0214\
Epoch 21/40\
88/88 [==============================] - 0s 4ms/step - loss: 0.0214 - val_loss: 0.0214\
Epoch 22/40\
88/88 [==============================] - 0s 4ms/step - loss: 0.0214 - val_loss: 0.0213\
Epoch 23/40\
88/88 [==============================] - 0s 4ms/step - loss: 0.0213 - val_loss: 0.0213\
Epoch 24/40\
88/88 [==============================] - 0s 4ms/step - loss: 0.0212 - val_loss: 0.0212\
Epoch 25/40\
88/88 [==============================] - 0s 4ms/step - loss: 0.0212 - val_loss: 0.0212\
Epoch 26/40\
88/88 [==============================] - 0s 4ms/step - loss: 0.0211 - val_loss: 0.0211\
Epoch 27/40\
88/88 [==============================] - 0s 4ms/step - loss: 0.0211 - val_loss: 0.0210\
Epoch 28/40\
88/88 [==============================] - 0s 4ms/step - loss: 0.0210 - val_loss: 0.0210\
Epoch 29/40\
88/88 [==============================] - 0s 4ms/step - loss: 0.0210 - val_loss: 0.0209\
Epoch 30/40\
88/88 [==============================] - 0s 4ms/step - loss: 0.0209 - val_loss: 0.0209\
Epoch 31/40\
88/88 [==============================] - 0s 4ms/step - loss: 0.0209 - val_loss: 0.0209\
Epoch 32/40\
88/88 [==============================] - 0s 4ms/step - loss: 0.0209 - val_loss: 0.0209\
Epoch 33/40\
88/88 [==============================] - 0s 4ms/step - loss: 0.0209 - val_loss: 0.0209\
Epoch 34/40\
88/88 [==============================] - 0s 4ms/step - loss: 0.0209 - val_loss: 0.0209\
Epoch 35/40\
88/88 [==============================] - 0s 4ms/step - loss: 0.0209 - val_loss: 0.0209\
Epoch 36/40\
88/88 [==============================] - 0s 4ms/step - loss: 0.0209 - val_loss: 0.0209\
Epoch 37/40\
69/88 [======================>.......] - ETA: 0s - loss: 0.0209^CTraceback (most recent call last):\
  File "tuning.py", line 67, in <module>\
    param_tuning()\
  File "tuning.py", line 62, in param_tuning\
    study.optimize(tuning_objective, n_trials=150)\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/study.py", line 338, in optimize\
    self._optimize_sequential(\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/study.py", line 747, in _optimize_sequential\
    self._run_trial_and_callbacks(func, catch, callbacks, gc_after_trial)\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/study.py", line 776, in _run_trial_and_callbacks\
    trial = self._run_trial(func, catch, gc_after_trial)\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/study.py", line 799, in _run_trial\
    result = func(trial)\
  File "tuning.py", line 47, in tuning_objective\
    myModel.run_training(train_x, train_y, test_x, test_y)\
  File "../models/arch_base.py", line 47, in run_training\
    history = self.model.fit(\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py", line 108, in _method_wrapper\
    return method(self, *args, **kwargs)\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py", line 1098, in fit\
    tmp_logs = train_function(iterator)\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py", line 780, in __call__\
    result = self._call(*args, **kwds)\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py", line 807, in _call\
    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 2829, in __call__\
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 1843, in _filtered_call\
    return self._call_flat(\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 1923, in _call_flat\
    return self._build_call_outputs(self._inference_function.call(\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 545, in call\
    outputs = execute.execute(\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/tensorflow/python/eager/execute.py", line 59, in quick_execute\
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\
KeyboardInterrupt\
\
(ml) baconbaker@MacBook-Pro-2 experimentation % python tuning.py \
\cf10 [I 2020-11-05 16:16:21,214]\cf2  A new study created in memory with name: no-name-f39cab38-445a-4bc2-be4c-3fd2bf0a484d\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
2020-11-05 16:16:28.139550: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\
2020-11-05 16:16:28.152803: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fb41bda92c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\
2020-11-05 16:16:28.152821: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\
Epoch 1/40\
88/88 [==============================] - 19s 214ms/step - loss: 0.3141 - val_loss: 0.1022\
Epoch 2/40\
88/88 [==============================] - 17s 197ms/step - loss: 0.0295 - val_loss: 0.0287\
Epoch 3/40\
88/88 [==============================] - 18s 208ms/step - loss: 0.0212 - val_loss: 0.0201\
Epoch 4/40\
88/88 [==============================] - 18s 199ms/step - loss: 0.0189 - val_loss: 0.0188\
Epoch 5/40\
88/88 [==============================] - 18s 202ms/step - loss: 0.0174 - val_loss: 0.0185\
Epoch 6/40\
88/88 [==============================] - 19s 218ms/step - loss: 0.0163 - val_loss: 0.0182\
Epoch 7/40\
88/88 [==============================] - 20s 224ms/step - loss: 0.0151 - val_loss: 0.0185\
Epoch 8/40\
88/88 [==============================] - 19s 213ms/step - loss: 0.0140 - val_loss: 0.0191\
Epoch 9/40\
88/88 [==============================] - 20s 231ms/step - loss: 0.0119 - val_loss: 0.0180\
Epoch 10/40\
88/88 [==============================] - 20s 225ms/step - loss: 0.0105 - val_loss: 0.0182\
Epoch 11/40\
88/88 [==============================] - 19s 212ms/step - loss: 0.0095 - val_loss: 0.0184\
Epoch 12/40\
88/88 [==============================] - 20s 222ms/step - loss: 0.0087 - val_loss: 0.0184\
Fold 1: 0.01841057 loss, 0.7435216 auc\
Epoch 1/40\
88/88 [==============================] - 19s 219ms/step - loss: 0.3136 - val_loss: 0.0507\
Epoch 2/40\
88/88 [==============================] - 18s 209ms/step - loss: 0.0293 - val_loss: 0.0256\
Epoch 3/40\
88/88 [==============================] - 19s 213ms/step - loss: 0.0213 - val_loss: 0.0203\
Epoch 4/40\
88/88 [==============================] - 18s 210ms/step - loss: 0.0189 - val_loss: 0.0191\
Epoch 5/40\
88/88 [==============================] - 19s 215ms/step - loss: 0.0176 - val_loss: 0.0185\
Epoch 6/40\
88/88 [==============================] - 18s 209ms/step - loss: 0.0165 - val_loss: 0.0180\
Epoch 7/40\
88/88 [==============================] - 19s 211ms/step - loss: 0.0156 - val_loss: 0.0201\
Epoch 8/40\
88/88 [==============================] - 19s 215ms/step - loss: 0.0146 - val_loss: 0.0184\
Epoch 9/40\
88/88 [==============================] - 19s 218ms/step - loss: 0.0124 - val_loss: 0.0177\
Epoch 10/40\
88/88 [==============================] - 19s 212ms/step - loss: 0.0112 - val_loss: 0.0179\
Epoch 11/40\
88/88 [==============================] - 18s 208ms/step - loss: 0.0103 - val_loss: 0.0181\
Epoch 12/40\
88/88 [==============================] - 18s 206ms/step - loss: 0.0095 - val_loss: 0.0181\
Fold 2: 0.018065184 loss, 0.753673 auc\
Epoch 1/40\
88/88 [==============================] - 18s 200ms/step - loss: 0.3138 - val_loss: 0.0660\
Epoch 2/40\
88/88 [==============================] - 17s 192ms/step - loss: 0.0294 - val_loss: 0.0253\
Epoch 3/40\
88/88 [==============================] - 16s 186ms/step - loss: 0.0213 - val_loss: 0.0200\
Epoch 4/40\
88/88 [==============================] - 17s 192ms/step - loss: 0.0189 - val_loss: 0.0190\
Epoch 5/40\
88/88 [==============================] - 17s 195ms/step - loss: 0.0176 - val_loss: 0.0180\
Epoch 6/40\
88/88 [==============================] - 16s 183ms/step - loss: 0.0164 - val_loss: 0.0177\
Epoch 7/40\
88/88 [==============================] - 16s 184ms/step - loss: 0.0153 - val_loss: 0.0180\
Epoch 8/40\
88/88 [==============================] - 17s 188ms/step - loss: 0.0142 - val_loss: 0.0186\
Epoch 9/40\
88/88 [==============================] - 16s 181ms/step - loss: 0.0120 - val_loss: 0.0178\
Fold 3: 0.01775909 loss, 0.7588347 auc\
Epoch 1/40\
88/88 [==============================] - 18s 203ms/step - loss: 0.3123 - val_loss: 0.0527\
Epoch 2/40\
88/88 [==============================] - 17s 195ms/step - loss: 0.0295 - val_loss: 0.0240\
Epoch 3/40\
88/88 [==============================] - 17s 193ms/step - loss: 0.0212 - val_loss: 0.0203\
Epoch 4/40\
88/88 [==============================] - 17s 197ms/step - loss: 0.0188 - val_loss: 0.0186\
Epoch 5/40\
88/88 [==============================] - 16s 182ms/step - loss: 0.0175 - val_loss: 0.0184\
Epoch 6/40\
88/88 [==============================] - 17s 192ms/step - loss: 0.0164 - val_loss: 0.0181\
Epoch 7/40\
88/88 [==============================] - 17s 193ms/step - loss: 0.0154 - val_loss: 0.0186\
Epoch 8/40\
88/88 [==============================] - 17s 188ms/step - loss: 0.0142 - val_loss: 0.0190\
Epoch 9/40\
88/88 [==============================] - 17s 196ms/step - loss: 0.0120 - val_loss: 0.0179\
Epoch 10/40\
88/88 [==============================] - 18s 203ms/step - loss: 0.0108 - val_loss: 0.0180\
Epoch 11/40\
88/88 [==============================] - 18s 202ms/step - loss: 0.0099 - val_loss: 0.0181\
Epoch 12/40\
88/88 [==============================] - 17s 194ms/step - loss: 0.0092 - val_loss: 0.0181\
Fold 4: 0.018103018 loss, 0.7553012 auc\
Epoch 1/40\
88/88 [==============================] - 17s 190ms/step - loss: 0.3136 - val_loss: 0.0744\
Epoch 2/40\
88/88 [==============================] - 16s 176ms/step - loss: 0.0291 - val_loss: 0.0233\
Epoch 3/40\
88/88 [==============================] - 17s 188ms/step - loss: 0.0211 - val_loss: 0.0202\
Epoch 4/40\
88/88 [==============================] - 17s 188ms/step - loss: 0.0188 - val_loss: 0.0189\
Epoch 5/40\
88/88 [==============================] - 17s 188ms/step - loss: 0.0175 - val_loss: 0.0184\
Epoch 6/40\
88/88 [==============================] - 17s 191ms/step - loss: 0.0164 - val_loss: 0.0183\
Epoch 7/40\
88/88 [==============================] - 19s 211ms/step - loss: 0.0152 - val_loss: 0.0184\
Epoch 8/40\
88/88 [==============================] - 18s 202ms/step - loss: 0.0141 - val_loss: 0.0190\
Epoch 9/40\
88/88 [==============================] - 17s 198ms/step - loss: 0.0118 - val_loss: 0.0180\
Epoch 10/40\
88/88 [==============================] - 17s 192ms/step - loss: 0.0103 - val_loss: 0.0182\
Epoch 11/40\
88/88 [==============================] - 17s 196ms/step - loss: 0.0095 - val_loss: 0.0184\
Epoch 12/40\
88/88 [==============================] - 17s 189ms/step - loss: 0.0086 - val_loss: 0.0183\
Fold 5: 0.018324608 loss, 0.744874 auc\
[0.01841057, 0.018065184, 0.01775909, 0.018103018, 0.018324608] [0.7435216, 0.753673, 0.7588347, 0.7553012, 0.744874]\
\cf10 [I 2020-11-05 16:34:55,080]\cf2  Trial 0 finished with value: 0.018132494390010835 and parameters: \{'layers': 7, 'neurons': 1331\}. Best is trial 0 with value: 0.018132494390010835.\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 5s 61ms/step - loss: 0.3575 - val_loss: 0.1140\
Epoch 2/40\
88/88 [==============================] - 4s 51ms/step - loss: 0.0363 - val_loss: 0.0259\
Epoch 3/40\
88/88 [==============================] - 5s 54ms/step - loss: 0.0232 - val_loss: 0.0216\
Epoch 4/40\
88/88 [==============================] - 5s 56ms/step - loss: 0.0197 - val_loss: 0.0195\
Epoch 5/40\
88/88 [==============================] - 5s 56ms/step - loss: 0.0179 - val_loss: 0.0187\
Epoch 6/40\
88/88 [==============================] - 5s 56ms/step - loss: 0.0163 - val_loss: 0.0183\
Epoch 7/40\
88/88 [==============================] - 5s 58ms/step - loss: 0.0148 - val_loss: 0.0182\
Epoch 8/40\
88/88 [==============================] - 6s 63ms/step - loss: 0.0130 - val_loss: 0.0185\
Epoch 9/40\
88/88 [==============================] - 5s 59ms/step - loss: 0.0111 - val_loss: 0.0187\
Epoch 10/40\
88/88 [==============================] - 5s 62ms/step - loss: 0.0085 - val_loss: 0.0184\
Fold 1: 0.01836379 loss, 0.7550674 auc\
Epoch 1/40\
88/88 [==============================] - 6s 63ms/step - loss: 0.3578 - val_loss: 0.0582\
Epoch 2/40\
88/88 [==============================] - 5s 53ms/step - loss: 0.0367 - val_loss: 0.0277\
Epoch 3/40\
88/88 [==============================] - 5s 56ms/step - loss: 0.0235 - val_loss: 0.0217\
Epoch 4/40\
88/88 [==============================] - 5s 53ms/step - loss: 0.0199 - val_loss: 0.0196\
Epoch 5/40\
88/88 [==============================] - 5s 56ms/step - loss: 0.0182 - val_loss: 0.0185\
Epoch 6/40\
88/88 [==============================] - 5s 54ms/step - loss: 0.0167 - val_loss: 0.0183\
Epoch 7/40\
88/88 [==============================] - 5s 53ms/step - loss: 0.0151 - val_loss: 0.0180\
Epoch 8/40\
88/88 [==============================] - 5s 54ms/step - loss: 0.0136 - val_loss: 0.0182\
Epoch 9/40\
88/88 [==============================] - 5s 56ms/step - loss: 0.0119 - val_loss: 0.0185\
Epoch 10/40\
88/88 [==============================] - 5s 55ms/step - loss: 0.0092 - val_loss: 0.0180\
Epoch 11/40\
88/88 [==============================] - 5s 56ms/step - loss: 0.0080 - val_loss: 0.0181\
Epoch 12/40\
88/88 [==============================] - 5s 59ms/step - loss: 0.0074 - val_loss: 0.0181\
Epoch 13/40\
88/88 [==============================] - 5s 56ms/step - loss: 0.0073 - val_loss: 0.0181\
Fold 2: 0.018130383 loss, 0.7617821 auc\
Epoch 1/40\
88/88 [==============================] - 6s 68ms/step - loss: 0.3592 - val_loss: 0.0602\
Epoch 2/40\
88/88 [==============================] - 5s 53ms/step - loss: 0.0367 - val_loss: 0.0265\
Epoch 3/40\
88/88 [==============================] - 5s 54ms/step - loss: 0.0232 - val_loss: 0.0212\
Epoch 4/40\
88/88 [==============================] - 5s 56ms/step - loss: 0.0197 - val_loss: 0.0194\
Epoch 5/40\
88/88 [==============================] - 5s 56ms/step - loss: 0.0179 - val_loss: 0.0182\
Epoch 6/40\
88/88 [==============================] - 5s 56ms/step - loss: 0.0164 - val_loss: 0.0180\
Epoch 7/40\
88/88 [==============================] - 5s 55ms/step - loss: 0.0149 - val_loss: 0.0178\
Epoch 8/40\
88/88 [==============================] - 5s 56ms/step - loss: 0.0133 - val_loss: 0.0182\
Epoch 9/40\
88/88 [==============================] - 5s 54ms/step - loss: 0.0115 - val_loss: 0.0184\
Epoch 10/40\
88/88 [==============================] - 5s 54ms/step - loss: 0.0090 - val_loss: 0.0180\
Fold 3: 0.018005202 loss, 0.76419955 auc\
Epoch 1/40\
88/88 [==============================] - 6s 64ms/step - loss: 0.3580 - val_loss: 0.0606\
Epoch 2/40\
88/88 [==============================] - 5s 52ms/step - loss: 0.0368 - val_loss: 0.0272\
Epoch 3/40\
88/88 [==============================] - 5s 56ms/step - loss: 0.0234 - val_loss: 0.0215\
Epoch 4/40\
88/88 [==============================] - 5s 53ms/step - loss: 0.0198 - val_loss: 0.0194\
Epoch 5/40\
88/88 [==============================] - 5s 54ms/step - loss: 0.0179 - val_loss: 0.0185\
Epoch 6/40\
88/88 [==============================] - 5s 56ms/step - loss: 0.0164 - val_loss: 0.0179\
Epoch 7/40\
88/88 [==============================] - 5s 56ms/step - loss: 0.0148 - val_loss: 0.0181\
Epoch 8/40\
88/88 [==============================] - 5s 54ms/step - loss: 0.0133 - val_loss: 0.0182\
Epoch 9/40\
88/88 [==============================] - 5s 60ms/step - loss: 0.0109 - val_loss: 0.0179\
Fold 4: 0.017873492 loss, 0.7679586 auc\
Epoch 1/40\
88/88 [==============================] - 6s 65ms/step - loss: 0.3574 - val_loss: 0.0582\
Epoch 2/40\
88/88 [==============================] - 5s 55ms/step - loss: 0.0367 - val_loss: 0.0266\
Epoch 3/40\
88/88 [==============================] - 5s 56ms/step - loss: 0.0233 - val_loss: 0.0217\
Epoch 4/40\
88/88 [==============================] - 5s 57ms/step - loss: 0.0198 - val_loss: 0.0198\
Epoch 5/40\
88/88 [==============================] - 5s 57ms/step - loss: 0.0179 - val_loss: 0.0187\
Epoch 6/40\
88/88 [==============================] - 5s 56ms/step - loss: 0.0163 - val_loss: 0.0181\
Epoch 7/40\
88/88 [==============================] - 5s 61ms/step - loss: 0.0148 - val_loss: 0.0182\
Epoch 8/40\
88/88 [==============================] - 5s 54ms/step - loss: 0.0132 - val_loss: 0.0184\
Epoch 9/40\
88/88 [==============================] - 5s 60ms/step - loss: 0.0106 - val_loss: 0.0180\
Epoch 10/40\
88/88 [==============================] - 5s 57ms/step - loss: 0.0094 - val_loss: 0.0180\
Epoch 11/40\
88/88 [==============================] - 5s 58ms/step - loss: 0.0088 - val_loss: 0.0180\
Epoch 12/40\
88/88 [==============================] - 5s 57ms/step - loss: 0.0087 - val_loss: 0.0180\
Fold 5: 0.018040827 loss, 0.76899105 auc\
[0.01836379, 0.018130383, 0.018005202, 0.017873492, 0.018040827] [0.7550674, 0.7617821, 0.76419955, 0.7679586, 0.76899105]\
\cf10 [I 2020-11-05 16:40:13,707]\cf2  Trial 1 finished with value: 0.018082738667726517 and parameters: \{'layers': 4, 'neurons': 739\}. Best is trial 1 with value: 0.018082738667726517.\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 9s 99ms/step - loss: 0.3100 - val_loss: 0.0448\
Epoch 2/40\
88/88 [==============================] - 8s 87ms/step - loss: 0.0275 - val_loss: 0.0230\
Epoch 3/40\
88/88 [==============================] - 8s 92ms/step - loss: 0.0202 - val_loss: 0.0194\
Epoch 4/40\
88/88 [==============================] - 9s 97ms/step - loss: 0.0178 - val_loss: 0.0183\
Epoch 5/40\
88/88 [==============================] - 8s 91ms/step - loss: 0.0161 - val_loss: 0.0177\
Epoch 6/40\
88/88 [==============================] - 8s 92ms/step - loss: 0.0142 - val_loss: 0.0178\
Epoch 7/40\
88/88 [==============================] - 8s 93ms/step - loss: 0.0118 - val_loss: 0.0178\
Epoch 8/40\
88/88 [==============================] - 8s 90ms/step - loss: 0.0085 - val_loss: 0.0176\
Epoch 9/40\
88/88 [==============================] - 8s 88ms/step - loss: 0.0072 - val_loss: 0.0176\
Epoch 10/40\
88/88 [==============================] - 8s 87ms/step - loss: 0.0064 - val_loss: 0.0176\
Epoch 11/40\
88/88 [==============================] - 8s 86ms/step - loss: 0.0058 - val_loss: 0.0176\
Fold 1: 0.017599214 loss, 0.7745152 auc\
Epoch 1/40\
88/88 [==============================] - 8s 94ms/step - loss: 0.3076 - val_loss: 0.0362\
Epoch 2/40\
88/88 [==============================] - 8s 88ms/step - loss: 0.0275 - val_loss: 0.0225\
Epoch 3/40\
88/88 [==============================] - 8s 89ms/step - loss: 0.0203 - val_loss: 0.0193\
Epoch 4/40\
88/88 [==============================] - 8s 95ms/step - loss: 0.0179 - val_loss: 0.0182\
Epoch 5/40\
88/88 [==============================] - 9s 99ms/step - loss: 0.0161 - val_loss: 0.0178\
Epoch 6/40\
88/88 [==============================] - 8s 95ms/step - loss: 0.0143 - val_loss: 0.0176\
Epoch 7/40\
88/88 [==============================] - 8s 89ms/step - loss: 0.0117 - val_loss: 0.0182\
Epoch 8/40\
88/88 [==============================] - 8s 90ms/step - loss: 0.0091 - val_loss: 0.0186\
Epoch 9/40\
88/88 [==============================] - 8s 89ms/step - loss: 0.0060 - val_loss: 0.0180\
Fold 2: 0.018035175 loss, 0.75495327 auc\
Epoch 1/40\
88/88 [==============================] - 8s 90ms/step - loss: 0.3105 - val_loss: 0.0400\
Epoch 2/40\
88/88 [==============================] - 7s 85ms/step - loss: 0.0275 - val_loss: 0.0225\
Epoch 3/40\
88/88 [==============================] - 7s 82ms/step - loss: 0.0203 - val_loss: 0.0197\
Epoch 4/40\
88/88 [==============================] - 8s 87ms/step - loss: 0.0181 - val_loss: 0.0180\
Epoch 5/40\
88/88 [==============================] - 8s 89ms/step - loss: 0.0162 - val_loss: 0.0175\
Epoch 6/40\
88/88 [==============================] - 7s 83ms/step - loss: 0.0145 - val_loss: 0.0176\
Epoch 7/40\
88/88 [==============================] - 7s 83ms/step - loss: 0.0121 - val_loss: 0.0178\
Epoch 8/40\
88/88 [==============================] - 8s 88ms/step - loss: 0.0088 - val_loss: 0.0174\
Epoch 9/40\
88/88 [==============================] - 7s 85ms/step - loss: 0.0074 - val_loss: 0.0174\
Epoch 10/40\
88/88 [==============================] - 7s 84ms/step - loss: 0.0066 - val_loss: 0.0175\
Epoch 11/40\
88/88 [==============================] - 7s 85ms/step - loss: 0.0059 - val_loss: 0.0174\
Fold 3: 0.017441934 loss, 0.7777762 auc\
Epoch 1/40\
88/88 [==============================] - 9s 98ms/step - loss: 0.3097 - val_loss: 0.0424\
Epoch 2/40\
83/88 [===========================>..] - ETA: 0s - loss: 0.0277^CTraceback (most recent call last):\
  File "tuning.py", line 67, in <module>\
    param_tuning()\
  File "tuning.py", line 62, in param_tuning\
    study.optimize(tuning_objective, n_trials=150)\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/study.py", line 338, in optimize\
    self._optimize_sequential(\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/study.py", line 747, in _optimize_sequential\
    self._run_trial_and_callbacks(func, catch, callbacks, gc_after_trial)\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/study.py", line 776, in _run_trial_and_callbacks\
    trial = self._run_trial(func, catch, gc_after_trial)\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/study.py", line 799, in _run_trial\
    result = func(trial)\
  File "tuning.py", line 47, in tuning_objective\
    myModel.run_training(train_x, train_y, test_x, test_y)\
  File "../models/arch_base.py", line 47, in run_training\
    history = self.model.fit(\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py", line 108, in _method_wrapper\
    return method(self, *args, **kwargs)\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py", line 1098, in fit\
    tmp_logs = train_function(iterator)\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py", line 780, in __call__\
    result = self._call(*args, **kwds)\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py", line 807, in _call\
    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 2829, in __call__\
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 1843, in _filtered_call\
    return self._call_flat(\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 1923, in _call_flat\
    return self._build_call_outputs(self._inference_function.call(\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 545, in call\
    outputs = execute.execute(\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/tensorflow/python/eager/execute.py", line 59, in quick_execute\
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\
KeyboardInterrupt\
\
(ml) baconbaker@MacBook-Pro-2 experimentation % python tuning.py \
\cf10 [I 2020-11-05 16:45:15,238]\cf2  A new study created in memory with name: no-name-79a487ce-ab72-44a7-85f9-1235019f825e\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
2020-11-05 16:45:22.310454: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\
2020-11-05 16:45:22.325657: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fec26724e90 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\
2020-11-05 16:45:22.325675: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\
Epoch 1/40\
88/88 [==============================] - 12s 137ms/step - loss: 0.3160 - val_loss: 0.0466\
Epoch 2/40\
88/88 [==============================] - 11s 130ms/step - loss: 0.0290 - val_loss: 0.0240\
Epoch 3/40\
88/88 [==============================] - 12s 138ms/step - loss: 0.0209 - val_loss: 0.0197\
Epoch 4/40\
88/88 [==============================] - 13s 151ms/step - loss: 0.0185 - val_loss: 0.0188\
Epoch 5/40\
88/88 [==============================] - 12s 141ms/step - loss: 0.0169 - val_loss: 0.0178\
Epoch 6/40\
88/88 [==============================] - 12s 139ms/step - loss: 0.0155 - val_loss: 0.0178\
Epoch 7/40\
88/88 [==============================] - 13s 143ms/step - loss: 0.0139 - val_loss: 0.0180\
Epoch 8/40\
88/88 [==============================] - 13s 152ms/step - loss: 0.0113 - val_loss: 0.0177\
Epoch 9/40\
88/88 [==============================] - 13s 152ms/step - loss: 0.0098 - val_loss: 0.0178\
Epoch 10/40\
88/88 [==============================] - 13s 145ms/step - loss: 0.0088 - val_loss: 0.0180\
Epoch 11/40\
88/88 [==============================] - 12s 141ms/step - loss: 0.0080 - val_loss: 0.0179\
Fold 1: 0.017910186 loss, 0.7648963 auc\
Epoch 1/40\
88/88 [==============================] - 12s 136ms/step - loss: 0.3111 - val_loss: 0.0436\
Epoch 2/40\
88/88 [==============================] - 11s 130ms/step - loss: 0.0292 - val_loss: 0.0243\
Epoch 3/40\
88/88 [==============================] - 11s 126ms/step - loss: 0.0211 - val_loss: 0.0202\
Epoch 4/40\
88/88 [==============================] - 11s 130ms/step - loss: 0.0186 - val_loss: 0.0185\
Epoch 5/40\
88/88 [==============================] - 12s 131ms/step - loss: 0.0171 - val_loss: 0.0179\
Epoch 6/40\
88/88 [==============================] - 11s 130ms/step - loss: 0.0155 - val_loss: 0.0179\
Epoch 7/40\
88/88 [==============================] - 11s 127ms/step - loss: 0.0142 - val_loss: 0.0181\
Epoch 8/40\
88/88 [==============================] - 11s 128ms/step - loss: 0.0115 - val_loss: 0.0177\
Epoch 9/40\
88/88 [==============================] - 12s 132ms/step - loss: 0.0101 - val_loss: 0.0179\
Epoch 10/40\
88/88 [==============================] - 11s 128ms/step - loss: 0.0091 - val_loss: 0.0180\
Epoch 11/40\
88/88 [==============================] - 11s 125ms/step - loss: 0.0083 - val_loss: 0.0180\
Fold 2: 0.017965041 loss, 0.7722014 auc\
Epoch 1/40\
88/88 [==============================] - 12s 134ms/step - loss: 0.3134 - val_loss: 0.0449\
Epoch 2/40\
88/88 [==============================] - 11s 126ms/step - loss: 0.0290 - val_loss: 0.0265\
Epoch 3/40\
88/88 [==============================] - 11s 125ms/step - loss: 0.0209 - val_loss: 0.0196\
Epoch 4/40\
88/88 [==============================] - 11s 125ms/step - loss: 0.0184 - val_loss: 0.0186\
Epoch 5/40\
88/88 [==============================] - 11s 123ms/step - loss: 0.0170 - val_loss: 0.0177\
Epoch 6/40\
88/88 [==============================] - 11s 122ms/step - loss: 0.0155 - val_loss: 0.0177\
Epoch 7/40\
88/88 [==============================] - 11s 125ms/step - loss: 0.0140 - val_loss: 0.0179\
Epoch 8/40\
88/88 [==============================] - 11s 123ms/step - loss: 0.0113 - val_loss: 0.0176\
Epoch 9/40\
88/88 [==============================] - 11s 124ms/step - loss: 0.0099 - val_loss: 0.0177\
Epoch 10/40\
88/88 [==============================] - 11s 123ms/step - loss: 0.0089 - val_loss: 0.0178\
Epoch 11/40\
88/88 [==============================] - 11s 123ms/step - loss: 0.0081 - val_loss: 0.0177\
Fold 3: 0.017743858 loss, 0.77412474 auc\
Epoch 1/40\
88/88 [==============================] - 12s 132ms/step - loss: 0.3117 - val_loss: 0.0444\
Epoch 2/40\
88/88 [==============================] - 11s 120ms/step - loss: 0.0290 - val_loss: 0.0234\
Epoch 3/40\
88/88 [==============================] - 11s 121ms/step - loss: 0.0209 - val_loss: 0.0200\
Epoch 4/40\
88/88 [==============================] - 11s 121ms/step - loss: 0.0184 - val_loss: 0.0184\
Epoch 5/40\
88/88 [==============================] - 10s 118ms/step - loss: 0.0168 - val_loss: 0.0179\
Epoch 6/40\
88/88 [==============================] - 11s 120ms/step - loss: 0.0154 - val_loss: 0.0177\
Epoch 7/40\
88/88 [==============================] - 10s 119ms/step - loss: 0.0139 - val_loss: 0.0183\
Epoch 8/40\
88/88 [==============================] - 11s 120ms/step - loss: 0.0118 - val_loss: 0.0187\
Epoch 9/40\
88/88 [==============================] - 11s 120ms/step - loss: 0.0089 - val_loss: 0.0181\
Fold 4: 0.018068273 loss, 0.759415 auc\
Epoch 1/40\
88/88 [==============================] - 11s 124ms/step - loss: 0.3126 - val_loss: 0.0544\
Epoch 2/40\
88/88 [==============================] - 10s 119ms/step - loss: 0.0288 - val_loss: 0.0233\
Epoch 3/40\
88/88 [==============================] - 11s 120ms/step - loss: 0.0208 - val_loss: 0.0194\
Epoch 4/40\
88/88 [==============================] - 10s 118ms/step - loss: 0.0183 - val_loss: 0.0183\
Epoch 5/40\
88/88 [==============================] - 11s 120ms/step - loss: 0.0168 - val_loss: 0.0179\
Epoch 6/40\
88/88 [==============================] - 10s 119ms/step - loss: 0.0153 - val_loss: 0.0177\
Epoch 7/40\
88/88 [==============================] - 10s 119ms/step - loss: 0.0137 - val_loss: 0.0185\
Epoch 8/40\
88/88 [==============================] - 10s 117ms/step - loss: 0.0120 - val_loss: 0.0185\
Epoch 9/40\
88/88 [==============================] - 11s 122ms/step - loss: 0.0090 - val_loss: 0.0181\
Fold 5: 0.01806464 loss, 0.75905794 auc\
[0.017910186, 0.017965041, 0.017743858, 0.018068273, 0.01806464] [0.7648963, 0.7722014, 0.77412474, 0.759415, 0.75905794]\
\cf10 [I 2020-11-05 16:56:03,692]\cf2  Trial 0 finished with value: 0.017950399592518807 and parameters: \{'layers': 4, 'neurons': 1362\}. Best is trial 0 with value: 0.017950399592518807.\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 12s 139ms/step - loss: 0.3012 - val_loss: 0.0366\
Epoch 2/40\
88/88 [==============================] - 12s 132ms/step - loss: 0.0267 - val_loss: 0.0220\
Epoch 3/40\
88/88 [==============================] - 12s 131ms/step - loss: 0.0202 - val_loss: 0.0196\
Epoch 4/40\
88/88 [==============================] - 12s 132ms/step - loss: 0.0178 - val_loss: 0.0181\
Epoch 5/40\
88/88 [==============================] - 11s 129ms/step - loss: 0.0162 - val_loss: 0.0177\
Epoch 6/40\
88/88 [==============================] - 12s 131ms/step - loss: 0.0145 - val_loss: 0.0178\
Epoch 7/40\
88/88 [==============================] - 11s 128ms/step - loss: 0.0125 - val_loss: 0.0180\
Epoch 8/40\
88/88 [==============================] - 11s 128ms/step - loss: 0.0092 - val_loss: 0.0177\
Epoch 9/40\
88/88 [==============================] - 11s 129ms/step - loss: 0.0077 - val_loss: 0.0178\
Epoch 10/40\
88/88 [==============================] - 12s 132ms/step - loss: 0.0069 - val_loss: 0.0178\
Epoch 11/40\
88/88 [==============================] - 12s 132ms/step - loss: 0.0068 - val_loss: 0.0178\
Fold 1: 0.017789576 loss, 0.77474594 auc\
Epoch 1/40\
88/88 [==============================] - 12s 140ms/step - loss: 0.3017 - val_loss: 0.0395\
Epoch 2/40\
88/88 [==============================] - 11s 128ms/step - loss: 0.0270 - val_loss: 0.0227\
Epoch 3/40\
88/88 [==============================] - 11s 130ms/step - loss: 0.0204 - val_loss: 0.0198\
Epoch 4/40\
88/88 [==============================] - 11s 129ms/step - loss: 0.0181 - val_loss: 0.0183\
Epoch 5/40\
88/88 [==============================] - 11s 128ms/step - loss: 0.0167 - val_loss: 0.0177\
Epoch 6/40\
88/88 [==============================] - 11s 129ms/step - loss: 0.0149 - val_loss: 0.0178\
Epoch 7/40\
88/88 [==============================] - 11s 130ms/step - loss: 0.0128 - val_loss: 0.0183\
Epoch 8/40\
88/88 [==============================] - 11s 131ms/step - loss: 0.0096 - val_loss: 0.0178\
Fold 2: 0.017825302 loss, 0.77416116 auc\
Epoch 1/40\
88/88 [==============================] - 12s 138ms/step - loss: 0.3027 - val_loss: 0.0386\
Epoch 2/40\
88/88 [==============================] - 12s 133ms/step - loss: 0.0267 - val_loss: 0.0220\
Epoch 3/40\
88/88 [==============================] - 12s 133ms/step - loss: 0.0200 - val_loss: 0.0194\
Epoch 4/40\
88/88 [==============================] - 12s 131ms/step - loss: 0.0179 - val_loss: 0.0180\
Epoch 5/40\
88/88 [==============================] - 12s 132ms/step - loss: 0.0162 - val_loss: 0.0175\
Epoch 6/40\
88/88 [==============================] - 11s 130ms/step - loss: 0.0146 - val_loss: 0.0177\
Epoch 7/40\
88/88 [==============================] - 12s 132ms/step - loss: 0.0125 - val_loss: 0.0180\
Epoch 8/40\
88/88 [==============================] - 12s 132ms/step - loss: 0.0091 - val_loss: 0.0175\
Epoch 9/40\
88/88 [==============================] - 12s 138ms/step - loss: 0.0075 - val_loss: 0.0176\
Epoch 10/40\
88/88 [==============================] - 12s 137ms/step - loss: 0.0066 - val_loss: 0.0175\
Epoch 11/40\
88/88 [==============================] - 13s 143ms/step - loss: 0.0065 - val_loss: 0.0175\
Fold 3: 0.017520802 loss, 0.7813777 auc\
Epoch 1/40\
88/88 [==============================] - 13s 146ms/step - loss: 0.3015 - val_loss: 0.0449\
Epoch 2/40\
88/88 [==============================] - 12s 132ms/step - loss: 0.0267 - val_loss: 0.0220\
Epoch 3/40\
88/88 [==============================] - 12s 137ms/step - loss: 0.0201 - val_loss: 0.0192\
Epoch 4/40\
88/88 [==============================] - 12s 140ms/step - loss: 0.0180 - val_loss: 0.0181\
Epoch 5/40\
88/88 [==============================] - 12s 139ms/step - loss: 0.0163 - val_loss: 0.0177\
Epoch 6/40\
88/88 [==============================] - 11s 127ms/step - loss: 0.0146 - val_loss: 0.0178\
Epoch 7/40\
88/88 [==============================] - 13s 148ms/step - loss: 0.0125 - val_loss: 0.0182\
Epoch 8/40\
88/88 [==============================] - 11s 130ms/step - loss: 0.0092 - val_loss: 0.0178\
Fold 4: 0.01775606 loss, 0.77110624 auc\
Epoch 1/40\
88/88 [==============================] - 13s 146ms/step - loss: 0.3013 - val_loss: 0.0380\
Epoch 2/40\
88/88 [==============================] - 12s 136ms/step - loss: 0.0266 - val_loss: 0.0223\
Epoch 3/40\
88/88 [==============================] - 12s 135ms/step - loss: 0.0201 - val_loss: 0.0193\
Epoch 4/40\
88/88 [==============================] - 12s 136ms/step - loss: 0.0179 - val_loss: 0.0183\
Epoch 5/40\
88/88 [==============================] - 12s 137ms/step - loss: 0.0164 - val_loss: 0.0177\
Epoch 6/40\
88/88 [==============================] - 12s 134ms/step - loss: 0.0147 - val_loss: 0.0176\
Epoch 7/40\
88/88 [==============================] - 12s 138ms/step - loss: 0.0128 - val_loss: 0.0180\
Epoch 8/40\
88/88 [==============================] - 12s 135ms/step - loss: 0.0103 - val_loss: 0.0184\
Epoch 9/40\
88/88 [==============================] - 12s 141ms/step - loss: 0.0069 - val_loss: 0.0180\
Fold 5: 0.017957056 loss, 0.7590238 auc\
[0.017789576, 0.017825302, 0.017520802, 0.01775606, 0.017957056] [0.77474594, 0.77416116, 0.7813777, 0.77110624, 0.7590238]\
\cf10 [I 2020-11-05 17:06:26,198]\cf2  Trial 1 finished with value: 0.0177697591483593 and parameters: \{'layers': 3, 'neurons': 1712\}. Best is trial 1 with value: 0.0177697591483593.\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 5s 52ms/step - loss: 0.3670 - val_loss: 0.0649\
Epoch 2/40\
88/88 [==============================] - 4s 42ms/step - loss: 0.0387 - val_loss: 0.0330\
Epoch 3/40\
88/88 [==============================] - 4s 43ms/step - loss: 0.0239 - val_loss: 0.0218\
Epoch 4/40\
88/88 [==============================] - 4s 42ms/step - loss: 0.0202 - val_loss: 0.0197\
Epoch 5/40\
88/88 [==============================] - 4s 42ms/step - loss: 0.0182 - val_loss: 0.0184\
Epoch 6/40\
88/88 [==============================] - 4s 43ms/step - loss: 0.0168 - val_loss: 0.0181\
Epoch 7/40\
88/88 [==============================] - 4s 46ms/step - loss: 0.0155 - val_loss: 0.0179\
Epoch 8/40\
88/88 [==============================] - 4s 47ms/step - loss: 0.0141 - val_loss: 0.0181\
Epoch 9/40\
88/88 [==============================] - 4s 46ms/step - loss: 0.0126 - val_loss: 0.0183\
Epoch 10/40\
88/88 [==============================] - 4s 48ms/step - loss: 0.0102 - val_loss: 0.0180\
Fold 1: 0.017950324 loss, 0.7632494 auc\
Epoch 1/40\
88/88 [==============================] - 5s 59ms/step - loss: 0.3670 - val_loss: 0.0730\
Epoch 2/40\
88/88 [==============================] - 4s 48ms/step - loss: 0.0386 - val_loss: 0.0275\
Epoch 3/40\
88/88 [==============================] - 5s 55ms/step - loss: 0.0241 - val_loss: 0.0218\
Epoch 4/40\
88/88 [==============================] - 5s 54ms/step - loss: 0.0202 - val_loss: 0.0194\
Epoch 5/40\
88/88 [==============================] - 4s 49ms/step - loss: 0.0184 - val_loss: 0.0186\
Epoch 6/40\
88/88 [==============================] - 5s 55ms/step - loss: 0.0172 - val_loss: 0.0178\
Epoch 7/40\
88/88 [==============================] - 4s 48ms/step - loss: 0.0157 - val_loss: 0.0178\
Epoch 8/40\
88/88 [==============================] - 4s 51ms/step - loss: 0.0143 - val_loss: 0.0181\
Epoch 9/40\
88/88 [==============================] - 5s 51ms/step - loss: 0.0121 - val_loss: 0.0178\
Epoch 10/40\
88/88 [==============================] - 5s 54ms/step - loss: 0.0109 - val_loss: 0.0179\
Epoch 11/40\
88/88 [==============================] - 4s 50ms/step - loss: 0.0101 - val_loss: 0.0179\
Epoch 12/40\
88/88 [==============================] - 5s 51ms/step - loss: 0.0100 - val_loss: 0.0179\
Fold 2: 0.017884217 loss, 0.77562505 auc\
Epoch 1/40\
88/88 [==============================] - 6s 71ms/step - loss: 0.3657 - val_loss: 0.0739\
Epoch 2/40\
88/88 [==============================] - 4s 50ms/step - loss: 0.0381 - val_loss: 0.0268\
Epoch 3/40\
88/88 [==============================] - 5s 52ms/step - loss: 0.0239 - val_loss: 0.0217\
Epoch 4/40\
88/88 [==============================] - 5s 54ms/step - loss: 0.0202 - val_loss: 0.0194\
Epoch 5/40\
88/88 [==============================] - 5s 54ms/step - loss: 0.0182 - val_loss: 0.0184\
Epoch 6/40\
88/88 [==============================] - 5s 55ms/step - loss: 0.0169 - val_loss: 0.0181\
Epoch 7/40\
88/88 [==============================] - 5s 55ms/step - loss: 0.0155 - val_loss: 0.0181\
Epoch 8/40\
88/88 [==============================] - 5s 54ms/step - loss: 0.0142 - val_loss: 0.0183\
Epoch 9/40\
88/88 [==============================] - 4s 48ms/step - loss: 0.0123 - val_loss: 0.0178\
Epoch 10/40\
88/88 [==============================] - 5s 57ms/step - loss: 0.0111 - val_loss: 0.0179\
Epoch 11/40\
88/88 [==============================] - 5s 59ms/step - loss: 0.0104 - val_loss: 0.0180\
Epoch 12/40\
88/88 [==============================] - 5s 51ms/step - loss: 0.0098 - val_loss: 0.0180\
Fold 3: 0.017958252 loss, 0.77679676 auc\
Epoch 1/40\
88/88 [==============================] - 5s 57ms/step - loss: 0.3683 - val_loss: 0.0668\
Epoch 2/40\
88/88 [==============================] - 5s 52ms/step - loss: 0.0384 - val_loss: 0.0277\
Epoch 3/40\
88/88 [==============================] - 4s 51ms/step - loss: 0.0238 - val_loss: 0.0223\
Epoch 4/40\
88/88 [==============================] - 4s 50ms/step - loss: 0.0200 - val_loss: 0.0198\
Epoch 5/40\
88/88 [==============================] - 4s 49ms/step - loss: 0.0181 - val_loss: 0.0186\
Epoch 6/40\
88/88 [==============================] - 4s 47ms/step - loss: 0.0166 - val_loss: 0.0185\
Epoch 7/40\
88/88 [==============================] - 4s 46ms/step - loss: 0.0152 - val_loss: 0.0183\
Epoch 8/40\
88/88 [==============================] - 4s 48ms/step - loss: 0.0137 - val_loss: 0.0184\
Epoch 9/40\
88/88 [==============================] - 5s 52ms/step - loss: 0.0122 - val_loss: 0.0189\
Epoch 10/40\
88/88 [==============================] - 4s 48ms/step - loss: 0.0099 - val_loss: 0.0184\
Fold 4: 0.018355923 loss, 0.7573533 auc\
Epoch 1/40\
88/88 [==============================] - 5s 53ms/step - loss: 0.3670 - val_loss: 0.0669\
Epoch 2/40\
88/88 [==============================] - 4s 51ms/step - loss: 0.0387 - val_loss: 0.0279\
Epoch 3/40\
88/88 [==============================] - 4s 49ms/step - loss: 0.0240 - val_loss: 0.0229\
Epoch 4/40\
88/88 [==============================] - 5s 52ms/step - loss: 0.0202 - val_loss: 0.0200\
Epoch 5/40\
88/88 [==============================] - 4s 49ms/step - loss: 0.0183 - val_loss: 0.0185\
Epoch 6/40\
88/88 [==============================] - 4s 49ms/step - loss: 0.0168 - val_loss: 0.0182\
Epoch 7/40\
88/88 [==============================] - 5s 53ms/step - loss: 0.0155 - val_loss: 0.0181\
Epoch 8/40\
88/88 [==============================] - 4s 51ms/step - loss: 0.0141 - val_loss: 0.0184\
Epoch 9/40\
88/88 [==============================] - 5s 52ms/step - loss: 0.0119 - val_loss: 0.0180\
Epoch 10/40\
88/88 [==============================] - 5s 58ms/step - loss: 0.0107 - val_loss: 0.0181\
Epoch 11/40\
88/88 [==============================] - 5s 51ms/step - loss: 0.0100 - val_loss: 0.0182\
Epoch 12/40\
88/88 [==============================] - 5s 55ms/step - loss: 0.0093 - val_loss: 0.0182\
Fold 5: 0.018239317 loss, 0.7678284 auc\
[0.017950324, 0.017884217, 0.017958252, 0.018355923, 0.018239317] [0.7632494, 0.77562505, 0.77679676, 0.7573533, 0.7678284]\
\cf10 [I 2020-11-05 17:11:25,415]\cf2  Trial 2 finished with value: 0.018077606707811354 and parameters: \{'layers': 5, 'neurons': 662\}. Best is trial 1 with value: 0.0177697591483593.\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.3885 - val_loss: 0.0775\
Epoch 2/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0430 - val_loss: 0.0296\
Epoch 3/40\
88/88 [==============================] - 2s 18ms/step - loss: 0.0252 - val_loss: 0.0230\
Epoch 4/40\
88/88 [==============================] - 2s 19ms/step - loss: 0.0208 - val_loss: 0.0205\
Epoch 5/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0187 - val_loss: 0.0191\
Epoch 6/40\
88/88 [==============================] - 2s 17ms/step - loss: 0.0172 - val_loss: 0.0184\
Epoch 7/40\
88/88 [==============================] - 1s 17ms/step - loss: 0.0159 - val_loss: 0.0178\
Epoch 8/40\
88/88 [==============================] - 1s 17ms/step - loss: 0.0145 - val_loss: 0.0177\
Epoch 9/40\
88/88 [==============================] - 2s 17ms/step - loss: 0.0130 - val_loss: 0.0177\
Epoch 10/40\
88/88 [==============================] - 1s 17ms/step - loss: 0.0113 - val_loss: 0.0178\
Epoch 11/40\
88/88 [==============================] - 2s 17ms/step - loss: 0.0090 - val_loss: 0.0175\
Epoch 12/40\
88/88 [==============================] - 1s 17ms/step - loss: 0.0083 - val_loss: 0.0176\
Epoch 13/40\
88/88 [==============================] - 1s 17ms/step - loss: 0.0079 - val_loss: 0.0176\
Epoch 14/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0075 - val_loss: 0.0176\
Fold 1: 0.01764405 loss, 0.7837869 auc\
Epoch 1/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.3892 - val_loss: 0.0702\
Epoch 2/40\
88/88 [==============================] - 1s 17ms/step - loss: 0.0430 - val_loss: 0.0290\
Epoch 3/40\
88/88 [==============================] - 2s 18ms/step - loss: 0.0251 - val_loss: 0.0225\
Epoch 4/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0208 - val_loss: 0.0202\
Epoch 5/40\
88/88 [==============================] - 1s 17ms/step - loss: 0.0187 - val_loss: 0.0192\
Epoch 6/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0172 - val_loss: 0.0183\
Epoch 7/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0159 - val_loss: 0.0176\
Epoch 8/40\
88/88 [==============================] - 2s 17ms/step - loss: 0.0144 - val_loss: 0.0174\
Epoch 9/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0131 - val_loss: 0.0175\
Epoch 10/40\
88/88 [==============================] - 1s 14ms/step - loss: 0.0114 - val_loss: 0.0177\
Epoch 11/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0091 - val_loss: 0.0175\
Fold 2: 0.017454926 loss, 0.79019856 auc\
Epoch 1/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.3906 - val_loss: 0.0756\
Epoch 2/40\
88/88 [==============================] - 1s 17ms/step - loss: 0.0432 - val_loss: 0.0293\
Epoch 3/40\
88/88 [==============================] - 1s 17ms/step - loss: 0.0251 - val_loss: 0.0227\
Epoch 4/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0208 - val_loss: 0.0202\
Epoch 5/40\
88/88 [==============================] - 1s 17ms/step - loss: 0.0187 - val_loss: 0.0189\
Epoch 6/40\
88/88 [==============================] - 2s 17ms/step - loss: 0.0173 - val_loss: 0.0184\
Epoch 7/40\
88/88 [==============================] - 2s 17ms/step - loss: 0.0159 - val_loss: 0.0182\
Epoch 8/40\
88/88 [==============================] - 2s 18ms/step - loss: 0.0146 - val_loss: 0.0174\
Epoch 9/40\
88/88 [==============================] - 1s 17ms/step - loss: 0.0131 - val_loss: 0.0175\
Epoch 10/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0117 - val_loss: 0.0178\
Epoch 11/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0093 - val_loss: 0.0174\
Fold 3: 0.01737859 loss, 0.79696774 auc\
Epoch 1/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.3898 - val_loss: 0.0748\
Epoch 2/40\
88/88 [==============================] - 2s 18ms/step - loss: 0.0431 - val_loss: 0.0291\
Epoch 3/40\
88/88 [==============================] - 2s 17ms/step - loss: 0.0253 - val_loss: 0.0228\
Epoch 4/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0208 - val_loss: 0.0206\
Epoch 5/40\
88/88 [==============================] - 1s 17ms/step - loss: 0.0187 - val_loss: 0.0190\
Epoch 6/40\
88/88 [==============================] - 2s 18ms/step - loss: 0.0172 - val_loss: 0.0183\
Epoch 7/40\
88/88 [==============================] - 2s 18ms/step - loss: 0.0158 - val_loss: 0.0180\
Epoch 8/40\
88/88 [==============================] - 2s 17ms/step - loss: 0.0144 - val_loss: 0.0177\
Epoch 9/40\
88/88 [==============================] - 1s 17ms/step - loss: 0.0129 - val_loss: 0.0176\
Epoch 10/40\
88/88 [==============================] - 2s 19ms/step - loss: 0.0113 - val_loss: 0.0178\
Epoch 11/40\
88/88 [==============================] - 2s 18ms/step - loss: 0.0095 - val_loss: 0.0181\
Epoch 12/40\
88/88 [==============================] - 2s 18ms/step - loss: 0.0073 - val_loss: 0.0180\
Fold 4: 0.017955374 loss, 0.771704 auc\
Epoch 1/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.3924 - val_loss: 0.0901\
Epoch 2/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0432 - val_loss: 0.0297\
Epoch 3/40\
88/88 [==============================] - 1s 17ms/step - loss: 0.0251 - val_loss: 0.0226\
Epoch 4/40\
88/88 [==============================] - 2s 18ms/step - loss: 0.0207 - val_loss: 0.0202\
Epoch 5/40\
88/88 [==============================] - 2s 18ms/step - loss: 0.0186 - val_loss: 0.0190\
Epoch 6/40\
88/88 [==============================] - 2s 19ms/step - loss: 0.0172 - val_loss: 0.0181\
Epoch 7/40\
88/88 [==============================] - 1s 17ms/step - loss: 0.0159 - val_loss: 0.0178\
Epoch 8/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0145 - val_loss: 0.0175\
Epoch 9/40\
88/88 [==============================] - 1s 17ms/step - loss: 0.0130 - val_loss: 0.0175\
Epoch 10/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0112 - val_loss: 0.0178\
Epoch 11/40\
88/88 [==============================] - 2s 17ms/step - loss: 0.0089 - val_loss: 0.0175\
Fold 5: 0.017516296 loss, 0.79171526 auc\
[0.01764405, 0.017454926, 0.01737859, 0.017955374, 0.017516296] [0.7837869, 0.79019856, 0.79696774, 0.771704, 0.79171526]\
\cf10 [I 2020-11-05 17:13:24,063]\cf2  Trial 3 finished with value: 0.01758984699845314 and parameters: \{'layers': 1, 'neurons': 512\}. Best is trial 3 with value: 0.01758984699845314.\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 5s 59ms/step - loss: 0.3672 - val_loss: 0.0728\
Epoch 2/40\
88/88 [==============================] - 5s 54ms/step - loss: 0.0385 - val_loss: 0.0277\
Epoch 3/40\
88/88 [==============================] - 5s 52ms/step - loss: 0.0240 - val_loss: 0.0223\
Epoch 4/40\
88/88 [==============================] - 4s 51ms/step - loss: 0.0202 - val_loss: 0.0198\
Epoch 5/40\
88/88 [==============================] - 5s 52ms/step - loss: 0.0182 - val_loss: 0.0186\
Epoch 6/40\
88/88 [==============================] - 4s 47ms/step - loss: 0.0168 - val_loss: 0.0181\
Epoch 7/40\
88/88 [==============================] - 4s 47ms/step - loss: 0.0153 - val_loss: 0.0181\
Epoch 8/40\
88/88 [==============================] - 4s 48ms/step - loss: 0.0140 - val_loss: 0.0186\
Epoch 9/40\
88/88 [==============================] - 4s 47ms/step - loss: 0.0118 - val_loss: 0.0181\
Epoch 10/40\
88/88 [==============================] - 5s 55ms/step - loss: 0.0107 - val_loss: 0.0182\
Epoch 11/40\
88/88 [==============================] - 5s 56ms/step - loss: 0.0099 - val_loss: 0.0182\
Epoch 12/40\
88/88 [==============================] - 5s 55ms/step - loss: 0.0098 - val_loss: 0.0182\
Fold 1: 0.018174516 loss, 0.7686386 auc\
Epoch 1/40\
88/88 [==============================] - 5s 60ms/step - loss: 0.3672 - val_loss: 0.0674\
Epoch 2/40\
88/88 [==============================] - 5s 53ms/step - loss: 0.0388 - val_loss: 0.0280\
Epoch 3/40\
88/88 [==============================] - 5s 53ms/step - loss: 0.0240 - val_loss: 0.0221\
Epoch 4/40\
88/88 [==============================] - 5s 58ms/step - loss: 0.0201 - val_loss: 0.0194\
Epoch 5/40\
88/88 [==============================] - 4s 50ms/step - loss: 0.0183 - val_loss: 0.0184\
Epoch 6/40\
88/88 [==============================] - 5s 54ms/step - loss: 0.0169 - val_loss: 0.0179\
Epoch 7/40\
88/88 [==============================] - 4s 49ms/step - loss: 0.0155 - val_loss: 0.0179\
Epoch 8/40\
88/88 [==============================] - 5s 52ms/step - loss: 0.0141 - val_loss: 0.0183\
Epoch 9/40\
88/88 [==============================] - 5s 56ms/step - loss: 0.0119 - val_loss: 0.0179\
Epoch 10/40\
88/88 [==============================] - 5s 57ms/step - loss: 0.0106 - val_loss: 0.0179\
Fold 2: 0.017940756 loss, 0.7705742 auc\
Epoch 1/40\
88/88 [==============================] - 5s 59ms/step - loss: 0.3667 - val_loss: 0.0596\
Epoch 2/40\
88/88 [==============================] - 4s 47ms/step - loss: 0.0382 - val_loss: 0.0269\
Epoch 3/40\
88/88 [==============================] - 4s 50ms/step - loss: 0.0238 - val_loss: 0.0222\
Epoch 4/40\
88/88 [==============================] - 4s 50ms/step - loss: 0.0201 - val_loss: 0.0197\
Epoch 5/40\
88/88 [==============================] - 4s 46ms/step - loss: 0.0182 - val_loss: 0.0188\
Epoch 6/40\
88/88 [==============================] - 4s 45ms/step - loss: 0.0169 - val_loss: 0.0181\
Epoch 7/40\
88/88 [==============================] - 4s 47ms/step - loss: 0.0155 - val_loss: 0.0179\
Epoch 8/40\
88/88 [==============================] - 4s 46ms/step - loss: 0.0142 - val_loss: 0.0180\
Epoch 9/40\
88/88 [==============================] - 4s 49ms/step - loss: 0.0126 - val_loss: 0.0185\
Epoch 10/40\
88/88 [==============================] - 4s 45ms/step - loss: 0.0102 - val_loss: 0.0180\
Fold 3: 0.018012935 loss, 0.7593411 auc\
Epoch 1/40\
88/88 [==============================] - 4s 51ms/step - loss: 0.3674 - val_loss: 0.0692\
Epoch 2/40\
88/88 [==============================] - 4s 42ms/step - loss: 0.0381 - val_loss: 0.0276\
Epoch 3/40\
88/88 [==============================] - 4s 45ms/step - loss: 0.0236 - val_loss: 0.0217\
Epoch 4/40\
88/88 [==============================] - 4s 44ms/step - loss: 0.0199 - val_loss: 0.0191\
Epoch 5/40\
88/88 [==============================] - 4s 41ms/step - loss: 0.0181 - val_loss: 0.0187\
Epoch 6/40\
88/88 [==============================] - 4s 43ms/step - loss: 0.0167 - val_loss: 0.0187\
Epoch 7/40\
88/88 [==============================] - 4s 43ms/step - loss: 0.0154 - val_loss: 0.0182\
Epoch 8/40\
88/88 [==============================] - 4s 43ms/step - loss: 0.0139 - val_loss: 0.0183\
Epoch 9/40\
88/88 [==============================] - 4s 44ms/step - loss: 0.0124 - val_loss: 0.0184\
Epoch 10/40\
88/88 [==============================] - 4s 43ms/step - loss: 0.0101 - val_loss: 0.0183\
Fold 4: 0.018256975 loss, 0.7532359 auc\
Epoch 1/40\
88/88 [==============================] - 5s 62ms/step - loss: 0.3666 - val_loss: 0.0665\
Epoch 2/40\
88/88 [==============================] - 4s 47ms/step - loss: 0.0385 - val_loss: 0.0276\
Epoch 3/40\
88/88 [==============================] - 4s 45ms/step - loss: 0.0238 - val_loss: 0.0219\
Epoch 4/40\
88/88 [==============================] - 4s 45ms/step - loss: 0.0200 - val_loss: 0.0193\
Epoch 5/40\
88/88 [==============================] - 5s 54ms/step - loss: 0.0183 - val_loss: 0.0187\
Epoch 6/40\
88/88 [==============================] - 5s 55ms/step - loss: 0.0167 - val_loss: 0.0182\
Epoch 7/40\
88/88 [==============================] - 4s 50ms/step - loss: 0.0153 - val_loss: 0.0181\
Epoch 8/40\
88/88 [==============================] - 4s 49ms/step - loss: 0.0139 - val_loss: 0.0182\
Epoch 9/40\
88/88 [==============================] - 4s 48ms/step - loss: 0.0116 - val_loss: 0.0179\
Epoch 10/40\
88/88 [==============================] - 4s 46ms/step - loss: 0.0104 - val_loss: 0.0181\
Epoch 11/40\
88/88 [==============================] - 4s 46ms/step - loss: 0.0097 - val_loss: 0.0181\
Epoch 12/40\
88/88 [==============================] - 4s 43ms/step - loss: 0.0091 - val_loss: 0.0181\
Fold 5: 0.018116532 loss, 0.77037495 auc\
[0.018174516, 0.017940756, 0.018012935, 0.018256975, 0.018116532] [0.7686386, 0.7705742, 0.7593411, 0.7532359, 0.77037495]\
\cf10 [I 2020-11-05 17:18:06,582]\cf2  Trial 4 finished with value: 0.018100342899560928 and parameters: \{'layers': 5, 'neurons': 662\}. Best is trial 3 with value: 0.01758984699845314.\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 5s 58ms/step - loss: 0.3366 - val_loss: 0.0482\
Epoch 2/40\
88/88 [==============================] - 4s 49ms/step - loss: 0.0318 - val_loss: 0.0245\
Epoch 3/40\
88/88 [==============================] - 5s 53ms/step - loss: 0.0216 - val_loss: 0.0203\
Epoch 4/40\
88/88 [==============================] - 5s 52ms/step - loss: 0.0187 - val_loss: 0.0185\
Epoch 5/40\
88/88 [==============================] - 5s 52ms/step - loss: 0.0170 - val_loss: 0.0179\
Epoch 6/40\
88/88 [==============================] - 4s 51ms/step - loss: 0.0154 - val_loss: 0.0178\
Epoch 7/40\
88/88 [==============================] - 5s 54ms/step - loss: 0.0137 - val_loss: 0.0176\
Epoch 8/40\
88/88 [==============================] - 5s 59ms/step - loss: 0.0112 - val_loss: 0.0180\
Epoch 9/40\
88/88 [==============================] - 5s 54ms/step - loss: 0.0089 - val_loss: 0.0183\
Epoch 10/40\
88/88 [==============================] - 5s 57ms/step - loss: 0.0062 - val_loss: 0.0180\
\cf10 [I 2020-11-05 17:19:08,050]\cf2  Trial 5 pruned. \
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 4s 44ms/step - loss: 0.3671 - val_loss: 0.0676\
Epoch 2/40\
88/88 [==============================] - 3s 36ms/step - loss: 0.0383 - val_loss: 0.0276\
Epoch 3/40\
88/88 [==============================] - 3s 36ms/step - loss: 0.0236 - val_loss: 0.0222\
Epoch 4/40\
88/88 [==============================] - 3s 40ms/step - loss: 0.0199 - val_loss: 0.0194\
Epoch 5/40\
88/88 [==============================] - 4s 42ms/step - loss: 0.0180 - val_loss: 0.0185\
Epoch 6/40\
88/88 [==============================] - 3s 38ms/step - loss: 0.0165 - val_loss: 0.0182\
Epoch 7/40\
88/88 [==============================] - 3s 39ms/step - loss: 0.0150 - val_loss: 0.0181\
Epoch 8/40\
88/88 [==============================] - 3s 36ms/step - loss: 0.0135 - val_loss: 0.0186\
Epoch 9/40\
88/88 [==============================] - 3s 36ms/step - loss: 0.0110 - val_loss: 0.0180\
Epoch 10/40\
88/88 [==============================] - 3s 39ms/step - loss: 0.0099 - val_loss: 0.0181\
Epoch 11/40\
88/88 [==============================] - 3s 38ms/step - loss: 0.0091 - val_loss: 0.0182\
Epoch 12/40\
88/88 [==============================] - 3s 37ms/step - loss: 0.0086 - val_loss: 0.0182\
\cf10 [I 2020-11-05 17:20:01,386]\cf2  Trial 6 pruned. \
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 6s 70ms/step - loss: 0.3518 - val_loss: 0.0688\
Epoch 2/40\
88/88 [==============================] - 5s 57ms/step - loss: 0.0355 - val_loss: 0.0269\
Epoch 3/40\
88/88 [==============================] - 5s 56ms/step - loss: 0.0230 - val_loss: 0.0218\
Epoch 4/40\
88/88 [==============================] - 5s 60ms/step - loss: 0.0197 - val_loss: 0.0194\
Epoch 5/40\
88/88 [==============================] - 5s 58ms/step - loss: 0.0180 - val_loss: 0.0188\
Epoch 6/40\
88/88 [==============================] - 6s 70ms/step - loss: 0.0166 - val_loss: 0.0180\
Epoch 7/40\
88/88 [==============================] - 6s 71ms/step - loss: 0.0151 - val_loss: 0.0182\
Epoch 8/40\
88/88 [==============================] - 6s 69ms/step - loss: 0.0136 - val_loss: 0.0185\
Epoch 9/40\
88/88 [==============================] - 6s 65ms/step - loss: 0.0114 - val_loss: 0.0179\
Epoch 10/40\
88/88 [==============================] - 6s 68ms/step - loss: 0.0101 - val_loss: 0.0181\
Epoch 11/40\
88/88 [==============================] - 6s 67ms/step - loss: 0.0094 - val_loss: 0.0181\
Epoch 12/40\
88/88 [==============================] - 6s 67ms/step - loss: 0.0093 - val_loss: 0.0181\
\cf10 [I 2020-11-05 17:21:25,832]\cf2  Trial 7 pruned. \
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 8s 90ms/step - loss: 0.3096 - val_loss: 0.0394\
Epoch 2/40\
88/88 [==============================] - 7s 83ms/step - loss: 0.0275 - val_loss: 0.0224\
Epoch 3/40\
88/88 [==============================] - 7s 83ms/step - loss: 0.0201 - val_loss: 0.0195\
Epoch 4/40\
88/88 [==============================] - 7s 83ms/step - loss: 0.0178 - val_loss: 0.0182\
Epoch 5/40\
88/88 [==============================] - 7s 82ms/step - loss: 0.0163 - val_loss: 0.0179\
Epoch 6/40\
88/88 [==============================] - 8s 86ms/step - loss: 0.0145 - val_loss: 0.0177\
Epoch 7/40\
88/88 [==============================] - 8s 89ms/step - loss: 0.0122 - val_loss: 0.0180\
Epoch 8/40\
88/88 [==============================] - 7s 79ms/step - loss: 0.0096 - val_loss: 0.0184\
Epoch 9/40\
88/88 [==============================] - 7s 84ms/step - loss: 0.0064 - val_loss: 0.0180\
\cf10 [I 2020-11-05 17:22:48,278]\cf2  Trial 8 pruned. \
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 3s 35ms/step - loss: 0.3612 - val_loss: 0.0628\
Epoch 2/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.0373 - val_loss: 0.0270\
Epoch 3/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.0234 - val_loss: 0.0217\
Epoch 4/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0198 - val_loss: 0.0195\
Epoch 5/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0178 - val_loss: 0.0185\
Epoch 6/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.0164 - val_loss: 0.0180\
Epoch 7/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0147 - val_loss: 0.0178\
Epoch 8/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0129 - val_loss: 0.0180\
Epoch 9/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0109 - val_loss: 0.0184\
Epoch 10/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0082 - val_loss: 0.0180\
\cf10 [I 2020-11-05 17:23:27,017]\cf2  Trial 9 pruned. \
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.3468 - val_loss: 0.0530\
Epoch 2/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0331 - val_loss: 0.0251\
Epoch 3/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0221 - val_loss: 0.0206\
Epoch 4/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0192 - val_loss: 0.0192\
Epoch 5/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0177 - val_loss: 0.0184\
Epoch 6/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0165 - val_loss: 0.0178\
Epoch 7/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0154 - val_loss: 0.0175\
Epoch 8/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0142 - val_loss: 0.0173\
Epoch 9/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0136 - val_loss: 0.0171\
Epoch 10/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0120 - val_loss: 0.0172\
Epoch 11/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0105 - val_loss: 0.0172\
Epoch 12/40\
88/88 [==============================] - 2s 19ms/step - loss: 0.0086 - val_loss: 0.0170\
Epoch 13/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0081 - val_loss: 0.0170\
Epoch 14/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0078 - val_loss: 0.0171\
Epoch 15/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0075 - val_loss: 0.0171\
Fold 1: 0.017080022 loss, 0.80479556 auc\
Epoch 1/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.3454 - val_loss: 0.0501\
Epoch 2/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0329 - val_loss: 0.0246\
Epoch 3/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0221 - val_loss: 0.0206\
Epoch 4/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0192 - val_loss: 0.0191\
Epoch 5/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0176 - val_loss: 0.0183\
Epoch 6/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0163 - val_loss: 0.0178\
Epoch 7/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0155 - val_loss: 0.0175\
Epoch 8/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0141 - val_loss: 0.0174\
Epoch 9/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0129 - val_loss: 0.0171\
Epoch 10/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0121 - val_loss: 0.0172\
Epoch 11/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0106 - val_loss: 0.0171\
Epoch 12/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0084 - val_loss: 0.0169\
Epoch 13/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0080 - val_loss: 0.0170\
Epoch 14/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0077 - val_loss: 0.0170\
Epoch 15/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0074 - val_loss: 0.0170\
Fold 2: 0.017035978 loss, 0.79812247 auc\
Epoch 1/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.3442 - val_loss: 0.0518\
Epoch 2/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0329 - val_loss: 0.0246\
Epoch 3/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0222 - val_loss: 0.0206\
Epoch 4/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0193 - val_loss: 0.0191\
Epoch 5/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0178 - val_loss: 0.0183\
Epoch 6/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0165 - val_loss: 0.0178\
Epoch 7/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0154 - val_loss: 0.0175\
Epoch 8/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0143 - val_loss: 0.0174\
Epoch 9/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0132 - val_loss: 0.0171\
Epoch 10/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0119 - val_loss: 0.0173\
Epoch 11/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0110 - val_loss: 0.0169\
Epoch 12/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0093 - val_loss: 0.0171\
Epoch 13/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0079 - val_loss: 0.0173\
Epoch 14/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0060 - val_loss: 0.0171\
Fold 3: 0.017132329 loss, 0.7843024 auc\
Epoch 1/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.3472 - val_loss: 0.0507\
Epoch 2/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0332 - val_loss: 0.0250\
Epoch 3/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0222 - val_loss: 0.0210\
Epoch 4/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0194 - val_loss: 0.0193\
Epoch 5/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0177 - val_loss: 0.0184\
Epoch 6/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0165 - val_loss: 0.0180\
Epoch 7/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0153 - val_loss: 0.0175\
Epoch 8/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0143 - val_loss: 0.0174\
Epoch 9/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0130 - val_loss: 0.0173\
Epoch 10/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0119 - val_loss: 0.0171\
Epoch 11/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0106 - val_loss: 0.0175\
Epoch 12/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0090 - val_loss: 0.0172\
Epoch 13/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0069 - val_loss: 0.0170\
Epoch 14/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0064 - val_loss: 0.0171\
Epoch 15/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0061 - val_loss: 0.0171\
Epoch 16/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0061 - val_loss: 0.0172\
Fold 4: 0.017158998 loss, 0.7910262 auc\
Epoch 1/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.3449 - val_loss: 0.0508\
Epoch 2/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0327 - val_loss: 0.0248\
Epoch 3/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0221 - val_loss: 0.0206\
Epoch 4/40\
88/88 [==============================] - 2s 19ms/step - loss: 0.0193 - val_loss: 0.0192\
Epoch 5/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0178 - val_loss: 0.0183\
Epoch 6/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0165 - val_loss: 0.0178\
Epoch 7/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0154 - val_loss: 0.0174\
Epoch 8/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0142 - val_loss: 0.0174\
Epoch 9/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0132 - val_loss: 0.0172\
Epoch 10/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0119 - val_loss: 0.0175\
Epoch 11/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0110 - val_loss: 0.0175\
Epoch 12/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0089 - val_loss: 0.0169\
Epoch 13/40\
88/88 [==============================] - 2s 19ms/step - loss: 0.0083 - val_loss: 0.0169\
Epoch 14/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0080 - val_loss: 0.0170\
Epoch 15/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0077 - val_loss: 0.0170\
Fold 5: 0.017016875 loss, 0.8007541 auc\
[0.017080022, 0.017035978, 0.017132329, 0.017158998, 0.017016875] [0.80479556, 0.79812247, 0.7843024, 0.7910262, 0.8007541]\
\cf10 [I 2020-11-05 17:26:21,691]\cf2  Trial 10 finished with value: 0.017084840312600137 and parameters: \{'layers': 0, 'neurons': 962\}. Best is trial 10 with value: 0.017084840312600137.\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.3380 - val_loss: 0.0508\
Epoch 2/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0315 - val_loss: 0.0239\
Epoch 3/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0217 - val_loss: 0.0202\
Epoch 4/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0190 - val_loss: 0.0190\
Epoch 5/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0174 - val_loss: 0.0183\
Epoch 6/40\
88/88 [==============================] - 2s 19ms/step - loss: 0.0161 - val_loss: 0.0177\
Epoch 7/40\
88/88 [==============================] - 2s 19ms/step - loss: 0.0149 - val_loss: 0.0176\
Epoch 8/40\
88/88 [==============================] - 2s 19ms/step - loss: 0.0137 - val_loss: 0.0174\
Epoch 9/40\
88/88 [==============================] - 2s 19ms/step - loss: 0.0127 - val_loss: 0.0172\
Epoch 10/40\
88/88 [==============================] - 2s 19ms/step - loss: 0.0112 - val_loss: 0.0173\
Epoch 11/40\
88/88 [==============================] - 2s 19ms/step - loss: 0.0098 - val_loss: 0.0173\
Epoch 12/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0076 - val_loss: 0.0171\
Epoch 13/40\
88/88 [==============================] - 2s 19ms/step - loss: 0.0070 - val_loss: 0.0171\
Epoch 14/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0067 - val_loss: 0.0172\
Epoch 15/40\
88/88 [==============================] - 2s 19ms/step - loss: 0.0064 - val_loss: 0.0172\
Fold 1: 0.017216144 loss, 0.789072 auc\
Epoch 1/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.3359 - val_loss: 0.0527\
Epoch 2/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0317 - val_loss: 0.0244\
Epoch 3/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0218 - val_loss: 0.0205\
Epoch 4/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0191 - val_loss: 0.0192\
Epoch 5/40\
88/88 [==============================] - 2s 19ms/step - loss: 0.0176 - val_loss: 0.0184\
Epoch 6/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0164 - val_loss: 0.0178\
Epoch 7/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0152 - val_loss: 0.0175\
Epoch 8/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0141 - val_loss: 0.0174\
Epoch 9/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0130 - val_loss: 0.0176\
Epoch 10/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0117 - val_loss: 0.0175\
Epoch 11/40\
88/88 [==============================] - 2s 18ms/step - loss: 0.0097 - val_loss: 0.0169\
Epoch 12/40\
88/88 [==============================] - 2s 19ms/step - loss: 0.0091 - val_loss: 0.0169\
Epoch 13/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0088 - val_loss: 0.0169\
Epoch 14/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0085 - val_loss: 0.0169\
Epoch 15/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0085 - val_loss: 0.0169\
Fold 2: 0.01694516 loss, 0.80684197 auc\
Epoch 1/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.3391 - val_loss: 0.0442\
Epoch 2/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0316 - val_loss: 0.0240\
Epoch 3/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0217 - val_loss: 0.0204\
Epoch 4/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0191 - val_loss: 0.0190\
Epoch 5/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0176 - val_loss: 0.0182\
Epoch 6/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0164 - val_loss: 0.0178\
Epoch 7/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0152 - val_loss: 0.0174\
Epoch 8/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0140 - val_loss: 0.0172\
Epoch 9/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0148 - val_loss: 0.0184\
Epoch 10/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0151 - val_loss: 0.0172\
Epoch 11/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0124 - val_loss: 0.0168\
Epoch 12/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0120 - val_loss: 0.0167\
Epoch 13/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0117 - val_loss: 0.0167\
Epoch 14/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0114 - val_loss: 0.0167\
Epoch 15/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0112 - val_loss: 0.0167\
Epoch 16/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0112 - val_loss: 0.0167\
Epoch 17/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0111 - val_loss: 0.0167\
Epoch 18/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0111 - val_loss: 0.0167\
Epoch 19/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0111 - val_loss: 0.0167\
Epoch 20/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0111 - val_loss: 0.0167\
Epoch 21/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0111 - val_loss: 0.0167\
Epoch 22/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0110 - val_loss: 0.0167\
Epoch 23/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0110 - val_loss: 0.0167\
Epoch 24/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0110 - val_loss: 0.0167\
Epoch 25/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0110 - val_loss: 0.0167\
Epoch 26/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0109 - val_loss: 0.0167\
Epoch 27/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0109 - val_loss: 0.0167\
Epoch 28/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0109 - val_loss: 0.0167\
Epoch 29/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0109 - val_loss: 0.0167\
Epoch 30/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0108 - val_loss: 0.0167\
Epoch 31/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0108 - val_loss: 0.0167\
Epoch 32/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0108 - val_loss: 0.0167\
Fold 3: 0.016656088 loss, 0.8155805 auc\
Epoch 1/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.3382 - val_loss: 0.0515\
Epoch 2/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0317 - val_loss: 0.0243\
Epoch 3/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0218 - val_loss: 0.0204\
Epoch 4/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0191 - val_loss: 0.0190\
Epoch 5/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0175 - val_loss: 0.0183\
Epoch 6/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0163 - val_loss: 0.0176\
Epoch 7/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0151 - val_loss: 0.0174\
Epoch 8/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0140 - val_loss: 0.0174\
Epoch 9/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0127 - val_loss: 0.0173\
Epoch 10/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0121 - val_loss: 0.0172\
Epoch 11/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0102 - val_loss: 0.0171\
Epoch 12/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0086 - val_loss: 0.0174\
Epoch 13/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0071 - val_loss: 0.0175\
Epoch 14/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0052 - val_loss: 0.0174\
Fold 4: 0.017446483 loss, 0.77870756 auc\
Epoch 1/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.3380 - val_loss: 0.0463\
Epoch 2/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0318 - val_loss: 0.0250\
Epoch 3/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0219 - val_loss: 0.0205\
Epoch 4/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0192 - val_loss: 0.0190\
Epoch 5/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0177 - val_loss: 0.0182\
Epoch 6/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0165 - val_loss: 0.0177\
Epoch 7/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0154 - val_loss: 0.0174\
Epoch 8/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0144 - val_loss: 0.0171\
Epoch 9/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0133 - val_loss: 0.0171\
Epoch 10/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0119 - val_loss: 0.0171\
Epoch 11/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0100 - val_loss: 0.0167\
Epoch 12/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0095 - val_loss: 0.0167\
Epoch 13/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0092 - val_loss: 0.0167\
Epoch 14/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0089 - val_loss: 0.0167\
Epoch 15/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0089 - val_loss: 0.0167\
Fold 5: 0.01673979 loss, 0.81569445 auc\
[0.017216144, 0.01694516, 0.016656088, 0.017446483, 0.01673979] [0.789072, 0.80684197, 0.8155805, 0.77870756, 0.81569445]\
\cf10 [I 2020-11-05 17:29:41,522]\cf2  Trial 11 finished with value: 0.017000732943415642 and parameters: \{'layers': 0, 'neurons': 1062\}. Best is trial 11 with value: 0.017000732943415642.\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 3s 28ms/step - loss: 0.3386 - val_loss: 0.0530\
Epoch 2/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0318 - val_loss: 0.0247\
Epoch 3/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0218 - val_loss: 0.0204\
Epoch 4/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0191 - val_loss: 0.0192\
Epoch 5/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0175 - val_loss: 0.0184\
Epoch 6/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0163 - val_loss: 0.0180\
Epoch 7/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0150 - val_loss: 0.0177\
Epoch 8/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0143 - val_loss: 0.0183\
Epoch 9/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0129 - val_loss: 0.0173\
Epoch 10/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0114 - val_loss: 0.0174\
Epoch 11/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0100 - val_loss: 0.0174\
Epoch 12/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0079 - val_loss: 0.0172\
Epoch 13/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0074 - val_loss: 0.0172\
Epoch 14/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0071 - val_loss: 0.0173\
Epoch 15/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0068 - val_loss: 0.0173\
Fold 1: 0.017324163 loss, 0.7855108 auc\
Epoch 1/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.3390 - val_loss: 0.0501\
Epoch 2/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0318 - val_loss: 0.0242\
Epoch 3/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0219 - val_loss: 0.0205\
Epoch 4/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0191 - val_loss: 0.0189\
Epoch 5/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0176 - val_loss: 0.0182\
Epoch 6/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0163 - val_loss: 0.0176\
Epoch 7/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0152 - val_loss: 0.0175\
Epoch 8/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0193 - val_loss: 0.0180\
Epoch 9/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0160 - val_loss: 0.0174\
Epoch 10/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0156 - val_loss: 0.0174\
Epoch 11/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0156 - val_loss: 0.0172\
Epoch 12/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0152 - val_loss: 0.0171\
Epoch 13/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0150 - val_loss: 0.0170\
Epoch 14/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0148 - val_loss: 0.0169\
Epoch 15/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0146 - val_loss: 0.0169\
Epoch 16/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0144 - val_loss: 0.0169\
Epoch 17/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0144 - val_loss: 0.0169\
Epoch 18/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0142 - val_loss: 0.0168\
Epoch 19/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0142 - val_loss: 0.0168\
Epoch 20/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0141 - val_loss: 0.0168\
Epoch 21/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0141 - val_loss: 0.0168\
Epoch 22/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0141 - val_loss: 0.0168\
Epoch 23/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0141 - val_loss: 0.0168\
Epoch 24/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0141 - val_loss: 0.0168\
Epoch 25/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0142 - val_loss: 0.0168\
Epoch 26/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0140 - val_loss: 0.0168\
Epoch 27/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0140 - val_loss: 0.0168\
Epoch 28/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0140 - val_loss: 0.0168\
Epoch 29/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0141 - val_loss: 0.0168\
Epoch 30/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0140 - val_loss: 0.0168\
Epoch 31/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0139 - val_loss: 0.0168\
Epoch 32/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0139 - val_loss: 0.0168\
Epoch 33/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0139 - val_loss: 0.0167\
Epoch 34/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0139 - val_loss: 0.0167\
Epoch 35/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0140 - val_loss: 0.0167\
Epoch 36/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0138 - val_loss: 0.0167\
Epoch 37/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0138 - val_loss: 0.0167\
Epoch 38/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0138 - val_loss: 0.0167\
Epoch 39/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0138 - val_loss: 0.0167\
Epoch 40/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0137 - val_loss: 0.0167\
Fold 2: 0.016716197 loss, 0.82281476 auc\
Epoch 1/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.3393 - val_loss: 0.0472\
Epoch 2/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0319 - val_loss: 0.0246\
Epoch 3/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0218 - val_loss: 0.0206\
Epoch 4/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0191 - val_loss: 0.0191\
Epoch 5/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0176 - val_loss: 0.0183\
Epoch 6/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0163 - val_loss: 0.0179\
Epoch 7/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0152 - val_loss: 0.0175\
Epoch 8/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0139 - val_loss: 0.0172\
Epoch 9/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0129 - val_loss: 0.0174\
Epoch 10/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0116 - val_loss: 0.0171\
Epoch 11/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0101 - val_loss: 0.0172\
Epoch 12/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0086 - val_loss: 0.0173\
Epoch 13/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0065 - val_loss: 0.0171\
Fold 3: 0.017147336 loss, 0.7952149 auc\
Epoch 1/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.3419 - val_loss: 0.0504\
Epoch 2/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0318 - val_loss: 0.0243\
Epoch 3/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0218 - val_loss: 0.0207\
Epoch 4/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0192 - val_loss: 0.0191\
Epoch 5/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0177 - val_loss: 0.0183\
Epoch 6/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0165 - val_loss: 0.0179\
Epoch 7/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0154 - val_loss: 0.0176\
Epoch 8/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0142 - val_loss: 0.0173\
Epoch 9/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0129 - val_loss: 0.0171\
Epoch 10/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0118 - val_loss: 0.0172\
Epoch 11/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0110 - val_loss: 0.0173\
Epoch 12/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0091 - val_loss: 0.0169\
Epoch 13/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0085 - val_loss: 0.0169\
Epoch 14/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0081 - val_loss: 0.0170\
Epoch 15/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0078 - val_loss: 0.0170\
Fold 4: 0.017003352 loss, 0.799454 auc\
Epoch 1/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.3370 - val_loss: 0.0482\
Epoch 2/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0322 - val_loss: 0.0334\
Epoch 3/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0264 - val_loss: 0.0198\
Epoch 4/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0193 - val_loss: 0.0191\
Epoch 5/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0182 - val_loss: 0.0185\
Epoch 6/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0172 - val_loss: 0.0180\
Epoch 7/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0164 - val_loss: 0.0176\
Epoch 8/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0155 - val_loss: 0.0173\
Epoch 9/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0145 - val_loss: 0.0172\
Epoch 10/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0134 - val_loss: 0.0171\
Epoch 11/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0123 - val_loss: 0.0170\
Epoch 12/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0111 - val_loss: 0.0170\
Epoch 13/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0098 - val_loss: 0.0171\
Epoch 14/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0079 - val_loss: 0.0170\
Fold 5: 0.017016975 loss, 0.7956183 auc\
[0.017324163, 0.016716197, 0.017147336, 0.017003352, 0.017016975] [0.7855108, 0.82281476, 0.7952149, 0.799454, 0.7956183]\
\cf10 [I 2020-11-05 17:33:23,930]\cf2  Trial 12 finished with value: 0.017041604593396188 and parameters: \{'layers': 0, 'neurons': 1062\}. Best is trial 11 with value: 0.017000732943415642.\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.3323 - val_loss: 0.0462\
Epoch 2/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0309 - val_loss: 0.0240\
Epoch 3/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0216 - val_loss: 0.0203\
Epoch 4/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0190 - val_loss: 0.0189\
Epoch 5/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0175 - val_loss: 0.0184\
Epoch 6/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0163 - val_loss: 0.0179\
Epoch 7/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0151 - val_loss: 0.0177\
Epoch 8/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0139 - val_loss: 0.0173\
Epoch 9/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0127 - val_loss: 0.0176\
Epoch 10/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0115 - val_loss: 0.0171\
Epoch 11/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0101 - val_loss: 0.0177\
Epoch 12/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0086 - val_loss: 0.0174\
Epoch 13/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0064 - val_loss: 0.0172\
Fold 1: 0.017248677 loss, 0.79058224 auc\
Epoch 1/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.3299 - val_loss: 0.0451\
Epoch 2/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0305 - val_loss: 0.0237\
Epoch 3/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0214 - val_loss: 0.0202\
Epoch 4/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0189 - val_loss: 0.0188\
Epoch 5/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0174 - val_loss: 0.0182\
Epoch 6/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0161 - val_loss: 0.0175\
Epoch 7/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0149 - val_loss: 0.0173\
Epoch 8/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0136 - val_loss: 0.0171\
Epoch 9/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0124 - val_loss: 0.0170\
Epoch 10/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0116 - val_loss: 0.0170\
Epoch 11/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0097 - val_loss: 0.0170\
Epoch 12/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0075 - val_loss: 0.0168\
Epoch 13/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0070 - val_loss: 0.0169\
Epoch 14/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0067 - val_loss: 0.0169\
Epoch 15/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0064 - val_loss: 0.0170\
Fold 2: 0.016963497 loss, 0.797519 auc\
Epoch 1/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.3317 - val_loss: 0.0458\
Epoch 2/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0305 - val_loss: 0.0239\
Epoch 3/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0214 - val_loss: 0.0201\
Epoch 4/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0188 - val_loss: 0.0189\
Epoch 5/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0174 - val_loss: 0.0182\
Epoch 6/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0162 - val_loss: 0.0177\
Epoch 7/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0172 - val_loss: 0.0186\
Epoch 8/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0157 - val_loss: 0.0179\
Epoch 9/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0141 - val_loss: 0.0172\
Epoch 10/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0137 - val_loss: 0.0170\
Epoch 11/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0134 - val_loss: 0.0169\
Epoch 12/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0132 - val_loss: 0.0169\
Epoch 13/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0130 - val_loss: 0.0168\
Epoch 14/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0128 - val_loss: 0.0168\
Epoch 15/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0127 - val_loss: 0.0168\
Epoch 16/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0127 - val_loss: 0.0168\
Epoch 17/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0127 - val_loss: 0.0168\
Epoch 18/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0127 - val_loss: 0.0168\
Epoch 19/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0127 - val_loss: 0.0168\
Epoch 20/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0126 - val_loss: 0.0168\
Epoch 21/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0126 - val_loss: 0.0168\
Epoch 22/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0126 - val_loss: 0.0168\
Epoch 23/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0126 - val_loss: 0.0168\
Epoch 24/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0126 - val_loss: 0.0168\
Epoch 25/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0125 - val_loss: 0.0168\
Epoch 26/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0125 - val_loss: 0.0168\
Epoch 27/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0125 - val_loss: 0.0168\
Epoch 28/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0125 - val_loss: 0.0168\
Epoch 29/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0124 - val_loss: 0.0168\
Epoch 30/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0124 - val_loss: 0.0168\
Epoch 31/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0124 - val_loss: 0.0168\
Epoch 32/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0124 - val_loss: 0.0168\
Epoch 33/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0123 - val_loss: 0.0168\
Epoch 34/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0123 - val_loss: 0.0168\
Epoch 35/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0123 - val_loss: 0.0168\
Epoch 36/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0123 - val_loss: 0.0168\
Epoch 37/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0122 - val_loss: 0.0168\
Epoch 38/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0122 - val_loss: 0.0168\
Epoch 39/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0122 - val_loss: 0.0168\
Epoch 40/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0122 - val_loss: 0.0168\
Fold 3: 0.016774906 loss, 0.81866115 auc\
Epoch 1/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.3322 - val_loss: 0.0472\
Epoch 2/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0309 - val_loss: 0.0238\
Epoch 3/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0215 - val_loss: 0.0203\
Epoch 4/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0189 - val_loss: 0.0190\
Epoch 5/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0174 - val_loss: 0.0182\
Epoch 6/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0161 - val_loss: 0.0179\
Epoch 7/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0149 - val_loss: 0.0175\
Epoch 8/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0136 - val_loss: 0.0174\
Epoch 9/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0126 - val_loss: 0.0173\
Epoch 10/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0113 - val_loss: 0.0176\
Epoch 11/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0094 - val_loss: 0.0171\
Epoch 12/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0087 - val_loss: 0.0171\
Epoch 13/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0084 - val_loss: 0.0171\
Epoch 14/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0080 - val_loss: 0.0171\
Epoch 15/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0080 - val_loss: 0.0171\
Fold 4: 0.017106352 loss, 0.79509336 auc\
Epoch 1/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.3296 - val_loss: 0.0461\
Epoch 2/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0324 - val_loss: 0.0241\
Epoch 3/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0233 - val_loss: 0.0219\
Epoch 4/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0194 - val_loss: 0.0189\
Epoch 5/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0180 - val_loss: 0.0183\
Epoch 6/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0170 - val_loss: 0.0179\
Epoch 7/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0161 - val_loss: 0.0175\
Epoch 8/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0151 - val_loss: 0.0173\
Epoch 9/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0141 - val_loss: 0.0170\
Epoch 10/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0129 - val_loss: 0.0169\
Epoch 11/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0117 - val_loss: 0.0169\
Epoch 12/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0098 - val_loss: 0.0166\
Epoch 13/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0094 - val_loss: 0.0167\
Epoch 14/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0091 - val_loss: 0.0167\
Epoch 15/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0089 - val_loss: 0.0167\
Fold 5: 0.016695399 loss, 0.8144592 auc\
[0.017248677, 0.016963497, 0.016774906, 0.017106352, 0.016695399] [0.79058224, 0.797519, 0.81866115, 0.79509336, 0.8144592]\
\cf10 [I 2020-11-05 17:37:22,566]\cf2  Trial 13 finished with value: 0.016957766190171242 and parameters: \{'layers': 0, 'neurons': 1162\}. Best is trial 13 with value: 0.016957766190171242.\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 3s 33ms/step - loss: 0.3271 - val_loss: 0.0475\
Epoch 2/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0298 - val_loss: 0.0232\
Epoch 3/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0212 - val_loss: 0.0202\
Epoch 4/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0188 - val_loss: 0.0189\
Epoch 5/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0174 - val_loss: 0.0182\
Epoch 6/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0162 - val_loss: 0.0179\
Epoch 7/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0150 - val_loss: 0.0174\
Epoch 8/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0137 - val_loss: 0.0174\
Epoch 9/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0125 - val_loss: 0.0174\
Epoch 10/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0106 - val_loss: 0.0170\
Epoch 11/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0101 - val_loss: 0.0169\
Epoch 12/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0097 - val_loss: 0.0170\
Epoch 13/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0094 - val_loss: 0.0170\
Epoch 14/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0094 - val_loss: 0.0170\
Fold 1: 0.01696349 loss, 0.809808 auc\
Epoch 1/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.3258 - val_loss: 0.0445\
Epoch 2/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0295 - val_loss: 0.0230\
Epoch 3/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0210 - val_loss: 0.0200\
Epoch 4/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0186 - val_loss: 0.0187\
Epoch 5/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0171 - val_loss: 0.0181\
Epoch 6/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0159 - val_loss: 0.0176\
Epoch 7/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0146 - val_loss: 0.0175\
Epoch 8/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0136 - val_loss: 0.0172\
Epoch 9/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.0122 - val_loss: 0.0170\
Epoch 10/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0108 - val_loss: 0.0169\
Epoch 11/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0095 - val_loss: 0.0172\
Epoch 12/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0075 - val_loss: 0.0168\
Epoch 13/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0068 - val_loss: 0.0168\
Epoch 14/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0064 - val_loss: 0.0168\
Epoch 15/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0061 - val_loss: 0.0168\
Epoch 16/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0060 - val_loss: 0.0168\
Fold 2: 0.016847793 loss, 0.80005723 auc\
Epoch 1/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.3275 - val_loss: 0.0445\
Epoch 2/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0297 - val_loss: 0.0233\
Epoch 3/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0212 - val_loss: 0.0201\
Epoch 4/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0189 - val_loss: 0.0187\
Epoch 5/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0175 - val_loss: 0.0181\
Epoch 6/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0164 - val_loss: 0.0176\
Epoch 7/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0152 - val_loss: 0.0173\
Epoch 8/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0139 - val_loss: 0.0172\
Epoch 9/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0127 - val_loss: 0.0170\
Epoch 10/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0116 - val_loss: 0.0170\
Epoch 11/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0101 - val_loss: 0.0170\
Epoch 12/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0080 - val_loss: 0.0168\
Epoch 13/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0073 - val_loss: 0.0168\
Epoch 14/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0070 - val_loss: 0.0168\
Epoch 15/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0067 - val_loss: 0.0169\
Fold 3: 0.016870495 loss, 0.79966134 auc\
Epoch 1/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.3247 - val_loss: 0.0439\
Epoch 2/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0294 - val_loss: 0.0234\
Epoch 3/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0211 - val_loss: 0.0200\
Epoch 4/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0187 - val_loss: 0.0189\
Epoch 5/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0173 - val_loss: 0.0182\
Epoch 6/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0161 - val_loss: 0.0178\
Epoch 7/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0149 - val_loss: 0.0174\
Epoch 8/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0138 - val_loss: 0.0173\
Epoch 9/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0126 - val_loss: 0.0174\
Epoch 10/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0120 - val_loss: 0.0172\
Epoch 11/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0098 - val_loss: 0.0169\
Epoch 12/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0092 - val_loss: 0.0169\
Epoch 13/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0088 - val_loss: 0.0169\
Epoch 14/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0085 - val_loss: 0.0169\
Epoch 15/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0085 - val_loss: 0.0169\
Fold 4: 0.016919892 loss, 0.8043265 auc\
Epoch 1/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.3294 - val_loss: 0.0439\
Epoch 2/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0304 - val_loss: 0.0233\
Epoch 3/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0213 - val_loss: 0.0201\
Epoch 4/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0188 - val_loss: 0.0190\
Epoch 5/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0174 - val_loss: 0.0181\
Epoch 6/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0162 - val_loss: 0.0177\
Epoch 7/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0150 - val_loss: 0.0174\
Epoch 8/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0137 - val_loss: 0.0171\
Epoch 9/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0124 - val_loss: 0.0173\
Epoch 10/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0113 - val_loss: 0.0171\
Epoch 11/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0092 - val_loss: 0.0169\
Epoch 12/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0086 - val_loss: 0.0168\
Epoch 13/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0082 - val_loss: 0.0168\
Epoch 14/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0079 - val_loss: 0.0169\
Epoch 15/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0078 - val_loss: 0.0169\
Fold 5: 0.016865669 loss, 0.80465114 auc\
[0.01696349, 0.016847793, 0.016870495, 0.016919892, 0.016865669] [0.809808, 0.80005723, 0.79966134, 0.8043265, 0.80465114]\
\cf10 [I 2020-11-05 17:40:31,139]\cf2  Trial 14 finished with value: 0.01689346767961979 and parameters: \{'layers': 0, 'neurons': 1262\}. Best is trial 14 with value: 0.01689346767961979.\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 4s 49ms/step - loss: 0.3198 - val_loss: 0.0427\
Epoch 2/40\
88/88 [==============================] - 4s 43ms/step - loss: 0.0287 - val_loss: 0.0229\
Epoch 3/40\
88/88 [==============================] - 4s 42ms/step - loss: 0.0206 - val_loss: 0.0196\
Epoch 4/40\
88/88 [==============================] - 4s 41ms/step - loss: 0.0181 - val_loss: 0.0187\
Epoch 5/40\
88/88 [==============================] - 4s 43ms/step - loss: 0.0164 - val_loss: 0.0177\
Epoch 6/40\
88/88 [==============================] - 4s 42ms/step - loss: 0.0147 - val_loss: 0.0176\
Epoch 7/40\
88/88 [==============================] - 4s 43ms/step - loss: 0.0129 - val_loss: 0.0175\
Epoch 8/40\
88/88 [==============================] - 4s 42ms/step - loss: 0.0107 - val_loss: 0.0176\
Epoch 9/40\
88/88 [==============================] - 4s 42ms/step - loss: 0.0078 - val_loss: 0.0173\
Epoch 10/40\
88/88 [==============================] - 4s 46ms/step - loss: 0.0069 - val_loss: 0.0173\
Epoch 11/40\
88/88 [==============================] - 4s 46ms/step - loss: 0.0062 - val_loss: 0.0174\
Epoch 12/40\
88/88 [==============================] - 4s 44ms/step - loss: 0.0058 - val_loss: 0.0174\
Fold 1: 0.017389292 loss, 0.7751448 auc\
Epoch 1/40\
88/88 [==============================] - 5s 52ms/step - loss: 0.3213 - val_loss: 0.0448\
Epoch 2/40\
88/88 [==============================] - 4s 46ms/step - loss: 0.0288 - val_loss: 0.0229\
Epoch 3/40\
88/88 [==============================] - 4s 45ms/step - loss: 0.0206 - val_loss: 0.0201\
Epoch 4/40\
88/88 [==============================] - 4s 44ms/step - loss: 0.0181 - val_loss: 0.0186\
Epoch 5/40\
88/88 [==============================] - 4s 43ms/step - loss: 0.0166 - val_loss: 0.0181\
Epoch 6/40\
88/88 [==============================] - 4s 42ms/step - loss: 0.0149 - val_loss: 0.0175\
Epoch 7/40\
88/88 [==============================] - 4s 44ms/step - loss: 0.0129 - val_loss: 0.0175\
Epoch 8/40\
88/88 [==============================] - 4s 46ms/step - loss: 0.0109 - val_loss: 0.0178\
Epoch 9/40\
88/88 [==============================] - 4s 51ms/step - loss: 0.0080 - val_loss: 0.0173\
Epoch 10/40\
88/88 [==============================] - 4s 45ms/step - loss: 0.0070 - val_loss: 0.0173\
Epoch 11/40\
88/88 [==============================] - 4s 44ms/step - loss: 0.0064 - val_loss: 0.0174\
Epoch 12/40\
88/88 [==============================] - 4s 50ms/step - loss: 0.0059 - val_loss: 0.0174\
\cf10 [I 2020-11-05 17:42:24,057]\cf2  Trial 15 pruned. \
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 7s 85ms/step - loss: 0.3001 - val_loss: 0.0349\
Epoch 2/40\
88/88 [==============================] - 7s 79ms/step - loss: 0.0253 - val_loss: 0.0216\
Epoch 3/40\
88/88 [==============================] - 7s 80ms/step - loss: 0.0194 - val_loss: 0.0191\
Epoch 4/40\
88/88 [==============================] - 7s 78ms/step - loss: 0.0172 - val_loss: 0.0180\
Epoch 5/40\
88/88 [==============================] - 7s 78ms/step - loss: 0.0157 - val_loss: 0.0179\
Epoch 6/40\
88/88 [==============================] - 7s 78ms/step - loss: 0.0145 - val_loss: 0.0174\
Epoch 7/40\
88/88 [==============================] - 7s 79ms/step - loss: 0.0119 - val_loss: 0.0175\
Epoch 8/40\
88/88 [==============================] - 7s 79ms/step - loss: 0.0095 - val_loss: 0.0177\
Epoch 9/40\
88/88 [==============================] - 7s 79ms/step - loss: 0.0064 - val_loss: 0.0173\
Epoch 10/40\
88/88 [==============================] - 7s 79ms/step - loss: 0.0054 - val_loss: 0.0174\
Epoch 11/40\
88/88 [==============================] - 7s 79ms/step - loss: 0.0049 - val_loss: 0.0174\
Epoch 12/40\
88/88 [==============================] - 7s 78ms/step - loss: 0.0048 - val_loss: 0.0174\
Fold 1: 0.017419554 loss, 0.770443 auc\
Epoch 1/40\
88/88 [==============================] - 7s 85ms/step - loss: 0.2986 - val_loss: 0.0394\
Epoch 2/40\
88/88 [==============================] - 7s 79ms/step - loss: 0.0258 - val_loss: 0.0216\
Epoch 3/40\
88/88 [==============================] - 7s 80ms/step - loss: 0.0198 - val_loss: 0.0196\
Epoch 4/40\
88/88 [==============================] - 7s 78ms/step - loss: 0.0175 - val_loss: 0.0184\
Epoch 5/40\
88/88 [==============================] - 7s 79ms/step - loss: 0.0160 - val_loss: 0.0178\
Epoch 6/40\
88/88 [==============================] - 7s 79ms/step - loss: 0.0145 - val_loss: 0.0176\
Epoch 7/40\
88/88 [==============================] - 7s 81ms/step - loss: 0.0124 - val_loss: 0.0177\
Epoch 8/40\
88/88 [==============================] - 7s 79ms/step - loss: 0.0109 - val_loss: 0.0180\
Epoch 9/40\
88/88 [==============================] - 7s 79ms/step - loss: 0.0074 - val_loss: 0.0174\
Epoch 10/40\
88/88 [==============================] - 7s 81ms/step - loss: 0.0063 - val_loss: 0.0175\
Epoch 11/40\
88/88 [==============================] - 7s 80ms/step - loss: 0.0056 - val_loss: 0.0176\
Epoch 12/40\
88/88 [==============================] - 7s 80ms/step - loss: 0.0051 - val_loss: 0.0176\
\cf10 [I 2020-11-05 17:45:33,045]\cf2  Trial 16 pruned. \
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.3114 - val_loss: 0.0437\
Epoch 2/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0277 - val_loss: 0.0227\
Epoch 3/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0206 - val_loss: 0.0197\
Epoch 4/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0184 - val_loss: 0.0185\
Epoch 5/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0170 - val_loss: 0.0179\
Epoch 6/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0157 - val_loss: 0.0175\
Epoch 7/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0145 - val_loss: 0.0174\
Epoch 8/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0141 - val_loss: 0.0171\
Epoch 9/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0122 - val_loss: 0.0170\
Epoch 10/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0106 - val_loss: 0.0170\
Epoch 11/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0100 - val_loss: 0.0173\
Epoch 12/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0086 - val_loss: 0.0174\
Fold 1: 0.017400347 loss, 0.80092055 auc\
Epoch 1/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.3104 - val_loss: 0.0417\
Epoch 2/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0274 - val_loss: 0.0223\
Epoch 3/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0205 - val_loss: 0.0196\
Epoch 4/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0184 - val_loss: 0.0187\
Epoch 5/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0171 - val_loss: 0.0181\
Epoch 6/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0159 - val_loss: 0.0176\
Epoch 7/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0146 - val_loss: 0.0176\
Epoch 8/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0133 - val_loss: 0.0172\
Epoch 9/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0118 - val_loss: 0.0173\
Epoch 10/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0105 - val_loss: 0.0176\
Epoch 11/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0081 - val_loss: 0.0170\
Epoch 12/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0075 - val_loss: 0.0170\
Epoch 13/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0072 - val_loss: 0.0171\
Epoch 14/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0068 - val_loss: 0.0171\
Fold 2: 0.017124817 loss, 0.7921288 auc\
Epoch 1/40\
88/88 [==============================] - 4s 40ms/step - loss: 0.3123 - val_loss: 0.0393\
Epoch 2/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0280 - val_loss: 0.0257\
Epoch 3/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0218 - val_loss: 0.0241\
Epoch 4/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0188 - val_loss: 0.0186\
Epoch 5/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0172 - val_loss: 0.0181\
Epoch 6/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0161 - val_loss: 0.0178\
Epoch 7/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0150 - val_loss: 0.0173\
Epoch 8/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0138 - val_loss: 0.0173\
Epoch 9/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0124 - val_loss: 0.0172\
Epoch 10/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0110 - val_loss: 0.0171\
Epoch 11/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0105 - val_loss: 0.0189\
Epoch 12/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0097 - val_loss: 0.0181\
Epoch 13/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0088 - val_loss: 0.0172\
Fold 3: 0.017239902 loss, 0.79846984 auc\
Epoch 1/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.3141 - val_loss: 0.0419\
Epoch 2/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0276 - val_loss: 0.0225\
Epoch 3/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0204 - val_loss: 0.0195\
Epoch 4/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0183 - val_loss: 0.0186\
Epoch 5/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0169 - val_loss: 0.0179\
Epoch 6/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0156 - val_loss: 0.0174\
Epoch 7/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0143 - val_loss: 0.0172\
Epoch 8/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0130 - val_loss: 0.0169\
Epoch 9/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0117 - val_loss: 0.0173\
Epoch 10/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0110 - val_loss: 0.0171\
Epoch 11/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0084 - val_loss: 0.0166\
Epoch 12/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0077 - val_loss: 0.0166\
Epoch 13/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0074 - val_loss: 0.0166\
Epoch 14/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0070 - val_loss: 0.0167\
Epoch 15/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0070 - val_loss: 0.0167\
Fold 4: 0.016681666 loss, 0.80528843 auc\
Epoch 1/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.3151 - val_loss: 0.0389\
Epoch 2/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0276 - val_loss: 0.0223\
Epoch 3/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0205 - val_loss: 0.0197\
Epoch 4/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0184 - val_loss: 0.0187\
Epoch 5/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0171 - val_loss: 0.0180\
Epoch 6/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0159 - val_loss: 0.0175\
Epoch 7/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0147 - val_loss: 0.0174\
Epoch 8/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0133 - val_loss: 0.0172\
Epoch 9/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0119 - val_loss: 0.0171\
Epoch 10/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0108 - val_loss: 0.0172\
Epoch 11/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0084 - val_loss: 0.0169\
Epoch 12/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0079 - val_loss: 0.0169\
Epoch 13/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0075 - val_loss: 0.0169\
Epoch 14/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0072 - val_loss: 0.0169\
Fold 5: 0.01692254 loss, 0.8017408 auc\
[0.017400347, 0.017124817, 0.017239902, 0.016681666, 0.01692254] [0.80092055, 0.7921288, 0.79846984, 0.80528843, 0.8017408]\
\cf10 [I 2020-11-05 17:48:44,040]\cf2  Trial 17 finished with value: 0.01707385405898094 and parameters: \{'layers': 0, 'neurons': 1562\}. Best is trial 14 with value: 0.01689346767961979.\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 4s 45ms/step - loss: 0.3247 - val_loss: 0.0448\
Epoch 2/40\
88/88 [==============================] - 3s 39ms/step - loss: 0.0297 - val_loss: 0.0234\
Epoch 3/40\
88/88 [==============================] - 3s 39ms/step - loss: 0.0209 - val_loss: 0.0200\
Epoch 4/40\
88/88 [==============================] - 3s 38ms/step - loss: 0.0183 - val_loss: 0.0185\
Epoch 5/40\
88/88 [==============================] - 3s 38ms/step - loss: 0.0166 - val_loss: 0.0178\
Epoch 6/40\
88/88 [==============================] - 3s 39ms/step - loss: 0.0150 - val_loss: 0.0177\
Epoch 7/40\
88/88 [==============================] - 3s 38ms/step - loss: 0.0131 - val_loss: 0.0179\
Epoch 8/40\
88/88 [==============================] - 3s 39ms/step - loss: 0.0113 - val_loss: 0.0176\
Epoch 9/40\
88/88 [==============================] - 3s 39ms/step - loss: 0.0084 - val_loss: 0.0174\
Epoch 10/40\
88/88 [==============================] - 3s 38ms/step - loss: 0.0074 - val_loss: 0.0175\
Epoch 11/40\
88/88 [==============================] - 3s 38ms/step - loss: 0.0068 - val_loss: 0.0175\
Epoch 12/40\
88/88 [==============================] - 3s 38ms/step - loss: 0.0064 - val_loss: 0.0175\
\cf10 [I 2020-11-05 17:49:36,446]\cf2  Trial 18 pruned. \
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 3s 36ms/step - loss: 0.3034 - val_loss: 0.0374\
Epoch 2/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.0263 - val_loss: 0.0219\
Epoch 3/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0201 - val_loss: 0.0196\
Epoch 4/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0180 - val_loss: 0.0184\
Epoch 5/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0167 - val_loss: 0.0178\
Epoch 6/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.0154 - val_loss: 0.0175\
Epoch 7/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0141 - val_loss: 0.0173\
Epoch 8/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.0127 - val_loss: 0.0173\
Epoch 9/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.0113 - val_loss: 0.0171\
Epoch 10/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.0097 - val_loss: 0.0174\
Epoch 11/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.0081 - val_loss: 0.0173\
Epoch 12/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.0058 - val_loss: 0.0172\
Fold 1: 0.017151576 loss, 0.79111284 auc\
Epoch 1/40\
88/88 [==============================] - 3s 37ms/step - loss: 0.3007 - val_loss: 0.0380\
Epoch 2/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.0263 - val_loss: 0.0218\
Epoch 3/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.0201 - val_loss: 0.0196\
Epoch 4/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0181 - val_loss: 0.0185\
Epoch 5/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0168 - val_loss: 0.0180\
Epoch 6/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0156 - val_loss: 0.0175\
Epoch 7/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0142 - val_loss: 0.0174\
Epoch 8/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0130 - val_loss: 0.0171\
Epoch 9/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0113 - val_loss: 0.0173\
Epoch 10/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.0099 - val_loss: 0.0174\
Epoch 11/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0076 - val_loss: 0.0170\
Epoch 12/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0069 - val_loss: 0.0170\
Epoch 13/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0066 - val_loss: 0.0170\
Epoch 14/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0062 - val_loss: 0.0170\
Epoch 15/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0062 - val_loss: 0.0170\
Fold 2: 0.01704954 loss, 0.79299873 auc\
Epoch 1/40\
88/88 [==============================] - 3s 36ms/step - loss: 0.3047 - val_loss: 0.0347\
Epoch 2/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0268 - val_loss: 0.0222\
Epoch 3/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0200 - val_loss: 0.0192\
Epoch 4/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.0180 - val_loss: 0.0183\
Epoch 5/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.0167 - val_loss: 0.0180\
Epoch 6/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.0155 - val_loss: 0.0176\
Epoch 7/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0142 - val_loss: 0.0173\
Epoch 8/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.0128 - val_loss: 0.0172\
Epoch 9/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0114 - val_loss: 0.0175\
Epoch 10/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0094 - val_loss: 0.0169\
Epoch 11/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0087 - val_loss: 0.0169\
Epoch 12/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0083 - val_loss: 0.0169\
Epoch 13/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0079 - val_loss: 0.0169\
Epoch 14/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0079 - val_loss: 0.0169\
Fold 3: 0.016924577 loss, 0.8013641 auc\
Epoch 1/40\
88/88 [==============================] - 4s 43ms/step - loss: 0.3053 - val_loss: 0.0390\
Epoch 2/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0264 - val_loss: 0.0217\
Epoch 3/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.0200 - val_loss: 0.0195\
Epoch 4/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.0180 - val_loss: 0.0185\
Epoch 5/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.0166 - val_loss: 0.0179\
Epoch 6/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0153 - val_loss: 0.0175\
Epoch 7/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.0139 - val_loss: 0.0173\
Epoch 8/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0126 - val_loss: 0.0171\
Epoch 9/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.0111 - val_loss: 0.0173\
Epoch 10/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.0185 - val_loss: 0.0189\
Epoch 11/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.0141 - val_loss: 0.0177\
Fold 4: 0.017743872 loss, 0.79331607 auc\
Epoch 1/40\
88/88 [==============================] - 3s 36ms/step - loss: 0.3041 - val_loss: 0.0363\
Epoch 2/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0263 - val_loss: 0.0217\
Epoch 3/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0201 - val_loss: 0.0193\
Epoch 4/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0181 - val_loss: 0.0184\
Epoch 5/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0168 - val_loss: 0.0177\
Epoch 6/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0155 - val_loss: 0.0174\
Epoch 7/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0142 - val_loss: 0.0172\
Epoch 8/40\
88/88 [==============================] - 3s 28ms/step - loss: 0.0128 - val_loss: 0.0171\
Epoch 9/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0115 - val_loss: 0.0171\
Epoch 10/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0101 - val_loss: 0.0225\
Epoch 11/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0097 - val_loss: 0.0190\
Fold 5: 0.018959608 loss, 0.8150061 auc\
[0.017151576, 0.01704954, 0.016924577, 0.017743872, 0.018959608] [0.79111284, 0.79299873, 0.8013641, 0.79331607, 0.8150061]\
\cf10 [I 2020-11-05 17:52:58,049]\cf2  Trial 19 finished with value: 0.017565834522247314 and parameters: \{'layers': 0, 'neurons': 1812\}. Best is trial 14 with value: 0.01689346767961979.\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 4s 42ms/step - loss: 0.3277 - val_loss: 0.0465\
Epoch 2/40\
88/88 [==============================] - 3s 36ms/step - loss: 0.0299 - val_loss: 0.0232\
Epoch 3/40\
88/88 [==============================] - 3s 36ms/step - loss: 0.0210 - val_loss: 0.0200\
Epoch 4/40\
88/88 [==============================] - 3s 37ms/step - loss: 0.0183 - val_loss: 0.0189\
Epoch 5/40\
88/88 [==============================] - 3s 36ms/step - loss: 0.0165 - val_loss: 0.0180\
Epoch 6/40\
88/88 [==============================] - 3s 37ms/step - loss: 0.0148 - val_loss: 0.0176\
Epoch 7/40\
88/88 [==============================] - 3s 36ms/step - loss: 0.0132 - val_loss: 0.0175\
Epoch 8/40\
88/88 [==============================] - 3s 36ms/step - loss: 0.0111 - val_loss: 0.0176\
Epoch 9/40\
88/88 [==============================] - 3s 37ms/step - loss: 0.0081 - val_loss: 0.0174\
Epoch 10/40\
88/88 [==============================] - 3s 36ms/step - loss: 0.0071 - val_loss: 0.0174\
Epoch 11/40\
88/88 [==============================] - 3s 36ms/step - loss: 0.0066 - val_loss: 0.0175\
Epoch 12/40\
88/88 [==============================] - 3s 36ms/step - loss: 0.0061 - val_loss: 0.0175\
\cf10 [I 2020-11-05 17:53:48,035]\cf2  Trial 20 pruned. \
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.3361 - val_loss: 0.0469\
Epoch 2/40\
88/88 [==============================] - 2s 18ms/step - loss: 0.0309 - val_loss: 0.0240\
Epoch 3/40\
88/88 [==============================] - 2s 18ms/step - loss: 0.0215 - val_loss: 0.0203\
Epoch 4/40\
88/88 [==============================] - 2s 18ms/step - loss: 0.0189 - val_loss: 0.0190\
Epoch 5/40\
88/88 [==============================] - 2s 18ms/step - loss: 0.0175 - val_loss: 0.0184\
Epoch 6/40\
88/88 [==============================] - 2s 18ms/step - loss: 0.0162 - val_loss: 0.0178\
Epoch 7/40\
88/88 [==============================] - 2s 19ms/step - loss: 0.0150 - val_loss: 0.0176\
Epoch 8/40\
88/88 [==============================] - 2s 18ms/step - loss: 0.0139 - val_loss: 0.0176\
Epoch 9/40\
88/88 [==============================] - 2s 18ms/step - loss: 0.0130 - val_loss: 0.0173\
Epoch 10/40\
88/88 [==============================] - 2s 18ms/step - loss: 0.0118 - val_loss: 0.0172\
Epoch 11/40\
88/88 [==============================] - 2s 18ms/step - loss: 0.0100 - val_loss: 0.0174\
Epoch 12/40\
88/88 [==============================] - 2s 18ms/step - loss: 0.0086 - val_loss: 0.0175\
Epoch 13/40\
88/88 [==============================] - 2s 17ms/step - loss: 0.0064 - val_loss: 0.0174\
\cf10 [I 2020-11-05 17:54:18,579]\cf2  Trial 21 pruned. \
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.3509 - val_loss: 0.0545\
Epoch 2/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0337 - val_loss: 0.0249\
Epoch 3/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0224 - val_loss: 0.0209\
Epoch 4/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0194 - val_loss: 0.0192\
Epoch 5/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0178 - val_loss: 0.0184\
Epoch 6/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0166 - val_loss: 0.0178\
Epoch 7/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0155 - val_loss: 0.0174\
Epoch 8/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0145 - val_loss: 0.0173\
Epoch 9/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0144 - val_loss: 0.0181\
Epoch 10/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0134 - val_loss: 0.0174\
Epoch 11/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0128 - val_loss: 0.0169\
Epoch 12/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0123 - val_loss: 0.0168\
Epoch 13/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0120 - val_loss: 0.0169\
Epoch 14/40\
88/88 [==============================] - 1s 17ms/step - loss: 0.0117 - val_loss: 0.0168\
Epoch 15/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0116 - val_loss: 0.0168\
Epoch 16/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0116 - val_loss: 0.0168\
Epoch 17/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0115 - val_loss: 0.0168\
Epoch 18/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0116 - val_loss: 0.0168\
Epoch 19/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0115 - val_loss: 0.0167\
Epoch 20/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0115 - val_loss: 0.0167\
Epoch 21/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0115 - val_loss: 0.0167\
Epoch 22/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0115 - val_loss: 0.0167\
Epoch 23/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0114 - val_loss: 0.0168\
Epoch 24/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0115 - val_loss: 0.0168\
Fold 1: 0.016751539 loss, 0.8212907 auc\
Epoch 1/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.3515 - val_loss: 0.0580\
Epoch 2/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0337 - val_loss: 0.0249\
Epoch 3/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0224 - val_loss: 0.0211\
Epoch 4/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0194 - val_loss: 0.0194\
Epoch 5/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0178 - val_loss: 0.0186\
Epoch 6/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0165 - val_loss: 0.0180\
Epoch 7/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0154 - val_loss: 0.0175\
Epoch 8/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0142 - val_loss: 0.0176\
Epoch 9/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0172 - val_loss: 0.0177\
Epoch 10/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0145 - val_loss: 0.0174\
Epoch 11/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0140 - val_loss: 0.0172\
Epoch 12/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0137 - val_loss: 0.0172\
Epoch 13/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0135 - val_loss: 0.0170\
Epoch 14/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0132 - val_loss: 0.0170\
Epoch 15/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0130 - val_loss: 0.0169\
Epoch 16/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0128 - val_loss: 0.0169\
Epoch 17/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0128 - val_loss: 0.0169\
Epoch 18/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0128 - val_loss: 0.0169\
Fold 2: 0.016918274 loss, 0.82133657 auc\
Epoch 1/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.3481 - val_loss: 0.0504\
Epoch 2/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0335 - val_loss: 0.0252\
Epoch 3/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0223 - val_loss: 0.0207\
Epoch 4/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0194 - val_loss: 0.0191\
Epoch 5/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0178 - val_loss: 0.0182\
Epoch 6/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0166 - val_loss: 0.0177\
Epoch 7/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0154 - val_loss: 0.0173\
Epoch 8/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0144 - val_loss: 0.0171\
Epoch 9/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0132 - val_loss: 0.0171\
Epoch 10/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0123 - val_loss: 0.0175\
Epoch 11/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0106 - val_loss: 0.0168\
Epoch 12/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0099 - val_loss: 0.0167\
Epoch 13/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0096 - val_loss: 0.0168\
Epoch 14/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0093 - val_loss: 0.0168\
Epoch 15/40\
88/88 [==============================] - 1s 17ms/step - loss: 0.0092 - val_loss: 0.0168\
Fold 3: 0.016781868 loss, 0.80865896 auc\
Epoch 1/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.3514 - val_loss: 0.0510\
Epoch 2/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0341 - val_loss: 0.0251\
Epoch 3/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0225 - val_loss: 0.0209\
Epoch 4/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0195 - val_loss: 0.0194\
Epoch 5/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0178 - val_loss: 0.0186\
Epoch 6/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0166 - val_loss: 0.0181\
Epoch 7/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0155 - val_loss: 0.0177\
Epoch 8/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0144 - val_loss: 0.0174\
Epoch 9/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0133 - val_loss: 0.0173\
Epoch 10/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0120 - val_loss: 0.0174\
Epoch 11/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0109 - val_loss: 0.0173\
Epoch 12/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0087 - val_loss: 0.0170\
Epoch 13/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0082 - val_loss: 0.0170\
Epoch 14/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0079 - val_loss: 0.0170\
Epoch 15/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0076 - val_loss: 0.0170\
Fold 4: 0.017038582 loss, 0.80707735 auc\
Epoch 1/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.3512 - val_loss: 0.0527\
Epoch 2/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0336 - val_loss: 0.0249\
Epoch 3/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0222 - val_loss: 0.0205\
Epoch 4/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0193 - val_loss: 0.0191\
Epoch 5/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0178 - val_loss: 0.0183\
Epoch 6/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0166 - val_loss: 0.0178\
Epoch 7/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0154 - val_loss: 0.0174\
Epoch 8/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0144 - val_loss: 0.0171\
Epoch 9/40\
88/88 [==============================] - 1s 17ms/step - loss: 0.0132 - val_loss: 0.0172\
Epoch 10/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0120 - val_loss: 0.0171\
Epoch 11/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0101 - val_loss: 0.0167\
Epoch 12/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0096 - val_loss: 0.0167\
Epoch 13/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0092 - val_loss: 0.0167\
Epoch 14/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0090 - val_loss: 0.0167\
Epoch 15/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0089 - val_loss: 0.0168\
Fold 5: 0.016753564 loss, 0.8119843 auc\
[0.016751539, 0.016918274, 0.016781868, 0.017038582, 0.016753564] [0.8212907, 0.82133657, 0.80865896, 0.80707735, 0.8119843]\
\cf10 [I 2020-11-05 17:56:43,729]\cf2  Trial 22 finished with value: 0.016848765313625336 and parameters: \{'layers': 0, 'neurons': 912\}. Best is trial 22 with value: 0.016848765313625336.\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.3550 - val_loss: 0.0525\
Epoch 2/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0346 - val_loss: 0.0254\
Epoch 3/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0227 - val_loss: 0.0210\
Epoch 4/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0196 - val_loss: 0.0194\
Epoch 5/40\
88/88 [==============================] - 1s 14ms/step - loss: 0.0180 - val_loss: 0.0187\
Epoch 6/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0168 - val_loss: 0.0180\
Epoch 7/40\
88/88 [==============================] - 1s 14ms/step - loss: 0.0157 - val_loss: 0.0177\
Epoch 8/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0150 - val_loss: 0.0172\
Epoch 9/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0136 - val_loss: 0.0173\
Epoch 10/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0124 - val_loss: 0.0171\
Epoch 11/40\
88/88 [==============================] - 1s 14ms/step - loss: 0.0110 - val_loss: 0.0170\
Epoch 12/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0100 - val_loss: 0.0173\
Epoch 13/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0084 - val_loss: 0.0174\
Epoch 14/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0064 - val_loss: 0.0172\
Fold 1: 0.017199123 loss, 0.79408467 auc\
Epoch 1/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.3548 - val_loss: 0.0585\
Epoch 2/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0346 - val_loss: 0.0254\
Epoch 3/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0226 - val_loss: 0.0210\
Epoch 4/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0196 - val_loss: 0.0194\
Epoch 5/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0179 - val_loss: 0.0184\
Epoch 6/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0167 - val_loss: 0.0180\
Epoch 7/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0156 - val_loss: 0.0176\
Epoch 8/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0144 - val_loss: 0.0173\
Epoch 9/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0135 - val_loss: 0.0175\
Epoch 10/40\
88/88 [==============================] - 1s 14ms/step - loss: 0.0128 - val_loss: 0.0170\
Epoch 11/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0111 - val_loss: 0.0171\
Epoch 12/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0097 - val_loss: 0.0172\
Epoch 13/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0077 - val_loss: 0.0169\
Epoch 14/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0072 - val_loss: 0.0169\
Epoch 15/40\
88/88 [==============================] - 1s 14ms/step - loss: 0.0069 - val_loss: 0.0170\
Epoch 16/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0069 - val_loss: 0.0170\
Fold 2: 0.016984738 loss, 0.8002281 auc\
Epoch 1/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.3517 - val_loss: 0.0563\
Epoch 2/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0344 - val_loss: 0.0254\
Epoch 3/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0226 - val_loss: 0.0209\
Epoch 4/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0195 - val_loss: 0.0194\
Epoch 5/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0179 - val_loss: 0.0185\
Epoch 6/40\
88/88 [==============================] - 1s 14ms/step - loss: 0.0167 - val_loss: 0.0179\
Epoch 7/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0156 - val_loss: 0.0176\
Epoch 8/40\
88/88 [==============================] - 1s 14ms/step - loss: 0.0146 - val_loss: 0.0173\
Epoch 9/40\
88/88 [==============================] - 1s 14ms/step - loss: 0.0133 - val_loss: 0.0177\
Epoch 10/40\
88/88 [==============================] - 1s 14ms/step - loss: 0.0123 - val_loss: 0.0171\
Epoch 11/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0145 - val_loss: 0.0179\
Epoch 12/40\
88/88 [==============================] - 1s 14ms/step - loss: 0.0135 - val_loss: 0.0185\
Epoch 13/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0121 - val_loss: 0.0173\
Fold 3: 0.017303497 loss, 0.8056191 auc\
Epoch 1/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.3532 - val_loss: 0.0524\
Epoch 2/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0345 - val_loss: 0.0255\
Epoch 3/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0227 - val_loss: 0.0210\
Epoch 4/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0196 - val_loss: 0.0193\
Epoch 5/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0180 - val_loss: 0.0185\
Epoch 6/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0168 - val_loss: 0.0179\
Epoch 7/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0157 - val_loss: 0.0176\
Epoch 8/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0146 - val_loss: 0.0172\
Epoch 9/40\
88/88 [==============================] - 2s 17ms/step - loss: 0.0134 - val_loss: 0.0173\
Epoch 10/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0130 - val_loss: 0.0171\
Epoch 11/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0113 - val_loss: 0.0170\
Epoch 12/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0097 - val_loss: 0.0170\
Epoch 13/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0083 - val_loss: 0.0172\
Epoch 14/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0063 - val_loss: 0.0171\
Fold 4: 0.01713315 loss, 0.79228437 auc\
Epoch 1/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.3534 - val_loss: 0.0554\
Epoch 2/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0345 - val_loss: 0.0255\
Epoch 3/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0227 - val_loss: 0.0209\
Epoch 4/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0196 - val_loss: 0.0193\
Epoch 5/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0180 - val_loss: 0.0185\
Epoch 6/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0167 - val_loss: 0.0180\
Epoch 7/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0156 - val_loss: 0.0176\
Epoch 8/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0144 - val_loss: 0.0175\
Epoch 9/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0136 - val_loss: 0.0175\
Epoch 10/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0124 - val_loss: 0.0170\
Epoch 11/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0120 - val_loss: 0.0181\
Epoch 12/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0116 - val_loss: 0.0194\
Epoch 13/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0103 - val_loss: 0.0171\
Fold 5: 0.017132673 loss, 0.804345 auc\
[0.017199123, 0.016984738, 0.017303497, 0.01713315, 0.017132673] [0.79408467, 0.8002281, 0.8056191, 0.79228437, 0.804345]\
\cf10 [I 2020-11-05 17:58:42,313]\cf2  Trial 23 finished with value: 0.01715063638985157 and parameters: \{'layers': 0, 'neurons': 862\}. Best is trial 22 with value: 0.016848765313625336.\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 5s 58ms/step - loss: 0.3156 - val_loss: 0.0408\
Epoch 2/40\
88/88 [==============================] - 4s 44ms/step - loss: 0.0282 - val_loss: 0.0227\
Epoch 3/40\
88/88 [==============================] - 4s 44ms/step - loss: 0.0205 - val_loss: 0.0199\
Epoch 4/40\
88/88 [==============================] - 4s 44ms/step - loss: 0.0180 - val_loss: 0.0187\
Epoch 5/40\
88/88 [==============================] - 4s 44ms/step - loss: 0.0164 - val_loss: 0.0176\
Epoch 6/40\
88/88 [==============================] - 4s 44ms/step - loss: 0.0147 - val_loss: 0.0176\
Epoch 7/40\
88/88 [==============================] - 4s 44ms/step - loss: 0.0132 - val_loss: 0.0175\
Epoch 8/40\
88/88 [==============================] - 4s 45ms/step - loss: 0.0110 - val_loss: 0.0174\
Epoch 9/40\
88/88 [==============================] - 4s 45ms/step - loss: 0.0083 - val_loss: 0.0179\
Epoch 10/40\
88/88 [==============================] - 4s 43ms/step - loss: 0.0060 - val_loss: 0.0181\
Epoch 11/40\
88/88 [==============================] - 4s 45ms/step - loss: 0.0039 - val_loss: 0.0180\
\cf10 [I 2020-11-05 17:59:38,111]\cf2  Trial 24 pruned. \
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.3488 - val_loss: 0.0525\
Epoch 2/40\
88/88 [==============================] - 1s 17ms/step - loss: 0.0336 - val_loss: 0.0252\
Epoch 3/40\
88/88 [==============================] - 1s 17ms/step - loss: 0.0224 - val_loss: 0.0208\
Epoch 4/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0194 - val_loss: 0.0193\
Epoch 5/40\
88/88 [==============================] - 2s 18ms/step - loss: 0.0178 - val_loss: 0.0185\
Epoch 6/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0166 - val_loss: 0.0181\
Epoch 7/40\
88/88 [==============================] - 1s 17ms/step - loss: 0.0155 - val_loss: 0.0176\
Epoch 8/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0147 - val_loss: 0.0176\
Epoch 9/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0136 - val_loss: 0.0174\
Epoch 10/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0122 - val_loss: 0.0171\
Epoch 11/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0108 - val_loss: 0.0171\
Epoch 12/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0094 - val_loss: 0.0175\
Epoch 13/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0074 - val_loss: 0.0171\
Epoch 14/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0069 - val_loss: 0.0171\
Epoch 15/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0066 - val_loss: 0.0172\
Epoch 16/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0066 - val_loss: 0.0172\
Fold 1: 0.017195355 loss, 0.79058295 auc\
Epoch 1/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.3486 - val_loss: 0.0527\
Epoch 2/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0334 - val_loss: 0.0252\
Epoch 3/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0222 - val_loss: 0.0208\
Epoch 4/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0193 - val_loss: 0.0192\
Epoch 5/40\
88/88 [==============================] - 1s 17ms/step - loss: 0.0178 - val_loss: 0.0185\
Epoch 6/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0166 - val_loss: 0.0180\
Epoch 7/40\
88/88 [==============================] - 1s 15ms/step - loss: 0.0169 - val_loss: 0.0193\
Epoch 8/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0169 - val_loss: 0.0217\
Epoch 9/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0159 - val_loss: 0.0181\
\cf10 [I 2020-11-05 18:00:26,708]\cf2  Trial 25 pruned. \
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 4s 44ms/step - loss: 0.3299 - val_loss: 0.0435\
Epoch 2/40\
88/88 [==============================] - 3s 35ms/step - loss: 0.0304 - val_loss: 0.0238\
Epoch 3/40\
88/88 [==============================] - 3s 34ms/step - loss: 0.0211 - val_loss: 0.0200\
Epoch 4/40\
88/88 [==============================] - 3s 35ms/step - loss: 0.0184 - val_loss: 0.0185\
Epoch 5/40\
88/88 [==============================] - 3s 34ms/step - loss: 0.0167 - val_loss: 0.0179\
Epoch 6/40\
88/88 [==============================] - 3s 35ms/step - loss: 0.0151 - val_loss: 0.0177\
Epoch 7/40\
88/88 [==============================] - 3s 38ms/step - loss: 0.0132 - val_loss: 0.0175\
Epoch 8/40\
88/88 [==============================] - 3s 36ms/step - loss: 0.0114 - val_loss: 0.0176\
Epoch 9/40\
88/88 [==============================] - 3s 35ms/step - loss: 0.0089 - val_loss: 0.0180\
Epoch 10/40\
88/88 [==============================] - 3s 36ms/step - loss: 0.0063 - val_loss: 0.0175\
\cf10 [I 2020-11-05 18:01:09,740]\cf2  Trial 26 pruned. \
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 7s 76ms/step - loss: 0.3105 - val_loss: 0.0359\
Epoch 2/40\
88/88 [==============================] - 6s 69ms/step - loss: 0.0281 - val_loss: 0.0230\
Epoch 3/40\
88/88 [==============================] - 6s 69ms/step - loss: 0.0206 - val_loss: 0.0197\
Epoch 4/40\
88/88 [==============================] - 6s 69ms/step - loss: 0.0181 - val_loss: 0.0186\
Epoch 5/40\
88/88 [==============================] - 6s 69ms/step - loss: 0.0163 - val_loss: 0.0179\
Epoch 6/40\
88/88 [==============================] - 6s 69ms/step - loss: 0.0145 - val_loss: 0.0176\
Epoch 7/40\
88/88 [==============================] - 6s 69ms/step - loss: 0.0121 - val_loss: 0.0180\
Epoch 8/40\
88/88 [==============================] - 6s 69ms/step - loss: 0.0098 - val_loss: 0.0186\
Epoch 9/40\
88/88 [==============================] - 6s 69ms/step - loss: 0.0064 - val_loss: 0.0180\
\cf10 [I 2020-11-05 18:02:18,473]\cf2  Trial 27 pruned. \
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 7s 77ms/step - loss: 0.3195 - val_loss: 0.0445\
Epoch 2/40\
88/88 [==============================] - 6s 71ms/step - loss: 0.0295 - val_loss: 0.0234\
Epoch 3/40\
88/88 [==============================] - 6s 72ms/step - loss: 0.0210 - val_loss: 0.0201\
Epoch 4/40\
88/88 [==============================] - 6s 69ms/step - loss: 0.0185 - val_loss: 0.0211\
Epoch 5/40\
88/88 [==============================] - 6s 69ms/step - loss: 0.0170 - val_loss: 0.0178\
Epoch 6/40\
88/88 [==============================] - 6s 69ms/step - loss: 0.0153 - val_loss: 0.0176\
Epoch 7/40\
88/88 [==============================] - 6s 70ms/step - loss: 0.0134 - val_loss: 0.0183\
Epoch 8/40\
88/88 [==============================] - 6s 73ms/step - loss: 0.0111 - val_loss: 0.0184\
Epoch 9/40\
88/88 [==============================] - 6s 70ms/step - loss: 0.0081 - val_loss: 0.0180\
\cf10 [I 2020-11-05 18:03:28,667]\cf2  Trial 28 pruned. \
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.3210 - val_loss: 0.0488\
Epoch 2/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0290 - val_loss: 0.0230\
Epoch 3/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0210 - val_loss: 0.0203\
Epoch 4/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0187 - val_loss: 0.0189\
Epoch 5/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0173 - val_loss: 0.0183\
Epoch 6/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0161 - val_loss: 0.0179\
Epoch 7/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0149 - val_loss: 0.0176\
Epoch 8/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0136 - val_loss: 0.0174\
Epoch 9/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0123 - val_loss: 0.0176\
Epoch 10/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0110 - val_loss: 0.0175\
Epoch 11/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0087 - val_loss: 0.0172\
Epoch 12/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0082 - val_loss: 0.0172\
Epoch 13/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0079 - val_loss: 0.0173\
Epoch 14/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0076 - val_loss: 0.0173\
Fold 1: 0.017263627 loss, 0.7971902 auc\
Epoch 1/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.3211 - val_loss: 0.0425\
Epoch 2/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0289 - val_loss: 0.0227\
Epoch 3/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0208 - val_loss: 0.0198\
Epoch 4/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0184 - val_loss: 0.0186\
Epoch 5/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0170 - val_loss: 0.0178\
Epoch 6/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0157 - val_loss: 0.0175\
Epoch 7/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0144 - val_loss: 0.0172\
Epoch 8/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0132 - val_loss: 0.0171\
Epoch 9/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0122 - val_loss: 0.0177\
Epoch 10/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0105 - val_loss: 0.0169\
Epoch 11/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0088 - val_loss: 0.0171\
Epoch 12/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0073 - val_loss: 0.0171\
Epoch 13/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0052 - val_loss: 0.0170\
Fold 2: 0.017034298 loss, 0.7859851 auc\
Epoch 1/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.3218 - val_loss: 0.0448\
Epoch 2/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0288 - val_loss: 0.0233\
Epoch 3/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0212 - val_loss: 0.0197\
Epoch 4/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0185 - val_loss: 0.0186\
Epoch 5/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0173 - val_loss: 0.0181\
Epoch 6/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0161 - val_loss: 0.0176\
Epoch 7/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0149 - val_loss: 0.0174\
Epoch 8/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0137 - val_loss: 0.0172\
Epoch 9/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0124 - val_loss: 0.0172\
Epoch 10/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0113 - val_loss: 0.0171\
Epoch 11/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0090 - val_loss: 0.0168\
Epoch 12/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0085 - val_loss: 0.0168\
Epoch 13/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0082 - val_loss: 0.0168\
Epoch 14/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0078 - val_loss: 0.0168\
Epoch 15/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0078 - val_loss: 0.0168\
Fold 3: 0.016811676 loss, 0.80887514 auc\
Epoch 1/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.3206 - val_loss: 0.0431\
Epoch 2/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0289 - val_loss: 0.0232\
Epoch 3/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0208 - val_loss: 0.0198\
Epoch 4/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0185 - val_loss: 0.0186\
Epoch 5/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0171 - val_loss: 0.0180\
Epoch 6/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0159 - val_loss: 0.0176\
Epoch 7/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0146 - val_loss: 0.0174\
Epoch 8/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0135 - val_loss: 0.0171\
Epoch 9/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0138 - val_loss: 0.0173\
Epoch 10/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0113 - val_loss: 0.0171\
Epoch 11/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0091 - val_loss: 0.0168\
Epoch 12/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0087 - val_loss: 0.0169\
Epoch 13/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0084 - val_loss: 0.0169\
Epoch 14/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0082 - val_loss: 0.0169\
Fold 4: 0.016907418 loss, 0.80629086 auc\
Epoch 1/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.3199 - val_loss: 0.0429\
Epoch 2/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0288 - val_loss: 0.0231\
Epoch 3/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0209 - val_loss: 0.0199\
Epoch 4/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0187 - val_loss: 0.0188\
Epoch 5/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0173 - val_loss: 0.0181\
Epoch 6/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0160 - val_loss: 0.0177\
Epoch 7/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0147 - val_loss: 0.0174\
Epoch 8/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0134 - val_loss: 0.0171\
Epoch 9/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0143 - val_loss: 0.0177\
Epoch 10/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0118 - val_loss: 0.0170\
Epoch 11/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0095 - val_loss: 0.0167\
Epoch 12/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0091 - val_loss: 0.0167\
Epoch 13/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0088 - val_loss: 0.0167\
Epoch 14/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0085 - val_loss: 0.0168\
Epoch 15/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0085 - val_loss: 0.0168\
Fold 5: 0.016774464 loss, 0.8087623 auc\
[0.017263627, 0.017034298, 0.016811676, 0.016907418, 0.016774464] [0.7971902, 0.7859851, 0.80887514, 0.80629086, 0.8087623]\
\cf10 [I 2020-11-05 18:06:14,601]\cf2  Trial 29 finished with value: 0.01695829667150974 and parameters: \{'layers': 0, 'neurons': 1362\}. Best is trial 22 with value: 0.016848765313625336.\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 5s 58ms/step - loss: 0.3094 - val_loss: 0.0406\
Epoch 2/40\
88/88 [==============================] - 5s 52ms/step - loss: 0.0272 - val_loss: 0.0221\
Epoch 3/40\
88/88 [==============================] - 5s 52ms/step - loss: 0.0202 - val_loss: 0.0199\
Epoch 4/40\
88/88 [==============================] - 5s 52ms/step - loss: 0.0178 - val_loss: 0.0185\
Epoch 5/40\
88/88 [==============================] - 5s 52ms/step - loss: 0.0164 - val_loss: 0.0182\
Epoch 6/40\
88/88 [==============================] - 5s 52ms/step - loss: 0.0146 - val_loss: 0.0180\
Epoch 7/40\
88/88 [==============================] - 4s 49ms/step - loss: 0.0131 - val_loss: 0.0175\
Epoch 8/40\
88/88 [==============================] - 4s 46ms/step - loss: 0.0108 - val_loss: 0.0178\
Epoch 9/40\
88/88 [==============================] - 4s 51ms/step - loss: 0.0085 - val_loss: 0.0179\
Epoch 10/40\
88/88 [==============================] - 5s 52ms/step - loss: 0.0056 - val_loss: 0.0176\
\cf10 [I 2020-11-05 18:07:11,963]\cf2  Trial 30 pruned. \
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.3241 - val_loss: 0.0414\
Epoch 2/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0294 - val_loss: 0.0229\
Epoch 3/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0210 - val_loss: 0.0199\
Epoch 4/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0187 - val_loss: 0.0187\
Epoch 5/40\
88/88 [==============================] - 2s 19ms/step - loss: 0.0173 - val_loss: 0.0180\
Epoch 6/40\
88/88 [==============================] - 2s 19ms/step - loss: 0.0161 - val_loss: 0.0175\
Epoch 7/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0150 - val_loss: 0.0173\
Epoch 8/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0137 - val_loss: 0.0171\
Epoch 9/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0124 - val_loss: 0.0171\
Epoch 10/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0111 - val_loss: 0.0170\
Epoch 11/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0096 - val_loss: 0.0171\
Epoch 12/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0082 - val_loss: 0.0173\
Epoch 13/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0060 - val_loss: 0.0170\
Fold 1: 0.017046597 loss, 0.7899369 auc\
Epoch 1/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.3243 - val_loss: 0.0412\
Epoch 2/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0294 - val_loss: 0.0230\
Epoch 3/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0211 - val_loss: 0.0198\
Epoch 4/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0187 - val_loss: 0.0186\
Epoch 5/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0173 - val_loss: 0.0180\
Epoch 6/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0162 - val_loss: 0.0175\
Epoch 7/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0150 - val_loss: 0.0175\
Epoch 8/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0137 - val_loss: 0.0172\
Epoch 9/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0124 - val_loss: 0.0170\
Epoch 10/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0126 - val_loss: 0.0268\
Epoch 11/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0168 - val_loss: 0.0175\
Epoch 12/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0126 - val_loss: 0.0170\
Epoch 13/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0122 - val_loss: 0.0169\
Epoch 14/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0119 - val_loss: 0.0168\
Epoch 15/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0115 - val_loss: 0.0168\
Epoch 16/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0112 - val_loss: 0.0168\
Epoch 17/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0112 - val_loss: 0.0168\
Epoch 18/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0112 - val_loss: 0.0168\
Epoch 19/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0112 - val_loss: 0.0168\
Epoch 20/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0111 - val_loss: 0.0168\
Epoch 21/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0111 - val_loss: 0.0168\
Epoch 22/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0111 - val_loss: 0.0168\
Epoch 23/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0110 - val_loss: 0.0168\
Epoch 24/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0110 - val_loss: 0.0168\
Epoch 25/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0110 - val_loss: 0.0168\
Epoch 26/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0109 - val_loss: 0.0168\
Epoch 27/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0109 - val_loss: 0.0168\
Epoch 28/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0109 - val_loss: 0.0168\
Epoch 29/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0109 - val_loss: 0.0168\
Epoch 30/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0108 - val_loss: 0.0168\
Epoch 31/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0108 - val_loss: 0.0168\
Epoch 32/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0108 - val_loss: 0.0168\
Epoch 33/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0107 - val_loss: 0.0168\
Epoch 34/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0107 - val_loss: 0.0168\
Epoch 35/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0106 - val_loss: 0.0168\
Epoch 36/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0106 - val_loss: 0.0168\
Epoch 37/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0106 - val_loss: 0.0168\
Epoch 38/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0105 - val_loss: 0.0168\
Epoch 39/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0105 - val_loss: 0.0168\
Epoch 40/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0105 - val_loss: 0.0168\
Fold 2: 0.016773362 loss, 0.804313 auc\
Epoch 1/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.3236 - val_loss: 0.0447\
Epoch 2/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0292 - val_loss: 0.0235\
Epoch 3/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0210 - val_loss: 0.0198\
Epoch 4/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0186 - val_loss: 0.0187\
Epoch 5/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0172 - val_loss: 0.0181\
Epoch 6/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0159 - val_loss: 0.0177\
Epoch 7/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0147 - val_loss: 0.0174\
Epoch 8/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0135 - val_loss: 0.0172\
Epoch 9/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0122 - val_loss: 0.0178\
Epoch 10/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0120 - val_loss: 0.0171\
Epoch 11/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0097 - val_loss: 0.0173\
Epoch 12/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0080 - val_loss: 0.0173\
Epoch 13/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0059 - val_loss: 0.0172\
Fold 3: 0.017216725 loss, 0.78600377 auc\
Epoch 1/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.3257 - val_loss: 0.0461\
Epoch 2/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0293 - val_loss: 0.0232\
Epoch 3/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0210 - val_loss: 0.0199\
Epoch 4/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0187 - val_loss: 0.0188\
Epoch 5/40\
88/88 [==============================] - 2s 19ms/step - loss: 0.0172 - val_loss: 0.0181\
Epoch 6/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0160 - val_loss: 0.0175\
Epoch 7/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0148 - val_loss: 0.0174\
Epoch 8/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0137 - val_loss: 0.0172\
Epoch 9/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0123 - val_loss: 0.0170\
Epoch 10/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0110 - val_loss: 0.0171\
Epoch 11/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0095 - val_loss: 0.0172\
Epoch 12/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0073 - val_loss: 0.0169\
Epoch 13/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0067 - val_loss: 0.0169\
Epoch 14/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0063 - val_loss: 0.0170\
Epoch 15/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0060 - val_loss: 0.0170\
Fold 4: 0.016970301 loss, 0.7933789 auc\
Epoch 1/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.3248 - val_loss: 0.0549\
Epoch 2/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0296 - val_loss: 0.0236\
Epoch 3/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0211 - val_loss: 0.0201\
Epoch 4/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0188 - val_loss: 0.0191\
Epoch 5/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0174 - val_loss: 0.0183\
Epoch 6/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0163 - val_loss: 0.0180\
Epoch 7/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0151 - val_loss: 0.0175\
Epoch 8/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0139 - val_loss: 0.0174\
Epoch 9/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0126 - val_loss: 0.0173\
Epoch 10/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0114 - val_loss: 0.0175\
Epoch 11/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0100 - val_loss: 0.0173\
Epoch 12/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0078 - val_loss: 0.0171\
Epoch 13/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0073 - val_loss: 0.0171\
Epoch 14/40\
88/88 [==============================] - 2s 19ms/step - loss: 0.0070 - val_loss: 0.0171\
Epoch 15/40\
88/88 [==============================] - 2s 19ms/step - loss: 0.0067 - val_loss: 0.0171\
Fold 5: 0.01714138 loss, 0.7972736 auc\
[0.017046597, 0.016773362, 0.017216725, 0.016970301, 0.01714138] [0.7899369, 0.804313, 0.78600377, 0.7933789, 0.7972736]\
\cf10 [I 2020-11-05 18:10:32,639]\cf2  Trial 31 finished with value: 0.017029672861099243 and parameters: \{'layers': 0, 'neurons': 1312\}. Best is trial 22 with value: 0.016848765313625336.\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.3227 - val_loss: 0.0436\
Epoch 2/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0290 - val_loss: 0.0228\
Epoch 3/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0209 - val_loss: 0.0198\
Epoch 4/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0187 - val_loss: 0.0186\
Epoch 5/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0173 - val_loss: 0.0178\
Epoch 6/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0161 - val_loss: 0.0175\
Epoch 7/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0160 - val_loss: 0.0171\
Epoch 8/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0141 - val_loss: 0.0169\
Epoch 9/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0128 - val_loss: 0.0168\
Epoch 10/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0114 - val_loss: 0.0169\
Epoch 11/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0094 - val_loss: 0.0166\
Epoch 12/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0090 - val_loss: 0.0166\
Epoch 13/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0087 - val_loss: 0.0166\
Epoch 14/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0084 - val_loss: 0.0166\
Fold 1: 0.016606152 loss, 0.8139024 auc\
Epoch 1/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.3209 - val_loss: 0.0423\
Epoch 2/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0299 - val_loss: 0.0258\
Epoch 3/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0224 - val_loss: 0.0401\
Epoch 4/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0212 - val_loss: 0.0188\
Epoch 5/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0177 - val_loss: 0.0182\
Epoch 6/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0168 - val_loss: 0.0178\
Epoch 7/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0159 - val_loss: 0.0174\
Epoch 8/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0148 - val_loss: 0.0172\
Epoch 9/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0138 - val_loss: 0.0171\
Epoch 10/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0125 - val_loss: 0.0171\
Epoch 11/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0113 - val_loss: 0.0170\
Epoch 12/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0093 - val_loss: 0.0168\
Epoch 13/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0089 - val_loss: 0.0168\
Epoch 14/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0087 - val_loss: 0.0168\
Epoch 15/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0084 - val_loss: 0.0169\
Fold 2: 0.016856134 loss, 0.80803466 auc\
Epoch 1/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.3208 - val_loss: 0.0434\
Epoch 2/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0289 - val_loss: 0.0229\
Epoch 3/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0209 - val_loss: 0.0199\
Epoch 4/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0185 - val_loss: 0.0187\
Epoch 5/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0171 - val_loss: 0.0181\
Epoch 6/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0158 - val_loss: 0.0176\
Epoch 7/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0145 - val_loss: 0.0175\
Epoch 8/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0133 - val_loss: 0.0172\
Epoch 9/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0121 - val_loss: 0.0172\
Epoch 10/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0116 - val_loss: 0.0174\
Epoch 11/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0091 - val_loss: 0.0170\
Epoch 12/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0085 - val_loss: 0.0169\
Epoch 13/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0082 - val_loss: 0.0169\
Epoch 14/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0078 - val_loss: 0.0169\
Epoch 15/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0078 - val_loss: 0.0170\
Fold 3: 0.016962664 loss, 0.7991385 auc\
Epoch 1/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.3236 - val_loss: 0.0428\
Epoch 2/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0288 - val_loss: 0.0227\
Epoch 3/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0208 - val_loss: 0.0199\
Epoch 4/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0185 - val_loss: 0.0190\
Epoch 5/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0170 - val_loss: 0.0181\
Epoch 6/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0158 - val_loss: 0.0177\
Epoch 7/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0148 - val_loss: 0.0174\
Epoch 8/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0134 - val_loss: 0.0172\
Epoch 9/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0121 - val_loss: 0.0198\
Epoch 10/40\
88/88 [==============================] - 3s 28ms/step - loss: 0.0154 - val_loss: 0.0172\
Epoch 11/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0116 - val_loss: 0.0169\
Epoch 12/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0112 - val_loss: 0.0168\
Epoch 13/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0108 - val_loss: 0.0168\
Epoch 14/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0105 - val_loss: 0.0168\
Epoch 15/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0105 - val_loss: 0.0168\
Epoch 16/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0105 - val_loss: 0.0168\
Fold 4: 0.016773779 loss, 0.81480974 auc\
Epoch 1/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.3222 - val_loss: 0.0430\
Epoch 2/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0288 - val_loss: 0.0228\
Epoch 3/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0208 - val_loss: 0.0199\
Epoch 4/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0185 - val_loss: 0.0188\
Epoch 5/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0172 - val_loss: 0.0182\
Epoch 6/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0159 - val_loss: 0.0177\
Epoch 7/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0147 - val_loss: 0.0174\
Epoch 8/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0134 - val_loss: 0.0173\
Epoch 9/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0122 - val_loss: 0.0173\
Epoch 10/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0109 - val_loss: 0.0173\
Epoch 11/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0087 - val_loss: 0.0169\
Epoch 12/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0081 - val_loss: 0.0169\
Epoch 13/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0077 - val_loss: 0.0169\
Epoch 14/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0074 - val_loss: 0.0169\
Epoch 15/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0073 - val_loss: 0.0170\
Fold 5: 0.016951978 loss, 0.80341446 auc\
[0.016606152, 0.016856134, 0.016962664, 0.016773779, 0.016951978] [0.8139024, 0.80803466, 0.7991385, 0.81480974, 0.80341446]\
\cf10 [I 2020-11-05 18:13:29,421]\cf2  Trial 32 finished with value: 0.016830141469836236 and parameters: \{'layers': 0, 'neurons': 1362\}. Best is trial 32 with value: 0.016830141469836236.\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 3s 34ms/step - loss: 0.3321 - val_loss: 0.0450\
Epoch 2/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0307 - val_loss: 0.0236\
Epoch 3/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0214 - val_loss: 0.0204\
Epoch 4/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0189 - val_loss: 0.0190\
Epoch 5/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0174 - val_loss: 0.0182\
Epoch 6/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0161 - val_loss: 0.0180\
Epoch 7/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0150 - val_loss: 0.0176\
Epoch 8/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0137 - val_loss: 0.0175\
Epoch 9/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0125 - val_loss: 0.0172\
Epoch 10/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0114 - val_loss: 0.0172\
Epoch 11/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0099 - val_loss: 0.0173\
Epoch 12/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0077 - val_loss: 0.0170\
Epoch 13/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0071 - val_loss: 0.0171\
Epoch 14/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0068 - val_loss: 0.0171\
Epoch 15/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0065 - val_loss: 0.0171\
Fold 1: 0.01712411 loss, 0.78841543 auc\
Epoch 1/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.3334 - val_loss: 0.0455\
Epoch 2/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0308 - val_loss: 0.0232\
Epoch 3/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0215 - val_loss: 0.0201\
Epoch 4/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0189 - val_loss: 0.0187\
Epoch 5/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0174 - val_loss: 0.0180\
Epoch 6/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0162 - val_loss: 0.0176\
Epoch 7/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0149 - val_loss: 0.0175\
Epoch 8/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0137 - val_loss: 0.0170\
Epoch 9/40\
88/88 [==============================] - 2s 19ms/step - loss: 0.0126 - val_loss: 0.0172\
Epoch 10/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0112 - val_loss: 0.0170\
Epoch 11/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0090 - val_loss: 0.0168\
Epoch 12/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0085 - val_loss: 0.0168\
Epoch 13/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0081 - val_loss: 0.0168\
Epoch 14/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0078 - val_loss: 0.0168\
Fold 2: 0.016847117 loss, 0.8026221 auc\
Epoch 1/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.3326 - val_loss: 0.0463\
Epoch 2/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0308 - val_loss: 0.0236\
Epoch 3/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0216 - val_loss: 0.0204\
Epoch 4/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0190 - val_loss: 0.0190\
Epoch 5/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0175 - val_loss: 0.0183\
Epoch 6/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0163 - val_loss: 0.0179\
Epoch 7/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0152 - val_loss: 0.0176\
Epoch 8/40\
88/88 [==============================] - 2s 19ms/step - loss: 0.0142 - val_loss: 0.0174\
Epoch 9/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0128 - val_loss: 0.0170\
Epoch 10/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0115 - val_loss: 0.0172\
Epoch 11/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0101 - val_loss: 0.0172\
Epoch 12/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0079 - val_loss: 0.0170\
Epoch 13/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0074 - val_loss: 0.0170\
Epoch 14/40\
88/88 [==============================] - 2s 19ms/step - loss: 0.0071 - val_loss: 0.0170\
Epoch 15/40\
88/88 [==============================] - 2s 19ms/step - loss: 0.0071 - val_loss: 0.0171\
Fold 3: 0.017066594 loss, 0.7988087 auc\
Epoch 1/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.3312 - val_loss: 0.0456\
Epoch 2/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0304 - val_loss: 0.0237\
Epoch 3/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0215 - val_loss: 0.0205\
Epoch 4/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0190 - val_loss: 0.0190\
Epoch 5/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0175 - val_loss: 0.0184\
Epoch 6/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0163 - val_loss: 0.0179\
Epoch 7/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0150 - val_loss: 0.0176\
Epoch 8/40\
88/88 [==============================] - 2s 19ms/step - loss: 0.0138 - val_loss: 0.0173\
Epoch 9/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0127 - val_loss: 0.0172\
Epoch 10/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0112 - val_loss: 0.0173\
Epoch 11/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0099 - val_loss: 0.0178\
Epoch 12/40\
88/88 [==============================] - 2s 19ms/step - loss: 0.0078 - val_loss: 0.0171\
Epoch 13/40\
88/88 [==============================] - 2s 19ms/step - loss: 0.0072 - val_loss: 0.0171\
Epoch 14/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0069 - val_loss: 0.0171\
Epoch 15/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0066 - val_loss: 0.0171\
Fold 4: 0.017134713 loss, 0.7976652 auc\
Epoch 1/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.3325 - val_loss: 0.0443\
Epoch 2/40\
88/88 [==============================] - 2s 19ms/step - loss: 0.0305 - val_loss: 0.0232\
Epoch 3/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0215 - val_loss: 0.0200\
Epoch 4/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0190 - val_loss: 0.0188\
Epoch 5/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0176 - val_loss: 0.0181\
Epoch 6/40\
88/88 [==============================] - 2s 19ms/step - loss: 0.0164 - val_loss: 0.0177\
Epoch 7/40\
88/88 [==============================] - 2s 19ms/step - loss: 0.0152 - val_loss: 0.0173\
Epoch 8/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0140 - val_loss: 0.0171\
Epoch 9/40\
88/88 [==============================] - 2s 19ms/step - loss: 0.0128 - val_loss: 0.0169\
Epoch 10/40\
88/88 [==============================] - 2s 19ms/step - loss: 0.0120 - val_loss: 0.0169\
Epoch 11/40\
88/88 [==============================] - 2s 19ms/step - loss: 0.0100 - val_loss: 0.0170\
Epoch 12/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0080 - val_loss: 0.0170\
Epoch 13/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0075 - val_loss: 0.0167\
Epoch 14/40\
88/88 [==============================] - 2s 19ms/step - loss: 0.0071 - val_loss: 0.0168\
Epoch 15/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0068 - val_loss: 0.0168\
Epoch 16/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0065 - val_loss: 0.0168\
Fold 5: 0.016824711 loss, 0.79976743 auc\
[0.01712411, 0.016847117, 0.017066594, 0.017134713, 0.016824711] [0.78841543, 0.8026221, 0.7988087, 0.7976652, 0.79976743]\
\cf10 [I 2020-11-05 18:16:13,077]\cf2  Trial 33 finished with value: 0.016999449208378793 and parameters: \{'layers': 0, 'neurons': 1162\}. Best is trial 32 with value: 0.016830141469836236.\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 3s 34ms/step - loss: 0.3541 - val_loss: 0.0554\
Epoch 2/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0351 - val_loss: 0.0263\
Epoch 3/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0227 - val_loss: 0.0210\
Epoch 4/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0194 - val_loss: 0.0194\
Epoch 5/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0176 - val_loss: 0.0185\
Epoch 6/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0161 - val_loss: 0.0182\
Epoch 7/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0146 - val_loss: 0.0174\
Epoch 8/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0129 - val_loss: 0.0176\
Epoch 9/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0111 - val_loss: 0.0178\
Epoch 10/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0084 - val_loss: 0.0174\
\cf10 [I 2020-11-05 18:16:44,631]\cf2  Trial 34 pruned. \
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.3094 - val_loss: 0.0400\
Epoch 2/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0271 - val_loss: 0.0222\
Epoch 3/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0208 - val_loss: 0.0194\
Epoch 4/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0182 - val_loss: 0.0184\
Epoch 5/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0170 - val_loss: 0.0180\
Epoch 6/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0158 - val_loss: 0.0176\
Epoch 7/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0145 - val_loss: 0.0173\
Epoch 8/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0132 - val_loss: 0.0171\
Epoch 9/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0119 - val_loss: 0.0174\
Epoch 10/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0107 - val_loss: 0.0172\
Epoch 11/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0082 - val_loss: 0.0169\
Epoch 12/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0077 - val_loss: 0.0169\
Epoch 13/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0074 - val_loss: 0.0169\
Epoch 14/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0071 - val_loss: 0.0170\
Fold 1: 0.01697503 loss, 0.7957313 auc\
Epoch 1/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.3090 - val_loss: 0.0376\
Epoch 2/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0270 - val_loss: 0.0218\
Epoch 3/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0203 - val_loss: 0.0193\
Epoch 4/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0183 - val_loss: 0.0184\
Epoch 5/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0170 - val_loss: 0.0178\
Epoch 6/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0158 - val_loss: 0.0174\
Epoch 7/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0146 - val_loss: 0.0171\
Epoch 8/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0133 - val_loss: 0.0170\
Epoch 9/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0119 - val_loss: 0.0170\
Epoch 10/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0158 - val_loss: 0.0175\
Epoch 11/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0128 - val_loss: 0.0177\
Fold 2: 0.017718995 loss, 0.8163496 auc\
Epoch 1/40\
88/88 [==============================] - 3s 34ms/step - loss: 0.3096 - val_loss: 0.0389\
Epoch 2/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0273 - val_loss: 0.0221\
Epoch 3/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0204 - val_loss: 0.0197\
Epoch 4/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0183 - val_loss: 0.0185\
Epoch 5/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0170 - val_loss: 0.0178\
Epoch 6/40\
88/88 [==============================] - 3s 33ms/step - loss: 0.0158 - val_loss: 0.0175\
Epoch 7/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0147 - val_loss: 0.0172\
Epoch 8/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.0133 - val_loss: 0.0171\
Epoch 9/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0119 - val_loss: 0.0170\
Epoch 10/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.0103 - val_loss: 0.0171\
Epoch 11/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.0081 - val_loss: 0.0168\
Epoch 12/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0076 - val_loss: 0.0167\
Epoch 13/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0072 - val_loss: 0.0168\
Epoch 14/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0068 - val_loss: 0.0168\
Epoch 15/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0065 - val_loss: 0.0168\
Fold 3: 0.01684024 loss, 0.79545087 auc\
Epoch 1/40\
88/88 [==============================] - 3s 33ms/step - loss: 0.3092 - val_loss: 0.0378\
Epoch 2/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0268 - val_loss: 0.0224\
Epoch 3/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0202 - val_loss: 0.0194\
Epoch 4/40\
88/88 [==============================] - 3s 33ms/step - loss: 0.0182 - val_loss: 0.0186\
Epoch 5/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0169 - val_loss: 0.0180\
Epoch 6/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0157 - val_loss: 0.0176\
Epoch 7/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0144 - val_loss: 0.0174\
Epoch 8/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0131 - val_loss: 0.0174\
Epoch 9/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0119 - val_loss: 0.0172\
Epoch 10/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0107 - val_loss: 0.0180\
Epoch 11/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0091 - val_loss: 0.0178\
Epoch 12/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0066 - val_loss: 0.0172\
Fold 4: 0.017204616 loss, 0.7878349 auc\
Epoch 1/40\
88/88 [==============================] - 3s 34ms/step - loss: 0.3092 - val_loss: 0.0389\
Epoch 2/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0281 - val_loss: 0.0261\
Epoch 3/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0215 - val_loss: 0.0387\
Epoch 4/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0204 - val_loss: 0.0186\
Epoch 5/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0173 - val_loss: 0.0183\
Epoch 6/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0163 - val_loss: 0.0178\
Epoch 7/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0153 - val_loss: 0.0174\
Epoch 8/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0141 - val_loss: 0.0172\
Epoch 9/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0129 - val_loss: 0.0171\
Epoch 10/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0116 - val_loss: 0.0172\
Epoch 11/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0096 - val_loss: 0.0169\
Epoch 12/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0091 - val_loss: 0.0169\
Epoch 13/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0089 - val_loss: 0.0169\
Epoch 14/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0086 - val_loss: 0.0169\
Fold 5: 0.01690586 loss, 0.80978096 auc\
[0.01697503, 0.017718995, 0.01684024, 0.017204616, 0.01690586] [0.7957313, 0.8163496, 0.79545087, 0.7878349, 0.80978096]\
\cf10 [I 2020-11-05 18:19:54,421]\cf2  Trial 35 finished with value: 0.017128948122262955 and parameters: \{'layers': 0, 'neurons': 1662\}. Best is trial 32 with value: 0.016830141469836236.\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 4s 51ms/step - loss: 0.3162 - val_loss: 0.0439\
Epoch 2/40\
88/88 [==============================] - 4s 45ms/step - loss: 0.0282 - val_loss: 0.0224\
Epoch 3/40\
88/88 [==============================] - 4s 44ms/step - loss: 0.0204 - val_loss: 0.0196\
Epoch 4/40\
88/88 [==============================] - 4s 45ms/step - loss: 0.0181 - val_loss: 0.0188\
Epoch 5/40\
88/88 [==============================] - 4s 44ms/step - loss: 0.0164 - val_loss: 0.0178\
Epoch 6/40\
88/88 [==============================] - 4s 45ms/step - loss: 0.0147 - val_loss: 0.0174\
Epoch 7/40\
88/88 [==============================] - 4s 45ms/step - loss: 0.0129 - val_loss: 0.0176\
Epoch 8/40\
88/88 [==============================] - 4s 46ms/step - loss: 0.0109 - val_loss: 0.0178\
Epoch 9/40\
88/88 [==============================] - 4s 46ms/step - loss: 0.0078 - val_loss: 0.0173\
Epoch 10/40\
88/88 [==============================] - 4s 45ms/step - loss: 0.0068 - val_loss: 0.0174\
Epoch 11/40\
88/88 [==============================] - 4s 44ms/step - loss: 0.0062 - val_loss: 0.0174\
Epoch 12/40\
88/88 [==============================] - 4s 44ms/step - loss: 0.0062 - val_loss: 0.0174\
\cf10 [I 2020-11-05 18:20:54,017]\cf2  Trial 36 pruned. \
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 3s 33ms/step - loss: 0.3824 - val_loss: 0.0806\
Epoch 2/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0417 - val_loss: 0.0285\
Epoch 3/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0248 - val_loss: 0.0223\
Epoch 4/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0205 - val_loss: 0.0201\
Epoch 5/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0185 - val_loss: 0.0189\
Epoch 6/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0168 - val_loss: 0.0181\
Epoch 7/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0155 - val_loss: 0.0180\
Epoch 8/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0139 - val_loss: 0.0182\
Epoch 9/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0124 - val_loss: 0.0186\
Epoch 10/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0100 - val_loss: 0.0181\
\cf10 [I 2020-11-05 18:21:28,381]\cf2  Trial 37 pruned. \
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 5s 54ms/step - loss: 0.3378 - val_loss: 0.0519\
Epoch 2/40\
88/88 [==============================] - 4s 46ms/step - loss: 0.0328 - val_loss: 0.0246\
Epoch 3/40\
88/88 [==============================] - 4s 46ms/step - loss: 0.0221 - val_loss: 0.0209\
Epoch 4/40\
88/88 [==============================] - 4s 46ms/step - loss: 0.0190 - val_loss: 0.0190\
Epoch 5/40\
88/88 [==============================] - 4s 46ms/step - loss: 0.0173 - val_loss: 0.0183\
Epoch 6/40\
88/88 [==============================] - 4s 47ms/step - loss: 0.0158 - val_loss: 0.0176\
Epoch 7/40\
88/88 [==============================] - 4s 47ms/step - loss: 0.0139 - val_loss: 0.0180\
Epoch 8/40\
88/88 [==============================] - 4s 46ms/step - loss: 0.0119 - val_loss: 0.0185\
Epoch 9/40\
88/88 [==============================] - 4s 46ms/step - loss: 0.0091 - val_loss: 0.0180\
\cf10 [I 2020-11-05 18:22:17,856]\cf2  Trial 38 pruned. \
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 3s 34ms/step - loss: 0.3045 - val_loss: 0.0408\
Epoch 2/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0265 - val_loss: 0.0217\
Epoch 3/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0202 - val_loss: 0.0193\
Epoch 4/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0184 - val_loss: 0.0183\
Epoch 5/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0170 - val_loss: 0.0178\
Epoch 6/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0159 - val_loss: 0.0173\
Epoch 7/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0146 - val_loss: 0.0170\
Epoch 8/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0132 - val_loss: 0.0169\
Epoch 9/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0118 - val_loss: 0.0168\
Epoch 10/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0119 - val_loss: 0.0173\
Epoch 11/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0097 - val_loss: 0.0168\
Epoch 12/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0071 - val_loss: 0.0166\
Epoch 13/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0066 - val_loss: 0.0166\
Epoch 14/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0063 - val_loss: 0.0166\
Epoch 15/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0060 - val_loss: 0.0167\
Fold 1: 0.016653525 loss, 0.799188 auc\
Epoch 1/40\
88/88 [==============================] - 3s 34ms/step - loss: 0.3046 - val_loss: 0.0410\
Epoch 2/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0267 - val_loss: 0.0221\
Epoch 3/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0202 - val_loss: 0.0194\
Epoch 4/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0182 - val_loss: 0.0185\
Epoch 5/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0170 - val_loss: 0.0179\
Epoch 6/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0158 - val_loss: 0.0175\
Epoch 7/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0146 - val_loss: 0.0173\
Epoch 8/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.0132 - val_loss: 0.0171\
Epoch 9/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0117 - val_loss: 0.0172\
Epoch 10/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0103 - val_loss: 0.0174\
Epoch 11/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0081 - val_loss: 0.0170\
Epoch 12/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0074 - val_loss: 0.0169\
Epoch 13/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0070 - val_loss: 0.0170\
Epoch 14/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0067 - val_loss: 0.0170\
Epoch 15/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0067 - val_loss: 0.0170\
Fold 2: 0.017011913 loss, 0.793262 auc\
Epoch 1/40\
88/88 [==============================] - 3s 35ms/step - loss: 0.3071 - val_loss: 0.0364\
Epoch 2/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0264 - val_loss: 0.0218\
Epoch 3/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0199 - val_loss: 0.0193\
Epoch 4/40\
88/88 [==============================] - 3s 28ms/step - loss: 0.0179 - val_loss: 0.0185\
Epoch 5/40\
88/88 [==============================] - 3s 28ms/step - loss: 0.0166 - val_loss: 0.0180\
Epoch 6/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0153 - val_loss: 0.0176\
Epoch 7/40\
88/88 [==============================] - 3s 28ms/step - loss: 0.0139 - val_loss: 0.0174\
Epoch 8/40\
88/88 [==============================] - 3s 28ms/step - loss: 0.0127 - val_loss: 0.0172\
Epoch 9/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0144 - val_loss: 0.0188\
Epoch 10/40\
88/88 [==============================] - 3s 28ms/step - loss: 0.0117 - val_loss: 0.0171\
Epoch 11/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0093 - val_loss: 0.0168\
Epoch 12/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0089 - val_loss: 0.0168\
Epoch 13/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0086 - val_loss: 0.0169\
Epoch 14/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0083 - val_loss: 0.0169\
Fold 3: 0.01688974 loss, 0.80387646 auc\
Epoch 1/40\
88/88 [==============================] - 3s 35ms/step - loss: 0.3067 - val_loss: 0.0413\
Epoch 2/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0267 - val_loss: 0.0225\
Epoch 3/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0201 - val_loss: 0.0194\
Epoch 4/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0181 - val_loss: 0.0185\
Epoch 5/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.0168 - val_loss: 0.0179\
Epoch 6/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0155 - val_loss: 0.0175\
Epoch 7/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0143 - val_loss: 0.0173\
Epoch 8/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0134 - val_loss: 0.0172\
Epoch 9/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0117 - val_loss: 0.0174\
Epoch 10/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0095 - val_loss: 0.0168\
Epoch 11/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.0089 - val_loss: 0.0168\
Epoch 12/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0086 - val_loss: 0.0168\
Epoch 13/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0083 - val_loss: 0.0168\
Epoch 14/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0082 - val_loss: 0.0168\
Fold 4: 0.016810054 loss, 0.8062942 auc\
Epoch 1/40\
88/88 [==============================] - 3s 34ms/step - loss: 0.3041 - val_loss: 0.0382\
Epoch 2/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0266 - val_loss: 0.0222\
Epoch 3/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0201 - val_loss: 0.0194\
Epoch 4/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0180 - val_loss: 0.0185\
Epoch 5/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0168 - val_loss: 0.0179\
Epoch 6/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0155 - val_loss: 0.0174\
Epoch 7/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0142 - val_loss: 0.0172\
Epoch 8/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0130 - val_loss: 0.0172\
Epoch 9/40\
88/88 [==============================] - 3s 28ms/step - loss: 0.0123 - val_loss: 0.0170\
Epoch 10/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0102 - val_loss: 0.0170\
Epoch 11/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0084 - val_loss: 0.0173\
Epoch 12/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0061 - val_loss: 0.0170\
Epoch 13/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0056 - val_loss: 0.0170\
Fold 5: 0.017041765 loss, 0.79066914 auc\
[0.016653525, 0.017011913, 0.01688974, 0.016810054, 0.017041765] [0.799188, 0.793262, 0.80387646, 0.8062942, 0.79066914]\
\cf10 [I 2020-11-05 18:25:47,398]\cf2  Trial 39 finished with value: 0.01688139922916889 and parameters: \{'layers': 0, 'neurons': 1762\}. Best is trial 32 with value: 0.016830141469836236.\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 7s 75ms/step - loss: 0.3027 - val_loss: 0.0366\
Epoch 2/40\
88/88 [==============================] - 6s 69ms/step - loss: 0.0261 - val_loss: 0.0215\
Epoch 3/40\
88/88 [==============================] - 6s 69ms/step - loss: 0.0197 - val_loss: 0.0192\
Epoch 4/40\
88/88 [==============================] - 6s 69ms/step - loss: 0.0175 - val_loss: 0.0179\
Epoch 5/40\
88/88 [==============================] - 6s 69ms/step - loss: 0.0159 - val_loss: 0.0174\
Epoch 6/40\
88/88 [==============================] - 6s 69ms/step - loss: 0.0141 - val_loss: 0.0174\
Epoch 7/40\
88/88 [==============================] - 6s 69ms/step - loss: 0.0124 - val_loss: 0.0176\
Epoch 8/40\
88/88 [==============================] - 6s 69ms/step - loss: 0.0095 - val_loss: 0.0168\
Epoch 9/40\
88/88 [==============================] - 6s 69ms/step - loss: 0.0083 - val_loss: 0.0169\
Epoch 10/40\
88/88 [==============================] - 6s 68ms/step - loss: 0.0075 - val_loss: 0.0170\
Epoch 11/40\
88/88 [==============================] - 6s 69ms/step - loss: 0.0069 - val_loss: 0.0170\
Fold 1: 0.01696081 loss, 0.78690624 auc\
Epoch 1/40\
88/88 [==============================] - 7s 74ms/step - loss: 0.3027 - val_loss: 0.0353\
Epoch 2/40\
88/88 [==============================] - 6s 68ms/step - loss: 0.0260 - val_loss: 0.0217\
Epoch 3/40\
88/88 [==============================] - 6s 68ms/step - loss: 0.0198 - val_loss: 0.0191\
Epoch 4/40\
88/88 [==============================] - 6s 67ms/step - loss: 0.0175 - val_loss: 0.0181\
Epoch 5/40\
88/88 [==============================] - 6s 69ms/step - loss: 0.0159 - val_loss: 0.0176\
Epoch 6/40\
88/88 [==============================] - 6s 67ms/step - loss: 0.0143 - val_loss: 0.0175\
Epoch 7/40\
88/88 [==============================] - 6s 71ms/step - loss: 0.0122 - val_loss: 0.0173\
Epoch 8/40\
88/88 [==============================] - 6s 69ms/step - loss: 0.0107 - val_loss: 0.0175\
Epoch 9/40\
88/88 [==============================] - 6s 70ms/step - loss: 0.0074 - val_loss: 0.0180\
Epoch 10/40\
88/88 [==============================] - 6s 70ms/step - loss: 0.0048 - val_loss: 0.0177\
Fold 2: 0.017685069 loss, 0.7693365 auc\
Epoch 1/40\
88/88 [==============================] - 7s 77ms/step - loss: 0.3015 - val_loss: 0.0372\
Epoch 2/40\
88/88 [==============================] - 6s 71ms/step - loss: 0.0259 - val_loss: 0.0217\
Epoch 3/40\
88/88 [==============================] - 6s 73ms/step - loss: 0.0196 - val_loss: 0.0192\
Epoch 4/40\
88/88 [==============================] - 6s 70ms/step - loss: 0.0174 - val_loss: 0.0186\
Epoch 5/40\
88/88 [==============================] - 6s 70ms/step - loss: 0.0159 - val_loss: 0.0178\
Epoch 6/40\
88/88 [==============================] - 6s 71ms/step - loss: 0.0141 - val_loss: 0.0189\
Epoch 7/40\
88/88 [==============================] - 6s 70ms/step - loss: 0.0123 - val_loss: 0.0177\
Epoch 8/40\
88/88 [==============================] - 6s 72ms/step - loss: 0.0096 - val_loss: 0.0179\
Epoch 9/40\
88/88 [==============================] - 6s 73ms/step - loss: 0.0074 - val_loss: 0.0183\
Epoch 10/40\
88/88 [==============================] - 6s 70ms/step - loss: 0.0048 - val_loss: 0.0179\
Fold 3: 0.017918523 loss, 0.75444424 auc\
Epoch 1/40\
88/88 [==============================] - 7s 76ms/step - loss: 0.3030 - val_loss: 0.0376\
Epoch 2/40\
88/88 [==============================] - 6s 70ms/step - loss: 0.0262 - val_loss: 0.0216\
Epoch 3/40\
88/88 [==============================] - 6s 70ms/step - loss: 0.0197 - val_loss: 0.0191\
Epoch 4/40\
88/88 [==============================] - 6s 70ms/step - loss: 0.0175 - val_loss: 0.0183\
Epoch 5/40\
88/88 [==============================] - 6s 73ms/step - loss: 0.0159 - val_loss: 0.0177\
Epoch 6/40\
88/88 [==============================] - 7s 75ms/step - loss: 0.0142 - val_loss: 0.0179\
Epoch 7/40\
88/88 [==============================] - 6s 68ms/step - loss: 0.0125 - val_loss: 0.0175\
Epoch 8/40\
88/88 [==============================] - 6s 66ms/step - loss: 0.0097 - val_loss: 0.0178\
Epoch 9/40\
88/88 [==============================] - 6s 69ms/step - loss: 0.0074 - val_loss: 0.0181\
Epoch 10/40\
88/88 [==============================] - 6s 66ms/step - loss: 0.0048 - val_loss: 0.0178\
Fold 4: 0.017810984 loss, 0.7598692 auc\
Epoch 1/40\
88/88 [==============================] - 7s 79ms/step - loss: 0.3019 - val_loss: 0.0374\
Epoch 2/40\
88/88 [==============================] - 6s 70ms/step - loss: 0.0260 - val_loss: 0.0221\
Epoch 3/40\
88/88 [==============================] - 6s 68ms/step - loss: 0.0196 - val_loss: 0.0192\
Epoch 4/40\
88/88 [==============================] - 6s 66ms/step - loss: 0.0176 - val_loss: 0.0186\
Epoch 5/40\
88/88 [==============================] - 6s 66ms/step - loss: 0.0160 - val_loss: 0.0179\
Epoch 6/40\
88/88 [==============================] - 6s 66ms/step - loss: 0.0145 - val_loss: 0.0176\
Epoch 7/40\
88/88 [==============================] - 6s 65ms/step - loss: 0.0124 - val_loss: 0.0178\
Epoch 8/40\
88/88 [==============================] - 6s 66ms/step - loss: 0.0105 - val_loss: 0.0183\
Epoch 9/40\
88/88 [==============================] - 6s 64ms/step - loss: 0.0076 - val_loss: 0.0174\
Epoch 10/40\
88/88 [==============================] - 6s 66ms/step - loss: 0.0064 - val_loss: 0.0175\
Epoch 11/40\
88/88 [==============================] - 6s 65ms/step - loss: 0.0057 - val_loss: 0.0176\
Epoch 12/40\
88/88 [==============================] - 6s 64ms/step - loss: 0.0052 - val_loss: 0.0176\
Fold 5: 0.01761556 loss, 0.77091914 auc\
[0.01696081, 0.017685069, 0.017918523, 0.017810984, 0.01761556] [0.78690624, 0.7693365, 0.75444424, 0.7598692, 0.77091914]\
\cf10 [I 2020-11-05 18:31:52,443]\cf2  Trial 40 finished with value: 0.017598189413547516 and parameters: \{'layers': 1, 'neurons': 1812\}. Best is trial 32 with value: 0.016830141469836236.\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 3s 33ms/step - loss: 0.3074 - val_loss: 0.0403\
Epoch 2/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0269 - val_loss: 0.0222\
Epoch 3/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0203 - val_loss: 0.0195\
Epoch 4/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0184 - val_loss: 0.0186\
Epoch 5/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0171 - val_loss: 0.0179\
Epoch 6/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0160 - val_loss: 0.0176\
Epoch 7/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0147 - val_loss: 0.0173\
Epoch 8/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0134 - val_loss: 0.0170\
Epoch 9/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0119 - val_loss: 0.0170\
Epoch 10/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0114 - val_loss: 0.0171\
Epoch 11/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0088 - val_loss: 0.0168\
Epoch 12/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0082 - val_loss: 0.0168\
Epoch 13/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0078 - val_loss: 0.0168\
Epoch 14/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.0075 - val_loss: 0.0168\
Epoch 15/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0075 - val_loss: 0.0168\
Fold 1: 0.016811205 loss, 0.79974145 auc\
Epoch 1/40\
88/88 [==============================] - 3s 38ms/step - loss: 0.3056 - val_loss: 0.0381\
Epoch 2/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0267 - val_loss: 0.0220\
Epoch 3/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0202 - val_loss: 0.0194\
Epoch 4/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0182 - val_loss: 0.0186\
Epoch 5/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0168 - val_loss: 0.0180\
Epoch 6/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0155 - val_loss: 0.0177\
Epoch 7/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0142 - val_loss: 0.0174\
Epoch 8/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0130 - val_loss: 0.0173\
Epoch 9/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0115 - val_loss: 0.0172\
Epoch 10/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.0100 - val_loss: 0.0175\
Epoch 11/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.0084 - val_loss: 0.0176\
Epoch 12/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0062 - val_loss: 0.0173\
Fold 2: 0.01730758 loss, 0.79556555 auc\
Epoch 1/40\
88/88 [==============================] - 3s 37ms/step - loss: 0.3059 - val_loss: 0.0436\
Epoch 2/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.0269 - val_loss: 0.0226\
Epoch 3/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.0202 - val_loss: 0.0197\
Epoch 4/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.0182 - val_loss: 0.0187\
Epoch 5/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0169 - val_loss: 0.0181\
Epoch 6/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0157 - val_loss: 0.0178\
Epoch 7/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0144 - val_loss: 0.0174\
Epoch 8/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.0131 - val_loss: 0.0172\
Epoch 9/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0117 - val_loss: 0.0175\
Epoch 10/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.0106 - val_loss: 0.0178\
Epoch 11/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0084 - val_loss: 0.0171\
Epoch 12/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0077 - val_loss: 0.0171\
Epoch 13/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0073 - val_loss: 0.0171\
Epoch 14/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0070 - val_loss: 0.0171\
Epoch 15/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0069 - val_loss: 0.0172\
Fold 3: 0.017151695 loss, 0.7964665 auc\
Epoch 1/40\
88/88 [==============================] - 3s 36ms/step - loss: 0.3066 - val_loss: 0.0415\
Epoch 2/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0283 - val_loss: 0.0218\
Epoch 3/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0215 - val_loss: 0.0280\
Epoch 4/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0191 - val_loss: 0.0184\
Epoch 5/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0172 - val_loss: 0.0179\
Epoch 6/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.0162 - val_loss: 0.0175\
Epoch 7/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.0151 - val_loss: 0.0172\
Epoch 8/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.0138 - val_loss: 0.0170\
Epoch 9/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.0125 - val_loss: 0.0169\
Epoch 10/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0111 - val_loss: 0.0170\
Epoch 11/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0090 - val_loss: 0.0167\
Epoch 12/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0085 - val_loss: 0.0167\
Epoch 13/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.0083 - val_loss: 0.0167\
Epoch 14/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0079 - val_loss: 0.0167\
Fold 4: 0.0167245 loss, 0.80253345 auc\
Epoch 1/40\
88/88 [==============================] - 3s 36ms/step - loss: 0.3077 - val_loss: 0.0383\
Epoch 2/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0267 - val_loss: 0.0219\
Epoch 3/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0202 - val_loss: 0.0194\
Epoch 4/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0182 - val_loss: 0.0185\
Epoch 5/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0169 - val_loss: 0.0178\
Epoch 6/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0157 - val_loss: 0.0174\
Epoch 7/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0144 - val_loss: 0.0172\
Epoch 8/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0134 - val_loss: 0.0170\
Epoch 9/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0117 - val_loss: 0.0169\
Epoch 10/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0110 - val_loss: 0.0171\
Epoch 11/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0086 - val_loss: 0.0168\
Epoch 12/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0080 - val_loss: 0.0168\
Epoch 13/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0076 - val_loss: 0.0168\
Epoch 14/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.0072 - val_loss: 0.0168\
Fold 5: 0.016811054 loss, 0.80059624 auc\
[0.016811205, 0.01730758, 0.017151695, 0.0167245, 0.016811054] [0.79974145, 0.79556555, 0.7964665, 0.80253345, 0.80059624]\
\cf10 [I 2020-11-05 18:35:22,088]\cf2  Trial 41 finished with value: 0.0169612068682909 and parameters: \{'layers': 0, 'neurons': 1712\}. Best is trial 32 with value: 0.016830141469836236.\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 3s 37ms/step - loss: 0.3031 - val_loss: 0.0368\
Epoch 2/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.0265 - val_loss: 0.0217\
Epoch 3/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.0200 - val_loss: 0.0190\
Epoch 4/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.0182 - val_loss: 0.0183\
Epoch 5/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.0168 - val_loss: 0.0178\
Epoch 6/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.0157 - val_loss: 0.0173\
Epoch 7/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.0144 - val_loss: 0.0171\
Epoch 8/40\
88/88 [==============================] - 3s 34ms/step - loss: 0.0131 - val_loss: 0.0169\
Epoch 9/40\
88/88 [==============================] - 3s 33ms/step - loss: 0.0116 - val_loss: 0.0169\
Epoch 10/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0100 - val_loss: 0.0170\
Epoch 11/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.0077 - val_loss: 0.0167\
Epoch 12/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.0071 - val_loss: 0.0167\
Epoch 13/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0068 - val_loss: 0.0167\
Epoch 14/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0065 - val_loss: 0.0167\
Fold 1: 0.01672657 loss, 0.80333346 auc\
Epoch 1/40\
88/88 [==============================] - 3s 38ms/step - loss: 0.2994 - val_loss: 0.0424\
Epoch 2/40\
88/88 [==============================] - 3s 33ms/step - loss: 0.0258 - val_loss: 0.0217\
Epoch 3/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0199 - val_loss: 0.0193\
Epoch 4/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.0180 - val_loss: 0.0184\
Epoch 5/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.0167 - val_loss: 0.0179\
Epoch 6/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.0155 - val_loss: 0.0174\
Epoch 7/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0141 - val_loss: 0.0172\
Epoch 8/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.0126 - val_loss: 0.0171\
Epoch 9/40\
88/88 [==============================] - 3s 33ms/step - loss: 0.0113 - val_loss: 0.0171\
Epoch 10/40\
88/88 [==============================] - 3s 33ms/step - loss: 0.0091 - val_loss: 0.0168\
Epoch 11/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.0084 - val_loss: 0.0167\
Epoch 12/40\
88/88 [==============================] - 3s 33ms/step - loss: 0.0081 - val_loss: 0.0168\
Epoch 13/40\
88/88 [==============================] - 3s 35ms/step - loss: 0.0077 - val_loss: 0.0168\
Epoch 14/40\
88/88 [==============================] - 3s 36ms/step - loss: 0.0077 - val_loss: 0.0168\
Fold 2: 0.016792411 loss, 0.8080899 auc\
Epoch 1/40\
88/88 [==============================] - 3s 37ms/step - loss: 0.3001 - val_loss: 0.0369\
Epoch 2/40\
88/88 [==============================] - 3s 34ms/step - loss: 0.0258 - val_loss: 0.0214\
Epoch 3/40\
88/88 [==============================] - 3s 33ms/step - loss: 0.0199 - val_loss: 0.0192\
Epoch 4/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.0179 - val_loss: 0.0184\
Epoch 5/40\
88/88 [==============================] - 3s 34ms/step - loss: 0.0166 - val_loss: 0.0179\
Epoch 6/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.0153 - val_loss: 0.0176\
Epoch 7/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.0141 - val_loss: 0.0174\
Epoch 8/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.0128 - val_loss: 0.0172\
Epoch 9/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.0114 - val_loss: 0.0175\
Epoch 10/40\
88/88 [==============================] - 3s 33ms/step - loss: 0.0100 - val_loss: 0.0180\
Epoch 11/40\
88/88 [==============================] - 3s 33ms/step - loss: 0.0080 - val_loss: 0.0172\
Fold 3: 0.017223926 loss, 0.7964865 auc\
Epoch 1/40\
88/88 [==============================] - 3s 37ms/step - loss: 0.3018 - val_loss: 0.0385\
Epoch 2/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.0259 - val_loss: 0.0217\
Epoch 3/40\
88/88 [==============================] - 3s 34ms/step - loss: 0.0199 - val_loss: 0.0193\
Epoch 4/40\
88/88 [==============================] - 3s 34ms/step - loss: 0.0179 - val_loss: 0.0183\
Epoch 5/40\
88/88 [==============================] - 3s 35ms/step - loss: 0.0165 - val_loss: 0.0177\
Epoch 6/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0153 - val_loss: 0.0175\
Epoch 7/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0139 - val_loss: 0.0173\
Epoch 8/40\
88/88 [==============================] - 3s 35ms/step - loss: 0.0126 - val_loss: 0.0172\
Epoch 9/40\
88/88 [==============================] - 3s 33ms/step - loss: 0.0113 - val_loss: 0.0173\
Epoch 10/40\
88/88 [==============================] - 3s 33ms/step - loss: 0.0098 - val_loss: 0.0173\
Epoch 11/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.0074 - val_loss: 0.0170\
Epoch 12/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.0067 - val_loss: 0.0169\
Epoch 13/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0063 - val_loss: 0.0170\
Epoch 14/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.0060 - val_loss: 0.0170\
Epoch 15/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0060 - val_loss: 0.0170\
Fold 4: 0.017002061 loss, 0.7916677 auc\
Epoch 1/40\
88/88 [==============================] - 3s 35ms/step - loss: 0.3023 - val_loss: 0.0391\
Epoch 2/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0270 - val_loss: 0.0250\
Epoch 3/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0211 - val_loss: 0.0228\
Epoch 4/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.0187 - val_loss: 0.0185\
Epoch 5/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0170 - val_loss: 0.0180\
Epoch 6/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0160 - val_loss: 0.0177\
Epoch 7/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0148 - val_loss: 0.0173\
Epoch 8/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.0136 - val_loss: 0.0173\
Epoch 9/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.0122 - val_loss: 0.0172\
Epoch 10/40\
88/88 [==============================] - 3s 33ms/step - loss: 0.0108 - val_loss: 0.0172\
Epoch 11/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0093 - val_loss: 0.0173\
Epoch 12/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.0070 - val_loss: 0.0170\
Epoch 13/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.0065 - val_loss: 0.0171\
Epoch 14/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0062 - val_loss: 0.0171\
Epoch 15/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0059 - val_loss: 0.0172\
Fold 5: 0.017164325 loss, 0.7912616 auc\
[0.01672657, 0.016792411, 0.017223926, 0.017002061, 0.017164325] [0.80333346, 0.8080899, 0.7964865, 0.7916677, 0.7912616]\
\cf10 [I 2020-11-05 18:39:09,944]\cf2  Trial 42 finished with value: 0.016981858760118484 and parameters: \{'layers': 0, 'neurons': 1912\}. Best is trial 32 with value: 0.016830141469836236.\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.3645 - val_loss: 0.0637\
Epoch 2/40\
88/88 [==============================] - 1s 14ms/step - loss: 0.0367 - val_loss: 0.0266\
Epoch 3/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0233 - val_loss: 0.0215\
Epoch 4/40\
88/88 [==============================] - 1s 16ms/step - loss: 0.0199 - val_loss: 0.0196\
Epoch 5/40\
88/88 [==============================] - 1s 14ms/step - loss: 0.0182 - val_loss: 0.0187\
Epoch 6/40\
88/88 [==============================] - 1s 13ms/step - loss: 0.0169 - val_loss: 0.0182\
Epoch 7/40\
88/88 [==============================] - 1s 13ms/step - loss: 0.0158 - val_loss: 0.0178\
Epoch 8/40\
88/88 [==============================] - 1s 13ms/step - loss: 0.0147 - val_loss: 0.0175\
Epoch 9/40\
88/88 [==============================] - 1s 13ms/step - loss: 0.0142 - val_loss: 0.0175\
Epoch 10/40\
88/88 [==============================] - 1s 13ms/step - loss: 0.0140 - val_loss: 0.0173\
Epoch 11/40\
88/88 [==============================] - 1s 13ms/step - loss: 0.0133 - val_loss: 0.0203\
Epoch 12/40\
88/88 [==============================] - 1s 12ms/step - loss: 0.0126 - val_loss: 0.0171\
Epoch 13/40\
88/88 [==============================] - 1s 14ms/step - loss: 0.0105 - val_loss: 0.0171\
Epoch 14/40\
88/88 [==============================] - 1s 14ms/step - loss: 0.0091 - val_loss: 0.0174\
Epoch 15/40\
88/88 [==============================] - 1s 14ms/step - loss: 0.0075 - val_loss: 0.0173\
\cf10 [I 2020-11-05 18:39:38,216]\cf2  Trial 43 pruned. \
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.3155 - val_loss: 0.0394\
Epoch 2/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0278 - val_loss: 0.0223\
Epoch 3/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0206 - val_loss: 0.0197\
Epoch 4/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0184 - val_loss: 0.0186\
Epoch 5/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0171 - val_loss: 0.0180\
Epoch 6/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0158 - val_loss: 0.0177\
Epoch 7/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0145 - val_loss: 0.0174\
Epoch 8/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0132 - val_loss: 0.0173\
Epoch 9/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0118 - val_loss: 0.0175\
Epoch 10/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0097 - val_loss: 0.0170\
Epoch 11/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0091 - val_loss: 0.0169\
Epoch 12/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0088 - val_loss: 0.0170\
Epoch 13/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0084 - val_loss: 0.0170\
Epoch 14/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0084 - val_loss: 0.0170\
Fold 1: 0.016971022 loss, 0.80339503 auc\
Epoch 1/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.3149 - val_loss: 0.0374\
Epoch 2/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0278 - val_loss: 0.0226\
Epoch 3/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0204 - val_loss: 0.0197\
Epoch 4/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0182 - val_loss: 0.0185\
Epoch 5/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0168 - val_loss: 0.0178\
Epoch 6/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0155 - val_loss: 0.0175\
Epoch 7/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0142 - val_loss: 0.0172\
Epoch 8/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0131 - val_loss: 0.0172\
Epoch 9/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0116 - val_loss: 0.0173\
Epoch 10/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0094 - val_loss: 0.0168\
Epoch 11/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0088 - val_loss: 0.0167\
Epoch 12/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0085 - val_loss: 0.0167\
Epoch 13/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0081 - val_loss: 0.0167\
Epoch 14/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0081 - val_loss: 0.0167\
Fold 2: 0.01667838 loss, 0.8058261 auc\
Epoch 1/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.3154 - val_loss: 0.0419\
Epoch 2/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0285 - val_loss: 0.0222\
Epoch 3/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0207 - val_loss: 0.0199\
Epoch 4/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0185 - val_loss: 0.0186\
Epoch 5/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0172 - val_loss: 0.0181\
Epoch 6/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0160 - val_loss: 0.0177\
Epoch 7/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0148 - val_loss: 0.0173\
Epoch 8/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0135 - val_loss: 0.0172\
Epoch 9/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0121 - val_loss: 0.0171\
Epoch 10/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0108 - val_loss: 0.0171\
Epoch 11/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0087 - val_loss: 0.0168\
Epoch 12/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0081 - val_loss: 0.0168\
Epoch 13/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0077 - val_loss: 0.0169\
Epoch 14/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0074 - val_loss: 0.0169\
Epoch 15/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0074 - val_loss: 0.0169\
Fold 3: 0.016913678 loss, 0.8035518 auc\
Epoch 1/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.3144 - val_loss: 0.0446\
Epoch 2/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0281 - val_loss: 0.0227\
Epoch 3/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0206 - val_loss: 0.0196\
Epoch 4/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0185 - val_loss: 0.0186\
Epoch 5/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0172 - val_loss: 0.0181\
Epoch 6/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0161 - val_loss: 0.0176\
Epoch 7/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0149 - val_loss: 0.0173\
Epoch 8/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0135 - val_loss: 0.0170\
Epoch 9/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0122 - val_loss: 0.0172\
Epoch 10/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0128 - val_loss: 0.0174\
Epoch 11/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0100 - val_loss: 0.0169\
Epoch 12/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0095 - val_loss: 0.0168\
Epoch 13/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0091 - val_loss: 0.0168\
Epoch 14/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0087 - val_loss: 0.0169\
Epoch 15/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0087 - val_loss: 0.0169\
Fold 4: 0.01686811 loss, 0.8058018 auc\
Epoch 1/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.3151 - val_loss: 0.0412\
Epoch 2/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0277 - val_loss: 0.0223\
Epoch 3/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0206 - val_loss: 0.0199\
Epoch 4/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0186 - val_loss: 0.0187\
Epoch 5/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0172 - val_loss: 0.0180\
Epoch 6/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0160 - val_loss: 0.0176\
Epoch 7/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0148 - val_loss: 0.0173\
Epoch 8/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0140 - val_loss: 0.0172\
Epoch 9/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0123 - val_loss: 0.0171\
Epoch 10/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0108 - val_loss: 0.0172\
Epoch 11/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0086 - val_loss: 0.0169\
Epoch 12/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0081 - val_loss: 0.0168\
Epoch 13/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0077 - val_loss: 0.0169\
Epoch 14/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0074 - val_loss: 0.0169\
Epoch 15/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0074 - val_loss: 0.0169\
Fold 5: 0.016897222 loss, 0.80204904 auc\
[0.016971022, 0.01667838, 0.016913678, 0.01686811, 0.016897222] [0.80339503, 0.8058261, 0.8035518, 0.8058018, 0.80204904]\
\cf10 [I 2020-11-05 18:42:49,741]\cf2  Trial 44 finished with value: 0.01686568260192871 and parameters: \{'layers': 0, 'neurons': 1512\}. Best is trial 32 with value: 0.016830141469836236.\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.3153 - val_loss: 0.0399\
Epoch 2/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0282 - val_loss: 0.0225\
Epoch 3/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0207 - val_loss: 0.0198\
Epoch 4/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0186 - val_loss: 0.0186\
Epoch 5/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0173 - val_loss: 0.0180\
Epoch 6/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0161 - val_loss: 0.0175\
Epoch 7/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0149 - val_loss: 0.0172\
Epoch 8/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0136 - val_loss: 0.0170\
Epoch 9/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0122 - val_loss: 0.0169\
Epoch 10/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0108 - val_loss: 0.0171\
Epoch 11/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0086 - val_loss: 0.0167\
Epoch 12/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0081 - val_loss: 0.0167\
Epoch 13/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0077 - val_loss: 0.0167\
Epoch 14/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0074 - val_loss: 0.0167\
Epoch 15/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0073 - val_loss: 0.0168\
Fold 1: 0.016750444 loss, 0.8044703 auc\
Epoch 1/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.3166 - val_loss: 0.0414\
Epoch 2/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0278 - val_loss: 0.0223\
Epoch 3/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0205 - val_loss: 0.0195\
Epoch 4/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0183 - val_loss: 0.0185\
Epoch 5/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0170 - val_loss: 0.0180\
Epoch 6/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0157 - val_loss: 0.0176\
Epoch 7/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0144 - val_loss: 0.0172\
Epoch 8/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0133 - val_loss: 0.0171\
Epoch 9/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0119 - val_loss: 0.0170\
Epoch 10/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0103 - val_loss: 0.0171\
Epoch 11/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0162 - val_loss: 0.0178\
Epoch 12/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0126 - val_loss: 0.0172\
Fold 2: 0.017215608 loss, 0.812876 auc\
Epoch 1/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.3152 - val_loss: 0.0398\
Epoch 2/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0278 - val_loss: 0.0224\
Epoch 3/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0205 - val_loss: 0.0197\
Epoch 4/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0184 - val_loss: 0.0187\
Epoch 5/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0170 - val_loss: 0.0181\
Epoch 6/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0158 - val_loss: 0.0177\
Epoch 7/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0146 - val_loss: 0.0175\
Epoch 8/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0133 - val_loss: 0.0172\
Epoch 9/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0120 - val_loss: 0.0174\
Epoch 10/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0106 - val_loss: 0.0173\
Epoch 11/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0083 - val_loss: 0.0169\
Epoch 12/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0077 - val_loss: 0.0169\
Epoch 13/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0073 - val_loss: 0.0169\
Epoch 14/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0070 - val_loss: 0.0170\
Fold 3: 0.016960835 loss, 0.79757166 auc\
Epoch 1/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.3146 - val_loss: 0.0405\
Epoch 2/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0277 - val_loss: 0.0226\
Epoch 3/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0205 - val_loss: 0.0198\
Epoch 4/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0183 - val_loss: 0.0186\
Epoch 5/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0169 - val_loss: 0.0180\
Epoch 6/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0156 - val_loss: 0.0176\
Epoch 7/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0144 - val_loss: 0.0174\
Epoch 8/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0131 - val_loss: 0.0174\
Epoch 9/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0121 - val_loss: 0.0179\
Epoch 10/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0104 - val_loss: 0.0171\
Epoch 11/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0095 - val_loss: 0.0169\
Epoch 12/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0091 - val_loss: 0.0169\
Epoch 13/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0087 - val_loss: 0.0169\
Epoch 14/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0084 - val_loss: 0.0169\
Fold 4: 0.016940974 loss, 0.80124646 auc\
Epoch 1/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.3146 - val_loss: 0.0416\
Epoch 2/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0279 - val_loss: 0.0226\
Epoch 3/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0205 - val_loss: 0.0196\
Epoch 4/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0184 - val_loss: 0.0186\
Epoch 5/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0170 - val_loss: 0.0180\
Epoch 6/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0157 - val_loss: 0.0176\
Epoch 7/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0144 - val_loss: 0.0172\
Epoch 8/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0138 - val_loss: 0.0172\
Epoch 9/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0119 - val_loss: 0.0172\
Epoch 10/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0097 - val_loss: 0.0168\
Epoch 11/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0092 - val_loss: 0.0167\
Epoch 12/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0089 - val_loss: 0.0167\
Epoch 13/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0086 - val_loss: 0.0168\
Epoch 14/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0085 - val_loss: 0.0168\
Fold 5: 0.016774604 loss, 0.81281525 auc\
[0.016750444, 0.017215608, 0.016960835, 0.016940974, 0.016774604] [0.8044703, 0.812876, 0.79757166, 0.80124646, 0.81281525]\
\cf10 [I 2020-11-05 18:45:50,144]\cf2  Trial 45 finished with value: 0.01692849285900593 and parameters: \{'layers': 0, 'neurons': 1512\}. Best is trial 32 with value: 0.016830141469836236.\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 3s 38ms/step - loss: 0.3095 - val_loss: 0.0364\
Epoch 2/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.0270 - val_loss: 0.0219\
Epoch 3/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0203 - val_loss: 0.0195\
Epoch 4/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0183 - val_loss: 0.0185\
Epoch 5/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0170 - val_loss: 0.0180\
Epoch 6/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0158 - val_loss: 0.0175\
Epoch 7/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0145 - val_loss: 0.0172\
Epoch 8/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0132 - val_loss: 0.0170\
Epoch 9/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0118 - val_loss: 0.0172\
Epoch 10/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0109 - val_loss: 0.0170\
Epoch 11/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0083 - val_loss: 0.0167\
Epoch 12/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0077 - val_loss: 0.0167\
Epoch 13/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0074 - val_loss: 0.0168\
Epoch 14/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0070 - val_loss: 0.0168\
Epoch 15/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0070 - val_loss: 0.0168\
Fold 1: 0.016798824 loss, 0.8016316 auc\
Epoch 1/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.3086 - val_loss: 0.0360\
Epoch 2/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0268 - val_loss: 0.0222\
Epoch 3/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0202 - val_loss: 0.0193\
Epoch 4/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0182 - val_loss: 0.0185\
Epoch 5/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0168 - val_loss: 0.0178\
Epoch 6/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0155 - val_loss: 0.0174\
Epoch 7/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0141 - val_loss: 0.0172\
Epoch 8/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0143 - val_loss: 0.0184\
Epoch 9/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0124 - val_loss: 0.0171\
Epoch 10/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0102 - val_loss: 0.0167\
Epoch 11/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0096 - val_loss: 0.0167\
Epoch 12/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0093 - val_loss: 0.0167\
Epoch 13/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0090 - val_loss: 0.0167\
Epoch 14/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0090 - val_loss: 0.0167\
Fold 2: 0.016695285 loss, 0.81280816 auc\
Epoch 1/40\
88/88 [==============================] - 3s 34ms/step - loss: 0.3112 - val_loss: 0.0419\
Epoch 2/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0279 - val_loss: 0.0220\
Epoch 3/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0212 - val_loss: 0.0231\
Epoch 4/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0186 - val_loss: 0.0186\
Epoch 5/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0172 - val_loss: 0.0182\
Epoch 6/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.0161 - val_loss: 0.0177\
Epoch 7/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0151 - val_loss: 0.0174\
Epoch 8/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0139 - val_loss: 0.0172\
Epoch 9/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0126 - val_loss: 0.0171\
Epoch 10/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0111 - val_loss: 0.0172\
Epoch 11/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0090 - val_loss: 0.0168\
Epoch 12/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0086 - val_loss: 0.0168\
Epoch 13/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0083 - val_loss: 0.0169\
Epoch 14/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0080 - val_loss: 0.0169\
Fold 3: 0.016886886 loss, 0.8080659 auc\
Epoch 1/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.3077 - val_loss: 0.0395\
Epoch 2/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0269 - val_loss: 0.0222\
Epoch 3/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0202 - val_loss: 0.0195\
Epoch 4/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0182 - val_loss: 0.0186\
Epoch 5/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0168 - val_loss: 0.0180\
Epoch 6/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0156 - val_loss: 0.0176\
Epoch 7/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0143 - val_loss: 0.0173\
Epoch 8/40\
88/88 [==============================] - 3s 28ms/step - loss: 0.0129 - val_loss: 0.0174\
Epoch 9/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0117 - val_loss: 0.0171\
Epoch 10/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0102 - val_loss: 0.0174\
Epoch 11/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0170 - val_loss: 0.0196\
Epoch 12/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0129 - val_loss: 0.0177\
Fold 4: 0.017661296 loss, 0.8007402 auc\
Epoch 1/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.3072 - val_loss: 0.0441\
Epoch 2/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0270 - val_loss: 0.0224\
Epoch 3/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0203 - val_loss: 0.0195\
Epoch 4/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0183 - val_loss: 0.0186\
Epoch 5/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0170 - val_loss: 0.0179\
Epoch 6/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0158 - val_loss: 0.0175\
Epoch 7/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0146 - val_loss: 0.0174\
Epoch 8/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0132 - val_loss: 0.0172\
Epoch 9/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0118 - val_loss: 0.0172\
Epoch 10/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0103 - val_loss: 0.0173\
Epoch 11/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0080 - val_loss: 0.0169\
Epoch 12/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0074 - val_loss: 0.0169\
Epoch 13/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0070 - val_loss: 0.0170\
Epoch 14/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0067 - val_loss: 0.0170\
Epoch 15/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0067 - val_loss: 0.0170\
Fold 5: 0.016980032 loss, 0.7977795 auc\
[0.016798824, 0.016695285, 0.016886886, 0.017661296, 0.016980032] [0.8016316, 0.81280816, 0.8080659, 0.8007402, 0.7977795]\
\cf10 [I 2020-11-05 18:49:09,818]\cf2  Trial 46 finished with value: 0.017004464566707612 and parameters: \{'layers': 0, 'neurons': 1712\}. Best is trial 32 with value: 0.016830141469836236.\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 7s 80ms/step - loss: 0.3094 - val_loss: 0.0403\
Epoch 2/40\
88/88 [==============================] - 6s 74ms/step - loss: 0.0275 - val_loss: 0.0229\
Epoch 3/40\
88/88 [==============================] - 7s 82ms/step - loss: 0.0203 - val_loss: 0.0194\
Epoch 4/40\
88/88 [==============================] - 7s 80ms/step - loss: 0.0180 - val_loss: 0.0181\
Epoch 5/40\
88/88 [==============================] - 7s 76ms/step - loss: 0.0164 - val_loss: 0.0176\
Epoch 6/40\
88/88 [==============================] - 7s 76ms/step - loss: 0.0145 - val_loss: 0.0175\
Epoch 7/40\
88/88 [==============================] - 7s 78ms/step - loss: 0.0125 - val_loss: 0.0178\
Epoch 8/40\
88/88 [==============================] - 7s 76ms/step - loss: 0.0099 - val_loss: 0.0183\
Epoch 9/40\
88/88 [==============================] - 7s 78ms/step - loss: 0.0066 - val_loss: 0.0177\
\cf10 [I 2020-11-05 18:50:25,243]\cf2  Trial 47 pruned. \
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 7s 74ms/step - loss: 0.3013 - val_loss: 0.0363\
Epoch 2/40\
88/88 [==============================] - 6s 73ms/step - loss: 0.0257 - val_loss: 0.0215\
Epoch 3/40\
88/88 [==============================] - 7s 77ms/step - loss: 0.0196 - val_loss: 0.0196\
Epoch 4/40\
88/88 [==============================] - 6s 71ms/step - loss: 0.0173 - val_loss: 0.0182\
Epoch 5/40\
88/88 [==============================] - 6s 68ms/step - loss: 0.0157 - val_loss: 0.0177\
Epoch 6/40\
88/88 [==============================] - 6s 69ms/step - loss: 0.0140 - val_loss: 0.0177\
Epoch 7/40\
88/88 [==============================] - 6s 74ms/step - loss: 0.0119 - val_loss: 0.0178\
Epoch 8/40\
88/88 [==============================] - 6s 68ms/step - loss: 0.0087 - val_loss: 0.0171\
Epoch 9/40\
88/88 [==============================] - 6s 70ms/step - loss: 0.0076 - val_loss: 0.0172\
Epoch 10/40\
88/88 [==============================] - 6s 69ms/step - loss: 0.0068 - val_loss: 0.0172\
Epoch 11/40\
88/88 [==============================] - 6s 70ms/step - loss: 0.0062 - val_loss: 0.0173\
\cf10 [I 2020-11-05 18:51:47,187]\cf2  Trial 48 pruned. \
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.3214 - val_loss: 0.0461\
Epoch 2/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0294 - val_loss: 0.0236\
Epoch 3/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0211 - val_loss: 0.0200\
Epoch 4/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0187 - val_loss: 0.0186\
Epoch 5/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0173 - val_loss: 0.0180\
Epoch 6/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0162 - val_loss: 0.0177\
Epoch 7/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0150 - val_loss: 0.0174\
Epoch 8/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0137 - val_loss: 0.0172\
Epoch 9/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0124 - val_loss: 0.0171\
Epoch 10/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0109 - val_loss: 0.0176\
Epoch 11/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0089 - val_loss: 0.0169\
Epoch 12/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0083 - val_loss: 0.0169\
Epoch 13/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0079 - val_loss: 0.0170\
Epoch 14/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0076 - val_loss: 0.0170\
Fold 1: 0.016994575 loss, 0.8025401 auc\
Epoch 1/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.3216 - val_loss: 0.0419\
Epoch 2/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0293 - val_loss: 0.0232\
Epoch 3/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0211 - val_loss: 0.0201\
Epoch 4/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0188 - val_loss: 0.0189\
Epoch 5/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0174 - val_loss: 0.0183\
Epoch 6/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0162 - val_loss: 0.0177\
Epoch 7/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0150 - val_loss: 0.0175\
Epoch 8/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0138 - val_loss: 0.0172\
Epoch 9/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0124 - val_loss: 0.0172\
Epoch 10/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0110 - val_loss: 0.0174\
Epoch 11/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0089 - val_loss: 0.0170\
Epoch 12/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0084 - val_loss: 0.0169\
Epoch 13/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0081 - val_loss: 0.0169\
Epoch 14/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0077 - val_loss: 0.0170\
Epoch 15/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0077 - val_loss: 0.0170\
Fold 2: 0.016972182 loss, 0.80084574 auc\
Epoch 1/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.3213 - val_loss: 0.0445\
Epoch 2/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0292 - val_loss: 0.0228\
Epoch 3/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0210 - val_loss: 0.0200\
Epoch 4/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0186 - val_loss: 0.0188\
Epoch 5/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0172 - val_loss: 0.0183\
Epoch 6/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0160 - val_loss: 0.0177\
Epoch 7/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0150 - val_loss: 0.0174\
Epoch 8/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0135 - val_loss: 0.0174\
Epoch 9/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0125 - val_loss: 0.0172\
Epoch 10/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0110 - val_loss: 0.0173\
Epoch 11/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0094 - val_loss: 0.0172\
Epoch 12/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0071 - val_loss: 0.0170\
Epoch 13/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0066 - val_loss: 0.0171\
Epoch 14/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0063 - val_loss: 0.0171\
Epoch 15/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0060 - val_loss: 0.0171\
Fold 3: 0.01714806 loss, 0.7901557 auc\
Epoch 1/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.3217 - val_loss: 0.0384\
Epoch 2/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0287 - val_loss: 0.0230\
Epoch 3/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0208 - val_loss: 0.0197\
Epoch 4/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0185 - val_loss: 0.0186\
Epoch 5/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0171 - val_loss: 0.0180\
Epoch 6/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0158 - val_loss: 0.0175\
Epoch 7/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0145 - val_loss: 0.0172\
Epoch 8/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0133 - val_loss: 0.0171\
Epoch 9/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0122 - val_loss: 0.0173\
Epoch 10/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0100 - val_loss: 0.0167\
Epoch 11/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0094 - val_loss: 0.0167\
Epoch 12/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0090 - val_loss: 0.0167\
Epoch 13/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0087 - val_loss: 0.0167\
Epoch 14/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0087 - val_loss: 0.0167\
Epoch 15/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0086 - val_loss: 0.0167\
Epoch 16/40\
88/88 [==============================] - 2s 22ms/step - loss: 0.0086 - val_loss: 0.0167\
Fold 4: 0.016673638 loss, 0.8147971 auc\
Epoch 1/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.3189 - val_loss: 0.0398\
Epoch 2/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0299 - val_loss: 0.0253\
Epoch 3/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0251 - val_loss: 0.0196\
Epoch 4/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0188 - val_loss: 0.0188\
Epoch 5/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0178 - val_loss: 0.0182\
Epoch 6/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0169 - val_loss: 0.0178\
Epoch 7/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0159 - val_loss: 0.0175\
Epoch 8/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0149 - val_loss: 0.0172\
Epoch 9/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0138 - val_loss: 0.0171\
Epoch 10/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0125 - val_loss: 0.0171\
Epoch 11/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0113 - val_loss: 0.0170\
Epoch 12/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0099 - val_loss: 0.0170\
Epoch 13/40\
88/88 [==============================] - 2s 21ms/step - loss: 0.0088 - val_loss: 0.0171\
Epoch 14/40\
88/88 [==============================] - 2s 20ms/step - loss: 0.0066 - val_loss: 0.0170\
Fold 5: 0.01698506 loss, 0.7968384 auc\
[0.016994575, 0.016972182, 0.01714806, 0.016673638, 0.01698506] [0.8025401, 0.80084574, 0.7901557, 0.8147971, 0.7968384]\
\cf10 [I 2020-11-05 18:54:39,756]\cf2  Trial 49 finished with value: 0.016954703256487846 and parameters: \{'layers': 0, 'neurons': 1362\}. Best is trial 32 with value: 0.016830141469836236.\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.3137 - val_loss: 0.0381\
Epoch 2/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0275 - val_loss: 0.0220\
Epoch 3/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0205 - val_loss: 0.0196\
Epoch 4/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0184 - val_loss: 0.0187\
Epoch 5/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0171 - val_loss: 0.0183\
Epoch 6/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0160 - val_loss: 0.0177\
Epoch 7/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0148 - val_loss: 0.0174\
Epoch 8/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0135 - val_loss: 0.0172\
Epoch 9/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0121 - val_loss: 0.0172\
Epoch 10/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0109 - val_loss: 0.0171\
Epoch 11/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0086 - val_loss: 0.0168\
Epoch 12/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0080 - val_loss: 0.0168\
Epoch 13/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0077 - val_loss: 0.0168\
Epoch 14/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0074 - val_loss: 0.0169\
Fold 1: 0.016855707 loss, 0.8008345 auc\
Epoch 1/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.3121 - val_loss: 0.0380\
Epoch 2/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0273 - val_loss: 0.0225\
Epoch 3/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0204 - val_loss: 0.0196\
Epoch 4/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0183 - val_loss: 0.0186\
Epoch 5/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0170 - val_loss: 0.0180\
Epoch 6/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0158 - val_loss: 0.0175\
Epoch 7/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0148 - val_loss: 0.0172\
Epoch 8/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0133 - val_loss: 0.0172\
Epoch 9/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0120 - val_loss: 0.0173\
Epoch 10/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0098 - val_loss: 0.0168\
Epoch 11/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0093 - val_loss: 0.0168\
Epoch 12/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0090 - val_loss: 0.0168\
Epoch 13/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0086 - val_loss: 0.0168\
Epoch 14/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0086 - val_loss: 0.0168\
Epoch 15/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0086 - val_loss: 0.0168\
Fold 2: 0.016776957 loss, 0.8096701 auc\
Epoch 1/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.3114 - val_loss: 0.0405\
Epoch 2/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0274 - val_loss: 0.0222\
Epoch 3/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0203 - val_loss: 0.0196\
Epoch 4/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0183 - val_loss: 0.0187\
Epoch 5/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0169 - val_loss: 0.0180\
Epoch 6/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0157 - val_loss: 0.0175\
Epoch 7/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0144 - val_loss: 0.0174\
Epoch 8/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0130 - val_loss: 0.0173\
Epoch 9/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0117 - val_loss: 0.0172\
Epoch 10/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0102 - val_loss: 0.0172\
Epoch 11/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0087 - val_loss: 0.0176\
Epoch 12/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0065 - val_loss: 0.0172\
Fold 3: 0.017205978 loss, 0.7879479 auc\
Epoch 1/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.3105 - val_loss: 0.0394\
Epoch 2/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0272 - val_loss: 0.0220\
Epoch 3/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0203 - val_loss: 0.0196\
Epoch 4/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0182 - val_loss: 0.0186\
Epoch 5/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0169 - val_loss: 0.0180\
Epoch 6/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0156 - val_loss: 0.0175\
Epoch 7/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0143 - val_loss: 0.0172\
Epoch 8/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0129 - val_loss: 0.0171\
Epoch 9/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0135 - val_loss: 0.0174\
Epoch 10/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0109 - val_loss: 0.0170\
Epoch 11/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0085 - val_loss: 0.0168\
Epoch 12/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0081 - val_loss: 0.0168\
Epoch 13/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0078 - val_loss: 0.0168\
Epoch 14/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0075 - val_loss: 0.0168\
Fold 4: 0.016834488 loss, 0.8050318 auc\
Epoch 1/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.3084 - val_loss: 0.0420\
Epoch 2/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0287 - val_loss: 0.0226\
Epoch 3/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0208 - val_loss: 0.0198\
Epoch 4/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0187 - val_loss: 0.0189\
Epoch 5/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0174 - val_loss: 0.0179\
Epoch 6/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0162 - val_loss: 0.0176\
Epoch 7/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0150 - val_loss: 0.0173\
Epoch 8/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0137 - val_loss: 0.0171\
Epoch 9/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0125 - val_loss: 0.0171\
Epoch 10/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0110 - val_loss: 0.0171\
Epoch 11/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0088 - val_loss: 0.0168\
Epoch 12/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0083 - val_loss: 0.0169\
Epoch 13/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0081 - val_loss: 0.0169\
Epoch 14/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0078 - val_loss: 0.0169\
Fold 5: 0.016918432 loss, 0.8043575 auc\
[0.016855707, 0.016776957, 0.017205978, 0.016834488, 0.016918432] [0.8008345, 0.8096701, 0.7879479, 0.8050318, 0.8043575]\
\cf10 [I 2020-11-05 18:57:34,061]\cf2  Trial 50 finished with value: 0.016918312385678293 and parameters: \{'layers': 0, 'neurons': 1612\}. Best is trial 32 with value: 0.016830141469836236.\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.3110 - val_loss: 0.0430\
Epoch 2/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0272 - val_loss: 0.0225\
Epoch 3/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0204 - val_loss: 0.0195\
Epoch 4/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0184 - val_loss: 0.0185\
Epoch 5/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0171 - val_loss: 0.0179\
Epoch 6/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0158 - val_loss: 0.0175\
Epoch 7/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0146 - val_loss: 0.0174\
Epoch 8/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0134 - val_loss: 0.0173\
Epoch 9/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0122 - val_loss: 0.0171\
Epoch 10/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0104 - val_loss: 0.0171\
Epoch 11/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0128 - val_loss: 0.0177\
Epoch 12/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0101 - val_loss: 0.0171\
Epoch 13/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0090 - val_loss: 0.0169\
Epoch 14/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0086 - val_loss: 0.0169\
Epoch 15/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0083 - val_loss: 0.0172\
Epoch 16/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0080 - val_loss: 0.0170\
Epoch 17/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0078 - val_loss: 0.0170\
Fold 1: 0.01697949 loss, 0.7968983 auc\
Epoch 1/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.3142 - val_loss: 0.0457\
Epoch 2/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0278 - val_loss: 0.0224\
Epoch 3/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0205 - val_loss: 0.0195\
Epoch 4/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0183 - val_loss: 0.0185\
Epoch 5/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0170 - val_loss: 0.0180\
Epoch 6/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0157 - val_loss: 0.0176\
Epoch 7/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0144 - val_loss: 0.0173\
Epoch 8/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0131 - val_loss: 0.0172\
Epoch 9/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0117 - val_loss: 0.0173\
Epoch 10/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0110 - val_loss: 0.0172\
Epoch 11/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0084 - val_loss: 0.0170\
Epoch 12/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0078 - val_loss: 0.0170\
Epoch 13/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0074 - val_loss: 0.0170\
Epoch 14/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0071 - val_loss: 0.0170\
Epoch 15/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0071 - val_loss: 0.0170\
Fold 2: 0.017044503 loss, 0.7921721 auc\
Epoch 1/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.3109 - val_loss: 0.0367\
Epoch 2/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0272 - val_loss: 0.0224\
Epoch 3/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0205 - val_loss: 0.0196\
Epoch 4/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0184 - val_loss: 0.0186\
Epoch 5/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0171 - val_loss: 0.0180\
Epoch 6/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0160 - val_loss: 0.0176\
Epoch 7/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0148 - val_loss: 0.0173\
Epoch 8/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0134 - val_loss: 0.0171\
Epoch 9/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0123 - val_loss: 0.0171\
Epoch 10/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0108 - val_loss: 0.0169\
Epoch 11/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0092 - val_loss: 0.0171\
Epoch 12/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0078 - val_loss: 0.0171\
Epoch 13/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0056 - val_loss: 0.0170\
Fold 3: 0.016976224 loss, 0.79357356 auc\
Epoch 1/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.3111 - val_loss: 0.0372\
Epoch 2/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0272 - val_loss: 0.0217\
Epoch 3/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0204 - val_loss: 0.0197\
Epoch 4/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0182 - val_loss: 0.0186\
Epoch 5/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0169 - val_loss: 0.0180\
Epoch 6/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0156 - val_loss: 0.0176\
Epoch 7/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0143 - val_loss: 0.0173\
Epoch 8/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0129 - val_loss: 0.0174\
Epoch 9/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0130 - val_loss: 0.0173\
Epoch 10/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0103 - val_loss: 0.0168\
Epoch 11/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0098 - val_loss: 0.0168\
Epoch 12/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0095 - val_loss: 0.0168\
Epoch 13/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0091 - val_loss: 0.0168\
Epoch 14/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0091 - val_loss: 0.0168\
Epoch 15/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0091 - val_loss: 0.0168\
Epoch 16/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0090 - val_loss: 0.0168\
Fold 4: 0.016774604 loss, 0.8088424 auc\
Epoch 1/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.3096 - val_loss: 0.0367\
Epoch 2/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0272 - val_loss: 0.0224\
Epoch 3/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0203 - val_loss: 0.0196\
Epoch 4/40\
88/88 [==============================] - 2s 23ms/step - loss: 0.0182 - val_loss: 0.0185\
Epoch 5/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0167 - val_loss: 0.0179\
Epoch 6/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0154 - val_loss: 0.0175\
Epoch 7/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0142 - val_loss: 0.0173\
Epoch 8/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0127 - val_loss: 0.0172\
Epoch 9/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0114 - val_loss: 0.0172\
Epoch 10/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0092 - val_loss: 0.0169\
Epoch 11/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0086 - val_loss: 0.0168\
Epoch 12/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.0082 - val_loss: 0.0169\
Epoch 13/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0078 - val_loss: 0.0168\
Epoch 14/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0078 - val_loss: 0.0168\
Fold 5: 0.016849061 loss, 0.80851763 auc\
[0.01697949, 0.017044503, 0.016976224, 0.016774604, 0.016849061] [0.7968983, 0.7921721, 0.79357356, 0.8088424, 0.80851763]\
\cf10 [I 2020-11-05 19:00:45,022]\cf2  Trial 51 finished with value: 0.01692477650940418 and parameters: \{'layers': 0, 'neurons': 1612\}. Best is trial 32 with value: 0.016830141469836236.\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 3s 34ms/step - loss: 0.3051 - val_loss: 0.0365\
Epoch 2/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.0266 - val_loss: 0.0220\
Epoch 3/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.0202 - val_loss: 0.0194\
Epoch 4/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0181 - val_loss: 0.0185\
Epoch 5/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0167 - val_loss: 0.0179\
Epoch 6/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0154 - val_loss: 0.0176\
Epoch 7/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0141 - val_loss: 0.0172\
Epoch 8/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0127 - val_loss: 0.0171\
Epoch 9/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0114 - val_loss: 0.0172\
Epoch 10/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.0101 - val_loss: 0.0172\
Epoch 11/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0076 - val_loss: 0.0168\
Epoch 12/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0070 - val_loss: 0.0169\
Epoch 13/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0066 - val_loss: 0.0169\
Epoch 14/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0062 - val_loss: 0.0169\
Fold 1: 0.016943675 loss, 0.79018927 auc\
Epoch 1/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.3032 - val_loss: 0.0378\
Epoch 2/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0263 - val_loss: 0.0217\
Epoch 3/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0201 - val_loss: 0.0195\
Epoch 4/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0181 - val_loss: 0.0186\
Epoch 5/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0168 - val_loss: 0.0181\
Epoch 6/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0157 - val_loss: 0.0175\
Epoch 7/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0144 - val_loss: 0.0174\
Epoch 8/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0130 - val_loss: 0.0172\
Epoch 9/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0119 - val_loss: 0.0175\
Epoch 10/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0128 - val_loss: 0.0209\
Epoch 11/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0144 - val_loss: 0.0208\
Fold 2: 0.020679858 loss, 0.8027095 auc\
Epoch 1/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.3070 - val_loss: 0.0363\
Epoch 2/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0265 - val_loss: 0.0217\
Epoch 3/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0201 - val_loss: 0.0197\
Epoch 4/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0181 - val_loss: 0.0184\
Epoch 5/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0166 - val_loss: 0.0179\
Epoch 6/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0154 - val_loss: 0.0176\
Epoch 7/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0143 - val_loss: 0.0175\
Epoch 8/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0127 - val_loss: 0.0172\
Epoch 9/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0111 - val_loss: 0.0173\
Epoch 10/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0097 - val_loss: 0.0174\
Epoch 11/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0074 - val_loss: 0.0170\
Epoch 12/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0068 - val_loss: 0.0170\
Epoch 13/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0064 - val_loss: 0.0171\
Epoch 14/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0060 - val_loss: 0.0171\
Fold 3: 0.017097054 loss, 0.7941883 auc\
Epoch 1/40\
88/88 [==============================] - 3s 34ms/step - loss: 0.3059 - val_loss: 0.0454\
Epoch 2/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0268 - val_loss: 0.0220\
Epoch 3/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0202 - val_loss: 0.0195\
Epoch 4/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0182 - val_loss: 0.0184\
Epoch 5/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0169 - val_loss: 0.0178\
Epoch 6/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0156 - val_loss: 0.0175\
Epoch 7/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0143 - val_loss: 0.0173\
Epoch 8/40\
88/88 [==============================] - 3s 28ms/step - loss: 0.0133 - val_loss: 0.0171\
Epoch 9/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0117 - val_loss: 0.0171\
Epoch 10/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0103 - val_loss: 0.0173\
Epoch 11/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0080 - val_loss: 0.0168\
Epoch 12/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0074 - val_loss: 0.0168\
Epoch 13/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0070 - val_loss: 0.0168\
Epoch 14/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0067 - val_loss: 0.0169\
Epoch 15/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0066 - val_loss: 0.0169\
Fold 4: 0.016865108 loss, 0.80133104 auc\
Epoch 1/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.3079 - val_loss: 0.0366\
Epoch 2/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0267 - val_loss: 0.0217\
Epoch 3/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0203 - val_loss: 0.0194\
Epoch 4/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0183 - val_loss: 0.0186\
Epoch 5/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0170 - val_loss: 0.0179\
Epoch 6/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0159 - val_loss: 0.0176\
Epoch 7/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0147 - val_loss: 0.0172\
Epoch 8/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0133 - val_loss: 0.0171\
Epoch 9/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0119 - val_loss: 0.0171\
Epoch 10/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0105 - val_loss: 0.0171\
Epoch 11/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0082 - val_loss: 0.0167\
Epoch 12/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0076 - val_loss: 0.0167\
Epoch 13/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.0072 - val_loss: 0.0167\
Epoch 14/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0069 - val_loss: 0.0168\
Fold 5: 0.016760897 loss, 0.8020753 auc\
[0.016943675, 0.020679858, 0.017097054, 0.016865108, 0.016760897] [0.79018927, 0.8027095, 0.7941883, 0.80133104, 0.8020753]\
\cf10 [I 2020-11-05 19:03:55,585]\cf2  Trial 52 finished with value: 0.0176693182438612 and parameters: \{'layers': 0, 'neurons': 1762\}. Best is trial 32 with value: 0.016830141469836236.\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.3124 - val_loss: 0.0434\
Epoch 2/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0274 - val_loss: 0.0224\
Epoch 3/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0204 - val_loss: 0.0198\
Epoch 4/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0183 - val_loss: 0.0186\
Epoch 5/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0169 - val_loss: 0.0182\
Epoch 6/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0157 - val_loss: 0.0177\
Epoch 7/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0144 - val_loss: 0.0174\
Epoch 8/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0131 - val_loss: 0.0173\
Epoch 9/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0129 - val_loss: 0.0177\
Epoch 10/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0106 - val_loss: 0.0170\
Epoch 11/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0101 - val_loss: 0.0169\
Epoch 12/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0097 - val_loss: 0.0169\
Epoch 13/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0093 - val_loss: 0.0169\
Epoch 14/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0089 - val_loss: 0.0170\
Epoch 15/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0089 - val_loss: 0.0170\
Fold 1: 0.016962541 loss, 0.8064012 auc\
Epoch 1/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.3115 - val_loss: 0.0436\
Epoch 2/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0273 - val_loss: 0.0223\
Epoch 3/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0203 - val_loss: 0.0195\
Epoch 4/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0181 - val_loss: 0.0185\
Epoch 5/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0167 - val_loss: 0.0179\
Epoch 6/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0154 - val_loss: 0.0175\
Epoch 7/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0141 - val_loss: 0.0173\
Epoch 8/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0139 - val_loss: 0.0171\
Epoch 9/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0128 - val_loss: 0.0173\
Epoch 10/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0106 - val_loss: 0.0171\
Epoch 11/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0084 - val_loss: 0.0168\
Epoch 12/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0080 - val_loss: 0.0169\
Epoch 13/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0077 - val_loss: 0.0169\
Epoch 14/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0074 - val_loss: 0.0169\
Fold 2: 0.016923554 loss, 0.79787105 auc\
Epoch 1/40\
88/88 [==============================] - 3s 36ms/step - loss: 0.3111 - val_loss: 0.0410\
Epoch 2/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0291 - val_loss: 0.0219\
Epoch 3/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0219 - val_loss: 0.0243\
Epoch 4/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0188 - val_loss: 0.0186\
Epoch 5/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0174 - val_loss: 0.0181\
Epoch 6/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0164 - val_loss: 0.0177\
Epoch 7/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0154 - val_loss: 0.0174\
Epoch 8/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0142 - val_loss: 0.0171\
Epoch 9/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0130 - val_loss: 0.0170\
Epoch 10/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0116 - val_loss: 0.0172\
Epoch 11/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0102 - val_loss: 0.0172\
Epoch 12/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0080 - val_loss: 0.0169\
Epoch 13/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0075 - val_loss: 0.0170\
Epoch 14/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0073 - val_loss: 0.0170\
Epoch 15/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0070 - val_loss: 0.0170\
Fold 3: 0.017046189 loss, 0.79546094 auc\
Epoch 1/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.3114 - val_loss: 0.0376\
Epoch 2/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0270 - val_loss: 0.0216\
Epoch 3/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0202 - val_loss: 0.0194\
Epoch 4/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0181 - val_loss: 0.0184\
Epoch 5/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0167 - val_loss: 0.0178\
Epoch 6/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0155 - val_loss: 0.0174\
Epoch 7/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0140 - val_loss: 0.0170\
Epoch 8/40\
88/88 [==============================] - 3s 28ms/step - loss: 0.0128 - val_loss: 0.0172\
Epoch 9/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0115 - val_loss: 0.0172\
Epoch 10/40\
88/88 [==============================] - 2s 26ms/step - loss: 0.0093 - val_loss: 0.0167\
Epoch 11/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0086 - val_loss: 0.0166\
Epoch 12/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0082 - val_loss: 0.0166\
Epoch 13/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0078 - val_loss: 0.0166\
Epoch 14/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.0078 - val_loss: 0.0167\
Fold 4: 0.016655471 loss, 0.808367 auc\
Epoch 1/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.3105 - val_loss: 0.0388\
Epoch 2/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0271 - val_loss: 0.0223\
Epoch 3/40\
88/88 [==============================] - 2s 27ms/step - loss: 0.0204 - val_loss: 0.0194\
Epoch 4/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0183 - val_loss: 0.0184\
Epoch 5/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0169 - val_loss: 0.0178\
Epoch 6/40\
88/88 [==============================] - 2s 28ms/step - loss: 0.0157 - val_loss: 0.0175\
Epoch 7/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0144 - val_loss: 0.0172\
Epoch 8/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0131 - val_loss: 0.0171\
Epoch 9/40\
88/88 [==============================] - 2s 24ms/step - loss: 0.0117 - val_loss: 0.0170\
Epoch 10/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0104 - val_loss: 0.0172\
Epoch 11/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0088 - val_loss: 0.0173\
Epoch 12/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0065 - val_loss: 0.0169\
Epoch 13/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0058 - val_loss: 0.0170\
Epoch 14/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0055 - val_loss: 0.0170\
Epoch 15/40\
88/88 [==============================] - 2s 25ms/step - loss: 0.0055 - val_loss: 0.0170\
Fold 5: 0.017042989 loss, 0.78697747 auc\
[0.016962541, 0.016923554, 0.017046189, 0.016655471, 0.017042989] [0.8064012, 0.79787105, 0.79546094, 0.808367, 0.78697747]\
\cf10 [I 2020-11-05 19:07:13,361]\cf2  Trial 53 finished with value: 0.016926148906350136 and parameters: \{'layers': 0, 'neurons': 1612\}. Best is trial 32 with value: 0.016830141469836236.\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 5s 53ms/step - loss: 0.3155 - val_loss: 0.0389\
Epoch 2/40\
88/88 [==============================] - 4s 50ms/step - loss: 0.0278 - val_loss: 0.0223\
Epoch 3/40\
88/88 [==============================] - 4s 48ms/step - loss: 0.0203 - val_loss: 0.0195\
Epoch 4/40\
88/88 [==============================] - 5s 53ms/step - loss: 0.0179 - val_loss: 0.0184\
Epoch 5/40\
88/88 [==============================] - 5s 52ms/step - loss: 0.0163 - val_loss: 0.0176\
Epoch 6/40\
88/88 [==============================] - 5s 52ms/step - loss: 0.0146 - val_loss: 0.0173\
Epoch 7/40\
88/88 [==============================] - 4s 46ms/step - loss: 0.0130 - val_loss: 0.0175\
Epoch 8/40\
88/88 [==============================] - 4s 50ms/step - loss: 0.0108 - val_loss: 0.0175\
Epoch 9/40\
88/88 [==============================] - 4s 49ms/step - loss: 0.0077 - val_loss: 0.0172\
Epoch 10/40\
88/88 [==============================] - 4s 49ms/step - loss: 0.0067 - val_loss: 0.0173\
Epoch 11/40\
88/88 [==============================] - 4s 49ms/step - loss: 0.0062 - val_loss: 0.0173\
Epoch 12/40\
88/88 [==============================] - 5s 52ms/step - loss: 0.0061 - val_loss: 0.0173\
\cf10 [I 2020-11-05 19:08:19,409]\cf2  Trial 54 pruned. \
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 4s 40ms/step - loss: 0.2969 - val_loss: 0.0383\
Epoch 2/40\
88/88 [==============================] - 3s 35ms/step - loss: 0.0255 - val_loss: 0.0217\
Epoch 3/40\
88/88 [==============================] - 3s 35ms/step - loss: 0.0197 - val_loss: 0.0191\
Epoch 4/40\
88/88 [==============================] - 3s 35ms/step - loss: 0.0179 - val_loss: 0.0183\
Epoch 5/40\
88/88 [==============================] - 3s 35ms/step - loss: 0.0167 - val_loss: 0.0178\
Epoch 6/40\
88/88 [==============================] - 3s 35ms/step - loss: 0.0154 - val_loss: 0.0174\
Epoch 7/40\
88/88 [==============================] - 3s 38ms/step - loss: 0.0141 - val_loss: 0.0172\
Epoch 8/40\
88/88 [==============================] - 3s 36ms/step - loss: 0.0127 - val_loss: 0.0171\
Epoch 9/40\
88/88 [==============================] - 3s 36ms/step - loss: 0.0121 - val_loss: 0.0176\
Epoch 10/40\
88/88 [==============================] - 3s 37ms/step - loss: 0.0102 - val_loss: 0.0169\
Epoch 11/40\
88/88 [==============================] - 3s 36ms/step - loss: 0.0095 - val_loss: 0.0168\
Epoch 12/40\
88/88 [==============================] - 4s 41ms/step - loss: 0.0090 - val_loss: 0.0168\
Epoch 13/40\
88/88 [==============================] - 3s 40ms/step - loss: 0.0086 - val_loss: 0.0168\
Epoch 14/40\
88/88 [==============================] - 3s 34ms/step - loss: 0.0086 - val_loss: 0.0168\
Epoch 15/40\
88/88 [==============================] - 3s 34ms/step - loss: 0.0085 - val_loss: 0.0168\
Fold 1: 0.016838478 loss, 0.803925 auc\
Epoch 1/40\
88/88 [==============================] - 4s 41ms/step - loss: 0.3008 - val_loss: 0.0356\
Epoch 2/40\
88/88 [==============================] - 3s 34ms/step - loss: 0.0256 - val_loss: 0.0211\
Epoch 3/40\
88/88 [==============================] - 4s 48ms/step - loss: 0.0197 - val_loss: 0.0192\
Epoch 4/40\
88/88 [==============================] - 4s 42ms/step - loss: 0.0179 - val_loss: 0.0185\
Epoch 5/40\
88/88 [==============================] - 3s 36ms/step - loss: 0.0167 - val_loss: 0.0178\
Epoch 6/40\
88/88 [==============================] - 3s 34ms/step - loss: 0.0154 - val_loss: 0.0175\
Epoch 7/40\
88/88 [==============================] - 3s 34ms/step - loss: 0.0141 - val_loss: 0.0173\
Epoch 8/40\
88/88 [==============================] - 3s 34ms/step - loss: 0.0128 - val_loss: 0.0173\
Epoch 9/40\
88/88 [==============================] - 3s 37ms/step - loss: 0.0114 - val_loss: 0.0174\
Epoch 10/40\
88/88 [==============================] - 3s 37ms/step - loss: 0.0092 - val_loss: 0.0168\
Epoch 11/40\
88/88 [==============================] - 3s 34ms/step - loss: 0.0086 - val_loss: 0.0168\
Epoch 12/40\
88/88 [==============================] - 3s 35ms/step - loss: 0.0083 - val_loss: 0.0168\
Epoch 13/40\
88/88 [==============================] - 3s 36ms/step - loss: 0.0079 - val_loss: 0.0168\
Epoch 14/40\
88/88 [==============================] - 3s 35ms/step - loss: 0.0079 - val_loss: 0.0168\
Fold 2: 0.016826661 loss, 0.80750006 auc\
Epoch 1/40\
88/88 [==============================] - 4s 41ms/step - loss: 0.2948 - val_loss: 0.0383\
Epoch 2/40\
88/88 [==============================] - 3s 35ms/step - loss: 0.0263 - val_loss: 0.0300\
Epoch 3/40\
88/88 [==============================] - 3s 35ms/step - loss: 0.0214 - val_loss: 0.0204\
Epoch 4/40\
88/88 [==============================] - 3s 34ms/step - loss: 0.0184 - val_loss: 0.0189\
Epoch 5/40\
88/88 [==============================] - 3s 35ms/step - loss: 0.0175 - val_loss: 0.0180\
Epoch 6/40\
88/88 [==============================] - 3s 36ms/step - loss: 0.0162 - val_loss: 0.0175\
Epoch 7/40\
88/88 [==============================] - 3s 35ms/step - loss: 0.0151 - val_loss: 0.0172\
Epoch 8/40\
88/88 [==============================] - 3s 38ms/step - loss: 0.0139 - val_loss: 0.0170\
Epoch 9/40\
88/88 [==============================] - 3s 36ms/step - loss: 0.0125 - val_loss: 0.0169\
Epoch 10/40\
88/88 [==============================] - 3s 37ms/step - loss: 0.0111 - val_loss: 0.0169\
Epoch 11/40\
88/88 [==============================] - 3s 36ms/step - loss: 0.0096 - val_loss: 0.0170\
Epoch 12/40\
88/88 [==============================] - 3s 35ms/step - loss: 0.0073 - val_loss: 0.0167\
Epoch 13/40\
88/88 [==============================] - 3s 35ms/step - loss: 0.0068 - val_loss: 0.0167\
Epoch 14/40\
88/88 [==============================] - 3s 35ms/step - loss: 0.0065 - val_loss: 0.0168\
Epoch 15/40\
88/88 [==============================] - 3s 35ms/step - loss: 0.0062 - val_loss: 0.0168\
Fold 3: 0.01680267 loss, 0.8002005 auc\
Epoch 1/40\
88/88 [==============================] - 4s 40ms/step - loss: 0.2989 - val_loss: 0.0332\
Epoch 2/40\
88/88 [==============================] - 3s 36ms/step - loss: 0.0254 - val_loss: 0.0218\
Epoch 3/40\
88/88 [==============================] - 3s 40ms/step - loss: 0.0197 - val_loss: 0.0192\
Epoch 4/40\
88/88 [==============================] - 3s 40ms/step - loss: 0.0179 - val_loss: 0.0184\
Epoch 5/40\
88/88 [==============================] - 3s 36ms/step - loss: 0.0166 - val_loss: 0.0179\
Epoch 6/40\
88/88 [==============================] - 3s 36ms/step - loss: 0.0154 - val_loss: 0.0176\
Epoch 7/40\
88/88 [==============================] - 3s 38ms/step - loss: 0.0141 - val_loss: 0.0175\
Epoch 8/40\
88/88 [==============================] - 3s 35ms/step - loss: 0.0127 - val_loss: 0.0172\
Epoch 9/40\
88/88 [==============================] - 3s 38ms/step - loss: 0.0113 - val_loss: 0.0172\
Epoch 10/40\
88/88 [==============================] - 3s 39ms/step - loss: 0.0113 - val_loss: 0.0415\
Epoch 11/40\
88/88 [==============================] - 3s 35ms/step - loss: 0.0158 - val_loss: 0.0192\
Epoch 12/40\
88/88 [==============================] - 3s 36ms/step - loss: 0.0115 - val_loss: 0.0177\
Fold 4: 0.017691564 loss, 0.8032421 auc\
Epoch 1/40\
88/88 [==============================] - 4s 41ms/step - loss: 0.2992 - val_loss: 0.0405\
Epoch 2/40\
88/88 [==============================] - 3s 39ms/step - loss: 0.0255 - val_loss: 0.0215\
Epoch 3/40\
88/88 [==============================] - 3s 38ms/step - loss: 0.0197 - val_loss: 0.0192\
Epoch 4/40\
88/88 [==============================] - 3s 36ms/step - loss: 0.0178 - val_loss: 0.0184\
Epoch 5/40\
88/88 [==============================] - 3s 36ms/step - loss: 0.0164 - val_loss: 0.0179\
Epoch 6/40\
88/88 [==============================] - 3s 36ms/step - loss: 0.0152 - val_loss: 0.0175\
Epoch 7/40\
88/88 [==============================] - 3s 36ms/step - loss: 0.0137 - val_loss: 0.0173\
Epoch 8/40\
88/88 [==============================] - 3s 36ms/step - loss: 0.0123 - val_loss: 0.0171\
Epoch 9/40\
88/88 [==============================] - 3s 36ms/step - loss: 0.0114 - val_loss: 0.0177\
Epoch 10/40\
88/88 [==============================] - 3s 35ms/step - loss: 0.0099 - val_loss: 0.0180\
Epoch 11/40\
88/88 [==============================] - 3s 36ms/step - loss: 0.0079 - val_loss: 0.0171\
Epoch 12/40\
88/88 [==============================] - 3s 36ms/step - loss: 0.0070 - val_loss: 0.0170\
Epoch 13/40\
88/88 [==============================] - 3s 37ms/step - loss: 0.0066 - val_loss: 0.0170\
Epoch 14/40\
88/88 [==============================] - 3s 36ms/step - loss: 0.0062 - val_loss: 0.0170\
Epoch 15/40\
88/88 [==============================] - 3s 36ms/step - loss: 0.0059 - val_loss: 0.0171\
Fold 5: 0.017067883 loss, 0.7896063 auc\
[0.016838478, 0.016826661, 0.01680267, 0.017691564, 0.017067883] [0.803925, 0.80750006, 0.8002005, 0.8032421, 0.7896063]\
\cf10 [I 2020-11-05 19:12:38,766]\cf2  Trial 55 finished with value: 0.01704545132815838 and parameters: \{'layers': 0, 'neurons': 2012\}. Best is trial 32 with value: 0.016830141469836236.\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/distributions.py:530: UserWarning: The distribution is specified by [512, 2048] and step=50, but the range is not divisible by `step`. It will be replaced by [512, 2012].\
  warnings.warn(\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
Epoch 1/40\
88/88 [==============================] - 11s 124ms/step - loss: 0.3168 - val_loss: 0.0453\
Epoch 2/40\
88/88 [==============================] - 10s 115ms/step - loss: 0.0295 - val_loss: 0.0233\
Epoch 3/40\
88/88 [==============================] - 10s 118ms/step - loss: 0.0211 - val_loss: 0.0200\
Epoch 4/40\
88/88 [==============================] - 10s 116ms/step - loss: 0.0188 - val_loss: 0.0192\
Epoch 5/40\
88/88 [==============================] - 9s 107ms/step - loss: 0.0172 - val_loss: 0.0179\
Epoch 6/40\
 6/88 [=>............................] - ETA: 6s - loss: 0.0157^CTraceback (most recent call last):\
  File "tuning.py", line 67, in <module>\
    param_tuning()\
  File "tuning.py", line 62, in param_tuning\
    study.optimize(tuning_objective, n_trials=150)\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/study.py", line 338, in optimize\
    self._optimize_sequential(\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/study.py", line 747, in _optimize_sequential\
    self._run_trial_and_callbacks(func, catch, callbacks, gc_after_trial)\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/study.py", line 776, in _run_trial_and_callbacks\
    trial = self._run_trial(func, catch, gc_after_trial)\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/study.py", line 799, in _run_trial\
    result = func(trial)\
  File "tuning.py", line 47, in tuning_objective\
    myModel.run_training(train_x, train_y, test_x, test_y)\
  File "../models/arch_base.py", line 47, in run_training\
    history = self.model.fit(\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py", line 108, in _method_wrapper\
    return method(self, *args, **kwargs)\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py", line 1098, in fit\
    tmp_logs = train_function(iterator)\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py", line 780, in __call__\
    result = self._call(*args, **kwds)\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py", line 807, in _call\
    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 2829, in __call__\
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 1843, in _filtered_call\
    return self._call_flat(\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 1923, in _call_flat\
    return self._build_call_outputs(self._inference_function.call(\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 545, in call\
    outputs = execute.execute(\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/tensorflow/python/eager/execute.py", line 59, in quick_execute\
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\
KeyboardInterrupt\
\
(ml) baconbaker@MacBook-Pro-2 experimentation % python tuning.py \
\cf10 [I 2020-11-05 19:17:09,555]\cf2  A new study created in memory with name: no-name-20f1ea11-53cc-46cd-8480-2aaa7ec9ad0f\
^CTraceback (most recent call last):\
  File "tuning.py", line 67, in <module>\
    param_tuning()\
  File "tuning.py", line 62, in param_tuning\
    study.optimize(tuning_objective, n_trials=150)\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/study.py", line 338, in optimize\
    self._optimize_sequential(\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/study.py", line 747, in _optimize_sequential\
    self._run_trial_and_callbacks(func, catch, callbacks, gc_after_trial)\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/study.py", line 776, in _run_trial_and_callbacks\
    trial = self._run_trial(func, catch, gc_after_trial)\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/study.py", line 799, in _run_trial\
    result = func(trial)\
  File "tuning.py", line 35, in tuning_objective\
    df_x = pd.read_csv(params['feature_csv'])\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/pandas/io/parsers.py", line 686, in read_csv\
    return _read(filepath_or_buffer, kwds)\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/pandas/io/parsers.py", line 458, in _read\
    data = parser.read(nrows)\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/pandas/io/parsers.py", line 1196, in read\
    ret = self._engine.read(nrows)\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/pandas/io/parsers.py", line 2155, in read\
    data = self._reader.read(nrows)\
  File "pandas/_libs/parsers.pyx", line 847, in pandas._libs.parsers.TextReader.read\
  File "pandas/_libs/parsers.pyx", line 862, in pandas._libs.parsers.TextReader._read_low_memory\
  File "pandas/_libs/parsers.pyx", line 941, in pandas._libs.parsers.TextReader._read_rows\
  File "pandas/_libs/parsers.pyx", line 1073, in pandas._libs.parsers.TextReader._convert_column_data\
  File "pandas/_libs/parsers.pyx", line 1119, in pandas._libs.parsers.TextReader._convert_tokens\
  File "pandas/_libs/parsers.pyx", line 1162, in pandas._libs.parsers.TextReader._convert_with_dtype\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/pandas/core/dtypes/common.py", line 530, in is_categorical_dtype\
    def is_categorical_dtype(arr_or_dtype) -> bool:\
KeyboardInterrupt\
\
(ml) baconbaker@MacBook-Pro-2 experimentation % python tuning.py\
\cf10 [I 2020-11-05 19:17:26,457]\cf2  A new study created in memory with name: no-name-9ff83409-eaac-437a-89a7-53dc9a07ccbb\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=None as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
5\
2020-11-05 19:17:41.560267: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\
2020-11-05 19:17:41.584694: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7facce81eea0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\
2020-11-05 19:17:41.584716: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\
Epoch 1/40\
88/88 [==============================] - 5s 54ms/step - loss: 0.3867 - val_loss: 0.0880\
Epoch 2/40\
88/88 [==============================] - 3s 36ms/step - loss: 0.0427 - val_loss: 0.0298\
Epoch 3/40\
88/88 [==============================] - 3s 36ms/step - loss: 0.0251 - val_loss: 0.0226\
Epoch 4/40\
88/88 [==============================] - 3s 39ms/step - loss: 0.0208 - val_loss: 0.0201\
Epoch 5/40\
88/88 [==============================] - 4s 40ms/step - loss: 0.0187 - val_loss: 0.0191\
Epoch 6/40\
88/88 [==============================] - 3s 39ms/step - loss: 0.0172 - val_loss: 0.0185\
Epoch 7/40\
88/88 [==============================] - 3s 40ms/step - loss: 0.0157 - val_loss: 0.0181\
Epoch 8/40\
88/88 [==============================] - 4s 42ms/step - loss: 0.0143 - val_loss: 0.0180\
Epoch 9/40\
88/88 [==============================] - 4s 43ms/step - loss: 0.0126 - val_loss: 0.0183\
Epoch 10/40\
88/88 [==============================] - 4s 44ms/step - loss: 0.0102 - val_loss: 0.0180\
Epoch 11/40\
88/88 [==============================] - 4s 47ms/step - loss: 0.0092 - val_loss: 0.0181\
Epoch 12/40\
88/88 [==============================] - 4s 49ms/step - loss: 0.0088 - val_loss: 0.0181\
Epoch 13/40\
88/88 [==============================] - 4s 45ms/step - loss: 0.0087 - val_loss: 0.0181\
Fold 1: 0.018096147 loss, 0.77101225 auc\
Epoch 1/40\
88/88 [==============================] - 6s 67ms/step - loss: 0.3883 - val_loss: 0.0773\
Epoch 2/40\
88/88 [==============================] - 4s 49ms/step - loss: 0.0430 - val_loss: 0.0298\
Epoch 3/40\
88/88 [==============================] - 4s 50ms/step - loss: 0.0254 - val_loss: 0.0232\
Epoch 4/40\
88/88 [==============================] - 4s 49ms/step - loss: 0.0209 - val_loss: 0.0203\
Epoch 5/40\
88/88 [==============================] - 4s 46ms/step - loss: 0.0188 - val_loss: 0.0188\
Epoch 6/40\
88/88 [==============================] - 4s 48ms/step - loss: 0.0173 - val_loss: 0.0183\
Epoch 7/40\
88/88 [==============================] - 4s 47ms/step - loss: 0.0160 - val_loss: 0.0180\
Epoch 8/40\
88/88 [==============================] - 4s 51ms/step - loss: 0.0145 - val_loss: 0.0180\
Epoch 9/40\
88/88 [==============================] - 4s 48ms/step - loss: 0.0128 - val_loss: 0.0182\
Epoch 10/40\
88/88 [==============================] - 4s 51ms/step - loss: 0.0105 - val_loss: 0.0180\
Epoch 11/40\
88/88 [==============================] - 4s 46ms/step - loss: 0.0095 - val_loss: 0.0181\
Fold 2: 0.018095618 loss, 0.7754745 auc\
Epoch 1/40\
88/88 [==============================] - 6s 64ms/step - loss: 0.3894 - val_loss: 0.0776\
Epoch 2/40\
88/88 [==============================] - 4s 44ms/step - loss: 0.0432 - val_loss: 0.0297\
Epoch 3/40\
88/88 [==============================] - 4s 47ms/step - loss: 0.0253 - val_loss: 0.0226\
Epoch 4/40\
88/88 [==============================] - 4s 45ms/step - loss: 0.0209 - val_loss: 0.0199\
Epoch 5/40\
88/88 [==============================] - 4s 46ms/step - loss: 0.0188 - val_loss: 0.0187\
Epoch 6/40\
88/88 [==============================] - 4s 51ms/step - loss: 0.0172 - val_loss: 0.0180\
Epoch 7/40\
88/88 [==============================] - 4s 48ms/step - loss: 0.0159 - val_loss: 0.0178\
Epoch 8/40\
43/88 [=============>................] - ETA: 2s - loss: 0.0145^CTraceback (most recent call last):\
  File "tuning.py", line 67, in <module>\
    param_tuning()\
  File "tuning.py", line 62, in param_tuning\
    study.optimize(tuning_objective, n_trials=50)\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/study.py", line 338, in optimize\
    self._optimize_sequential(\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/study.py", line 747, in _optimize_sequential\
    self._run_trial_and_callbacks(func, catch, callbacks, gc_after_trial)\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/study.py", line 776, in _run_trial_and_callbacks\
    trial = self._run_trial(func, catch, gc_after_trial)\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/study.py", line 799, in _run_trial\
    result = func(trial)\
  File "tuning.py", line 47, in tuning_objective\
    myModel.run_training(train_x, train_y, test_x, test_y)\
  File "../models/arch_base.py", line 47, in run_training\
    history = self.model.fit(\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py", line 108, in _method_wrapper\
    return method(self, *args, **kwargs)\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py", line 1098, in fit\
    tmp_logs = train_function(iterator)\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py", line 780, in __call__\
    result = self._call(*args, **kwds)\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py", line 807, in _call\
    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 2829, in __call__\
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 1843, in _filtered_call\
    return self._call_flat(\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 1923, in _call_flat\
    return self._build_call_outputs(self._inference_function.call(\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 545, in call\
    outputs = execute.execute(\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/tensorflow/python/eager/execute.py", line 59, in quick_execute\
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\
KeyboardInterrupt\
\
(ml) baconbaker@MacBook-Pro-2 experimentation % python tuning.py\
\cf10 [I 2020-11-05 19:20:28,461]\cf2  A new study created in memory with name: no-name-94d71526-282f-4232-8b5c-27c3e16be67a\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=1 as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/model_selection/_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\
  warnings.warn(\
5\
2020-11-05 19:20:42.156398: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA\
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\
2020-11-05 19:20:42.169475: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7ff6447130a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\
2020-11-05 19:20:42.169493: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\
Epoch 1/40\
88/88 [==============================] - 5s 62ms/step - loss: 0.3350 - val_loss: 0.0474\
Epoch 2/40\
88/88 [==============================] - 6s 64ms/step - loss: 0.0316 - val_loss: 0.0240\
Epoch 3/40\
88/88 [==============================] - 6s 73ms/step - loss: 0.0215 - val_loss: 0.0208\
Epoch 4/40\
88/88 [==============================] - 7s 80ms/step - loss: 0.0187 - val_loss: 0.0189\
Epoch 5/40\
88/88 [==============================] - 6s 74ms/step - loss: 0.0169 - val_loss: 0.0181\
Epoch 6/40\
88/88 [==============================] - 7s 77ms/step - loss: 0.0154 - val_loss: 0.0177\
Epoch 7/40\
88/88 [==============================] - 7s 75ms/step - loss: 0.0138 - val_loss: 0.0176\
Epoch 8/40\
88/88 [==============================] - 7s 76ms/step - loss: 0.0119 - val_loss: 0.0180\
Epoch 9/40\
88/88 [==============================] - 6s 70ms/step - loss: 0.0097 - val_loss: 0.0178\
Epoch 10/40\
88/88 [==============================] - 7s 75ms/step - loss: 0.0070 - val_loss: 0.0176\
Epoch 11/40\
88/88 [==============================] - 7s 74ms/step - loss: 0.0061 - val_loss: 0.0176\
Epoch 12/40\
88/88 [==============================] - 6s 70ms/step - loss: 0.0057 - val_loss: 0.0177\
Epoch 13/40\
88/88 [==============================] - 5s 60ms/step - loss: 0.0056 - val_loss: 0.0177\
Fold 1: 0.01768635 loss, 0.7687716 auc\
Epoch 1/40\
88/88 [==============================] - 8s 92ms/step - loss: 0.3367 - val_loss: 0.0477\
Epoch 2/40\
88/88 [==============================] - 6s 74ms/step - loss: 0.0320 - val_loss: 0.0240\
Epoch 3/40\
88/88 [==============================] - 7s 75ms/step - loss: 0.0217 - val_loss: 0.0205\
Epoch 4/40\
88/88 [==============================] - 7s 85ms/step - loss: 0.0188 - val_loss: 0.0190\
Epoch 5/40\
88/88 [==============================] - 8s 87ms/step - loss: 0.0171 - val_loss: 0.0180\
Epoch 6/40\
88/88 [==============================] - 7s 80ms/step - loss: 0.0155 - val_loss: 0.0175\
Epoch 7/40\
88/88 [==============================] - 7s 80ms/step - loss: 0.0141 - val_loss: 0.0175\
Epoch 8/40\
88/88 [==============================] - 7s 81ms/step - loss: 0.0126 - val_loss: 0.0174\
Epoch 9/40\
88/88 [==============================] - 6s 71ms/step - loss: 0.0097 - val_loss: 0.0171\
Epoch 10/40\
88/88 [==============================] - 6s 65ms/step - loss: 0.0089 - val_loss: 0.0171\
Epoch 11/40\
88/88 [==============================] - 5s 56ms/step - loss: 0.0083 - val_loss: 0.0172\
Epoch 12/40\
88/88 [==============================] - 6s 69ms/step - loss: 0.0078 - val_loss: 0.0172\
Epoch 13/40\
88/88 [==============================] - 6s 70ms/step - loss: 0.0078 - val_loss: 0.0172\
Fold 2: 0.017174006 loss, 0.79475504 auc\
Epoch 1/40\
88/88 [==============================] - 9s 99ms/step - loss: 0.3358 - val_loss: 0.0502\
Epoch 2/40\
88/88 [==============================] - 7s 82ms/step - loss: 0.0317 - val_loss: 0.0243\
Epoch 3/40\
88/88 [==============================] - 7s 82ms/step - loss: 0.0217 - val_loss: 0.0202\
Epoch 4/40\
88/88 [==============================] - 7s 85ms/step - loss: 0.0188 - val_loss: 0.0190\
Epoch 5/40\
88/88 [==============================] - 7s 82ms/step - loss: 0.0172 - val_loss: 0.0178\
Epoch 6/40\
88/88 [==============================] - 7s 82ms/step - loss: 0.0156 - val_loss: 0.0177\
Epoch 7/40\
88/88 [==============================] - 7s 81ms/step - loss: 0.0141 - val_loss: 0.0183\
Epoch 8/40\
88/88 [==============================] - 6s 68ms/step - loss: 0.0125 - val_loss: 0.0173\
Epoch 9/40\
88/88 [==============================] - 6s 70ms/step - loss: 0.0105 - val_loss: 0.0250\
Epoch 10/40\
88/88 [==============================] - 7s 75ms/step - loss: 0.0094 - val_loss: 0.0179\
Epoch 11/40\
88/88 [==============================] - 7s 80ms/step - loss: 0.0062 - val_loss: 0.0175\
Fold 3: 0.0175374 loss, 0.7699236 auc\
Epoch 1/40\
88/88 [==============================] - 6s 63ms/step - loss: 0.3370 - val_loss: 0.0493\
Epoch 2/40\
88/88 [==============================] - 5s 52ms/step - loss: 0.0317 - val_loss: 0.0241\
Epoch 3/40\
88/88 [==============================] - 5s 54ms/step - loss: 0.0216 - val_loss: 0.0206\
Epoch 4/40\
88/88 [==============================] - 4s 50ms/step - loss: 0.0187 - val_loss: 0.0191\
Epoch 5/40\
88/88 [==============================] - 4s 51ms/step - loss: 0.0169 - val_loss: 0.0181\
Epoch 6/40\
88/88 [==============================] - 6s 66ms/step - loss: 0.0155 - val_loss: 0.0178\
Epoch 7/40\
88/88 [==============================] - 7s 82ms/step - loss: 0.0137 - val_loss: 0.0177\
Epoch 8/40\
88/88 [==============================] - 7s 85ms/step - loss: 0.0118 - val_loss: 0.0179\
Epoch 9/40\
88/88 [==============================] - 7s 85ms/step - loss: 0.0090 - val_loss: 0.0174\
Epoch 10/40\
88/88 [==============================] - 8s 86ms/step - loss: 0.0081 - val_loss: 0.0174\
Epoch 11/40\
88/88 [==============================] - 8s 88ms/step - loss: 0.0075 - val_loss: 0.0175\
Epoch 12/40\
88/88 [==============================] - 8s 91ms/step - loss: 0.0070 - val_loss: 0.0175\
Fold 4: 0.017470194 loss, 0.7825475 auc\
Epoch 1/40\
88/88 [==============================] - 7s 76ms/step - loss: 0.3356 - val_loss: 0.0470\
Epoch 2/40\
88/88 [==============================] - 5s 58ms/step - loss: 0.0322 - val_loss: 0.0246\
Epoch 3/40\
88/88 [==============================] - 6s 67ms/step - loss: 0.0218 - val_loss: 0.0208\
Epoch 4/40\
88/88 [==============================] - 6s 72ms/step - loss: 0.0189 - val_loss: 0.0189\
Epoch 5/40\
88/88 [==============================] - 7s 80ms/step - loss: 0.0171 - val_loss: 0.0180\
Epoch 6/40\
88/88 [==============================] - 7s 75ms/step - loss: 0.0155 - val_loss: 0.0176\
Epoch 7/40\
88/88 [==============================] - 7s 76ms/step - loss: 0.0139 - val_loss: 0.0176\
Epoch 8/40\
88/88 [==============================] - 6s 70ms/step - loss: 0.0124 - val_loss: 0.0177\
Epoch 9/40\
88/88 [==============================] - 6s 72ms/step - loss: 0.0095 - val_loss: 0.0172\
Epoch 10/40\
88/88 [==============================] - 7s 77ms/step - loss: 0.0086 - val_loss: 0.0173\
Epoch 11/40\
88/88 [==============================] - 7s 76ms/step - loss: 0.0080 - val_loss: 0.0174\
Epoch 12/40\
88/88 [==============================] - 7s 74ms/step - loss: 0.0075 - val_loss: 0.0174\
Fold 5: 0.017379181 loss, 0.7818948 auc\
[0.01768635, 0.017174006, 0.0175374, 0.017470194, 0.017379181] [0.7687716, 0.79475504, 0.7699236, 0.7825475, 0.7818948]\
\cf10 [I 2020-11-05 19:28:19,933]\cf2  Trial 0 finished with value: 0.017449426278471947 and parameters: \{'layers': 1, 'neurons': 1024\}. Best is trial 0 with value: 0.017449426278471947.\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=1 as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/model_selection/_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\
  warnings.warn(\
5\
Epoch 1/40\
88/88 [==============================] - 9s 105ms/step - loss: 0.3715 - val_loss: 0.0706\
Epoch 2/40\
88/88 [==============================] - 7s 79ms/step - loss: 0.0387 - val_loss: 0.0274\
Epoch 3/40\
88/88 [==============================] - 7s 80ms/step - loss: 0.0238 - val_loss: 0.0220\
Epoch 4/40\
88/88 [==============================] - 7s 82ms/step - loss: 0.0200 - val_loss: 0.0196\
Epoch 5/40\
88/88 [==============================] - 7s 77ms/step - loss: 0.0180 - val_loss: 0.0186\
Epoch 6/40\
88/88 [==============================] - 6s 69ms/step - loss: 0.0166 - val_loss: 0.0183\
Epoch 7/40\
88/88 [==============================] - 6s 66ms/step - loss: 0.0152 - val_loss: 0.0179\
Epoch 8/40\
88/88 [==============================] - 7s 74ms/step - loss: 0.0137 - val_loss: 0.0184\
Epoch 9/40\
88/88 [==============================] - 7s 84ms/step - loss: 0.0121 - val_loss: 0.0185\
Epoch 10/40\
88/88 [==============================] - 8s 88ms/step - loss: 0.0097 - val_loss: 0.0182\
Fold 1: 0.01817612 loss, 0.7631101 auc\
Epoch 1/40\
88/88 [==============================] - 9s 105ms/step - loss: 0.3683 - val_loss: 0.0676\
Epoch 2/40\
88/88 [==============================] - 7s 83ms/step - loss: 0.0389 - val_loss: 0.0273\
Epoch 3/40\
88/88 [==============================] - 6s 74ms/step - loss: 0.0241 - val_loss: 0.0219\
Epoch 4/40\
88/88 [==============================] - 7s 74ms/step - loss: 0.0203 - val_loss: 0.0197\
Epoch 5/40\
88/88 [==============================] - 8s 86ms/step - loss: 0.0183 - val_loss: 0.0183\
Epoch 6/40\
88/88 [==============================] - 7s 85ms/step - loss: 0.0168 - val_loss: 0.0181\
Epoch 7/40\
88/88 [==============================] - 7s 82ms/step - loss: 0.0154 - val_loss: 0.0179\
Epoch 8/40\
88/88 [==============================] - 8s 90ms/step - loss: 0.0138 - val_loss: 0.0181\
Epoch 9/40\
88/88 [==============================] - 8s 89ms/step - loss: 0.0121 - val_loss: 0.0185\
Epoch 10/40\
88/88 [==============================] - 8s 95ms/step - loss: 0.0097 - val_loss: 0.0180\
Fold 2: 0.018008972 loss, 0.76580137 auc\
Epoch 1/40\
88/88 [==============================] - 7s 79ms/step - loss: 0.3691 - val_loss: 0.0647\
Epoch 2/40\
88/88 [==============================] - 7s 80ms/step - loss: 0.0389 - val_loss: 0.0279\
Epoch 3/40\
88/88 [==============================] - 8s 94ms/step - loss: 0.0239 - val_loss: 0.0216\
Epoch 4/40\
88/88 [==============================] - 9s 97ms/step - loss: 0.0202 - val_loss: 0.0195\
Epoch 5/40\
88/88 [==============================] - 9s 98ms/step - loss: 0.0183 - val_loss: 0.0184\
Epoch 6/40\
88/88 [==============================] - 8s 88ms/step - loss: 0.0169 - val_loss: 0.0179\
Epoch 7/40\
88/88 [==============================] - 8s 91ms/step - loss: 0.0154 - val_loss: 0.0178\
Epoch 8/40\
88/88 [==============================] - 8s 94ms/step - loss: 0.0139 - val_loss: 0.0178\
Epoch 9/40\
88/88 [==============================] - 7s 85ms/step - loss: 0.0123 - val_loss: 0.0183\
Epoch 10/40\
88/88 [==============================] - 6s 71ms/step - loss: 0.0098 - val_loss: 0.0179\
Fold 3: 0.017880665 loss, 0.7679994 auc\
Epoch 1/40\
88/88 [==============================] - 10s 119ms/step - loss: 0.3697 - val_loss: 0.0697\
Epoch 2/40\
88/88 [==============================] - 9s 97ms/step - loss: 0.0391 - val_loss: 0.0285\
Epoch 3/40\
88/88 [==============================] - 8s 94ms/step - loss: 0.0239 - val_loss: 0.0222\
Epoch 4/40\
88/88 [==============================] - 9s 98ms/step - loss: 0.0201 - val_loss: 0.0195\
Epoch 5/40\
88/88 [==============================] - 8s 94ms/step - loss: 0.0181 - val_loss: 0.0190\
Epoch 6/40\
88/88 [==============================] - 7s 82ms/step - loss: 0.0167 - val_loss: 0.0183\
Epoch 7/40\
88/88 [==============================] - 8s 97ms/step - loss: 0.0152 - val_loss: 0.0189\
Epoch 8/40\
88/88 [==============================] - 8s 96ms/step - loss: 0.0137 - val_loss: 0.0181\
Epoch 9/40\
88/88 [==============================] - 9s 98ms/step - loss: 0.0120 - val_loss: 0.0185\
Epoch 10/40\
88/88 [==============================] - 9s 102ms/step - loss: 0.0103 - val_loss: 0.0188\
Epoch 11/40\
88/88 [==============================] - 9s 101ms/step - loss: 0.0078 - val_loss: 0.0185\
Fold 4: 0.018491626 loss, 0.7546302 auc\
Epoch 1/40\
88/88 [==============================] - 9s 106ms/step - loss: 0.3691 - val_loss: 0.0676\
Epoch 2/40\
88/88 [==============================] - 8s 93ms/step - loss: 0.0390 - val_loss: 0.0279\
Epoch 3/40\
88/88 [==============================] - 8s 92ms/step - loss: 0.0241 - val_loss: 0.0220\
Epoch 4/40\
88/88 [==============================] - 9s 103ms/step - loss: 0.0203 - val_loss: 0.0197\
Epoch 5/40\
88/88 [==============================] - 9s 103ms/step - loss: 0.0183 - val_loss: 0.0185\
Epoch 6/40\
88/88 [==============================] - 9s 108ms/step - loss: 0.0169 - val_loss: 0.0179\
Epoch 7/40\
88/88 [==============================] - 9s 99ms/step - loss: 0.0156 - val_loss: 0.0177\
Epoch 8/40\
88/88 [==============================] - 7s 74ms/step - loss: 0.0141 - val_loss: 0.0180\
Epoch 9/40\
88/88 [==============================] - 8s 88ms/step - loss: 0.0124 - val_loss: 0.0181\
Epoch 10/40\
88/88 [==============================] - 9s 104ms/step - loss: 0.0099 - val_loss: 0.0180\
Fold 5: 0.01798055 loss, 0.7618818 auc\
[0.01817612, 0.018008972, 0.017880665, 0.018491626, 0.01798055] [0.7631101, 0.76580137, 0.7679994, 0.7546302, 0.7618818]\
\cf10 [I 2020-11-05 19:36:36,061]\cf2  Trial 1 finished with value: 0.01810758635401726 and parameters: \{'layers': 4, 'neurons': 640\}. Best is trial 0 with value: 0.017449426278471947.\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=1 as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/model_selection/_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\
  warnings.warn(\
5\
Epoch 1/40\
88/88 [==============================] - 11s 125ms/step - loss: 0.3311 - val_loss: 0.0585\
Epoch 2/40\
88/88 [==============================] - 12s 137ms/step - loss: 0.0321 - val_loss: 0.0247\
Epoch 3/40\
88/88 [==============================] - 13s 147ms/step - loss: 0.0219 - val_loss: 0.0209\
Epoch 4/40\
88/88 [==============================] - 13s 145ms/step - loss: 0.0190 - val_loss: 0.0191\
Epoch 5/40\
88/88 [==============================] - 12s 136ms/step - loss: 0.0172 - val_loss: 0.0183\
Epoch 6/40\
88/88 [==============================] - 10s 111ms/step - loss: 0.0157 - val_loss: 0.0182\
Epoch 7/40\
88/88 [==============================] - 11s 124ms/step - loss: 0.0143 - val_loss: 0.0181\
Epoch 8/40\
88/88 [==============================] - 12s 136ms/step - loss: 0.0125 - val_loss: 0.0186\
Epoch 9/40\
88/88 [==============================] - 12s 135ms/step - loss: 0.0097 - val_loss: 0.0180\
Epoch 10/40\
88/88 [==============================] - 12s 135ms/step - loss: 0.0083 - val_loss: 0.0181\
Epoch 11/40\
88/88 [==============================] - 12s 134ms/step - loss: 0.0075 - val_loss: 0.0183\
Epoch 12/40\
88/88 [==============================] - 11s 126ms/step - loss: 0.0068 - val_loss: 0.0182\
Fold 1: 0.018230172 loss, 0.75742984 auc\
Epoch 1/40\
88/88 [==============================] - 13s 152ms/step - loss: 0.3329 - val_loss: 0.0482\
Epoch 2/40\
88/88 [==============================] - 11s 130ms/step - loss: 0.0319 - val_loss: 0.0247\
Epoch 3/40\
88/88 [==============================] - 12s 132ms/step - loss: 0.0218 - val_loss: 0.0206\
Epoch 4/40\
88/88 [==============================] - 11s 127ms/step - loss: 0.0190 - val_loss: 0.0187\
Epoch 5/40\
88/88 [==============================] - 10s 114ms/step - loss: 0.0173 - val_loss: 0.0178\
Epoch 6/40\
88/88 [==============================] - 11s 126ms/step - loss: 0.0159 - val_loss: 0.0177\
Epoch 7/40\
88/88 [==============================] - 12s 137ms/step - loss: 0.0144 - val_loss: 0.0178\
Epoch 8/40\
88/88 [==============================] - 12s 140ms/step - loss: 0.0120 - val_loss: 0.0175\
Epoch 9/40\
88/88 [==============================] - 12s 132ms/step - loss: 0.0107 - val_loss: 0.0176\
Epoch 10/40\
88/88 [==============================] - 12s 136ms/step - loss: 0.0098 - val_loss: 0.0177\
Epoch 11/40\
88/88 [==============================] - 10s 111ms/step - loss: 0.0090 - val_loss: 0.0177\
Fold 2: 0.01770937 loss, 0.784915 auc\
Epoch 1/40\
88/88 [==============================] - 13s 146ms/step - loss: 0.3336 - val_loss: 0.0609\
Epoch 2/40\
88/88 [==============================] - 11s 121ms/step - loss: 0.0320 - val_loss: 0.0249\
Epoch 3/40\
88/88 [==============================] - 11s 121ms/step - loss: 0.0218 - val_loss: 0.0203\
Epoch 4/40\
88/88 [==============================] - 11s 123ms/step - loss: 0.0190 - val_loss: 0.0188\
Epoch 5/40\
88/88 [==============================] - 10s 108ms/step - loss: 0.0173 - val_loss: 0.0180\
Epoch 6/40\
88/88 [==============================] - 7s 82ms/step - loss: 0.0158 - val_loss: 0.0177\
Epoch 7/40\
88/88 [==============================] - 7s 82ms/step - loss: 0.0142 - val_loss: 0.0179\
Epoch 8/40\
88/88 [==============================] - 7s 80ms/step - loss: 0.0124 - val_loss: 0.0182\
Epoch 9/40\
88/88 [==============================] - 7s 78ms/step - loss: 0.0096 - val_loss: 0.0179\
Fold 3: 0.017852603 loss, 0.7668486 auc\
Epoch 1/40\
88/88 [==============================] - 8s 90ms/step - loss: 0.3333 - val_loss: 0.0556\
Epoch 2/40\
88/88 [==============================] - 7s 85ms/step - loss: 0.0321 - val_loss: 0.0249\
Epoch 3/40\
88/88 [==============================] - 7s 83ms/step - loss: 0.0218 - val_loss: 0.0206\
Epoch 4/40\
88/88 [==============================] - 7s 83ms/step - loss: 0.0191 - val_loss: 0.0188\
Epoch 5/40\
88/88 [==============================] - 7s 83ms/step - loss: 0.0172 - val_loss: 0.0186\
Epoch 6/40\
88/88 [==============================] - 7s 85ms/step - loss: 0.0158 - val_loss: 0.0180\
Epoch 7/40\
88/88 [==============================] - 7s 84ms/step - loss: 0.0143 - val_loss: 0.0183\
Epoch 8/40\
88/88 [==============================] - 8s 88ms/step - loss: 0.0125 - val_loss: 0.0183\
Epoch 9/40\
88/88 [==============================] - 8s 86ms/step - loss: 0.0096 - val_loss: 0.0181\
Fold 4: 0.018125685 loss, 0.75933766 auc\
Epoch 1/40\
88/88 [==============================] - 8s 92ms/step - loss: 0.3340 - val_loss: 0.0640\
Epoch 2/40\
88/88 [==============================] - 7s 80ms/step - loss: 0.0326 - val_loss: 0.0257\
Epoch 3/40\
88/88 [==============================] - 7s 77ms/step - loss: 0.0220 - val_loss: 0.0204\
Epoch 4/40\
88/88 [==============================] - 7s 81ms/step - loss: 0.0192 - val_loss: 0.0190\
Epoch 5/40\
88/88 [==============================] - 7s 83ms/step - loss: 0.0175 - val_loss: 0.0179\
Epoch 6/40\
88/88 [==============================] - 7s 85ms/step - loss: 0.0162 - val_loss: 0.0178\
Epoch 7/40\
88/88 [==============================] - 7s 80ms/step - loss: 0.0148 - val_loss: 0.0177\
Epoch 8/40\
88/88 [==============================] - 7s 84ms/step - loss: 0.0130 - val_loss: 0.0182\
Epoch 9/40\
88/88 [==============================] - 7s 80ms/step - loss: 0.0113 - val_loss: 0.0187\
Epoch 10/40\
88/88 [==============================] - 7s 79ms/step - loss: 0.0085 - val_loss: 0.0182\
Fold 5: 0.018157946 loss, 0.75306815 auc\
[0.018230172, 0.01770937, 0.017852603, 0.018125685, 0.018157946] [0.75742984, 0.784915, 0.7668486, 0.75933766, 0.75306815]\
\cf10 [I 2020-11-05 19:46:08,893]\cf2  Trial 2 finished with value: 0.018015155196189882 and parameters: \{'layers': 4, 'neurons': 1024\}. Best is trial 0 with value: 0.017449426278471947.\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=1 as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/model_selection/_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\
  warnings.warn(\
5\
Epoch 1/40\
88/88 [==============================] - 4s 43ms/step - loss: 0.3680 - val_loss: 0.0693\
Epoch 2/40\
88/88 [==============================] - 3s 33ms/step - loss: 0.0390 - val_loss: 0.0290\
Epoch 3/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0240 - val_loss: 0.0221\
Epoch 4/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.0202 - val_loss: 0.0197\
Epoch 5/40\
88/88 [==============================] - 3s 36ms/step - loss: 0.0182 - val_loss: 0.0186\
Epoch 6/40\
88/88 [==============================] - 3s 36ms/step - loss: 0.0167 - val_loss: 0.0181\
Epoch 7/40\
88/88 [==============================] - 3s 37ms/step - loss: 0.0154 - val_loss: 0.0180\
Epoch 8/40\
88/88 [==============================] - 3s 33ms/step - loss: 0.0140 - val_loss: 0.0183\
Epoch 9/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.0123 - val_loss: 0.0184\
Epoch 10/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0099 - val_loss: 0.0181\
Fold 1: 0.018131414 loss, 0.7604493 auc\
Epoch 1/40\
88/88 [==============================] - 4s 41ms/step - loss: 0.3697 - val_loss: 0.0660\
Epoch 2/40\
88/88 [==============================] - 3s 36ms/step - loss: 0.0390 - val_loss: 0.0275\
Epoch 3/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.0241 - val_loss: 0.0221\
Epoch 4/40\
88/88 [==============================] - 3s 34ms/step - loss: 0.0203 - val_loss: 0.0197\
Epoch 5/40\
88/88 [==============================] - 3s 33ms/step - loss: 0.0183 - val_loss: 0.0184\
Epoch 6/40\
88/88 [==============================] - 3s 37ms/step - loss: 0.0168 - val_loss: 0.0183\
Epoch 7/40\
88/88 [==============================] - 4s 41ms/step - loss: 0.0155 - val_loss: 0.0183\
Epoch 8/40\
88/88 [==============================] - 3s 34ms/step - loss: 0.0140 - val_loss: 0.0180\
Epoch 9/40\
88/88 [==============================] - 3s 36ms/step - loss: 0.0124 - val_loss: 0.0182\
Epoch 10/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.0105 - val_loss: 0.0187\
Epoch 11/40\
88/88 [==============================] - 3s 33ms/step - loss: 0.0082 - val_loss: 0.0182\
Fold 2: 0.018183993 loss, 0.75616205 auc\
Epoch 1/40\
88/88 [==============================] - 3s 40ms/step - loss: 0.3679 - val_loss: 0.0773\
Epoch 2/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.0389 - val_loss: 0.0277\
Epoch 3/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0240 - val_loss: 0.0218\
Epoch 4/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0203 - val_loss: 0.0195\
Epoch 5/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0183 - val_loss: 0.0184\
Epoch 6/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.0169 - val_loss: 0.0181\
Epoch 7/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0155 - val_loss: 0.0179\
Epoch 8/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0139 - val_loss: 0.0179\
Epoch 9/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.0124 - val_loss: 0.0183\
Epoch 10/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0099 - val_loss: 0.0180\
Fold 3: 0.017955532 loss, 0.76939553 auc\
Epoch 1/40\
88/88 [==============================] - 3s 37ms/step - loss: 0.3700 - val_loss: 0.0662\
Epoch 2/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0388 - val_loss: 0.0285\
Epoch 3/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0240 - val_loss: 0.0225\
Epoch 4/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0202 - val_loss: 0.0197\
Epoch 5/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.0182 - val_loss: 0.0190\
Epoch 6/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.0168 - val_loss: 0.0182\
Epoch 7/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.0153 - val_loss: 0.0194\
Epoch 8/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.0138 - val_loss: 0.0182\
Epoch 9/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0115 - val_loss: 0.0180\
Epoch 10/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0105 - val_loss: 0.0181\
Epoch 11/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.0097 - val_loss: 0.0182\
Epoch 12/40\
88/88 [==============================] - 3s 30ms/step - loss: 0.0091 - val_loss: 0.0182\
Fold 4: 0.018152233 loss, 0.77772236 auc\
Epoch 1/40\
88/88 [==============================] - 3s 39ms/step - loss: 0.3700 - val_loss: 0.0698\
Epoch 2/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.0393 - val_loss: 0.0275\
Epoch 3/40\
88/88 [==============================] - 3s 33ms/step - loss: 0.0242 - val_loss: 0.0221\
Epoch 4/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.0203 - val_loss: 0.0197\
Epoch 5/40\
88/88 [==============================] - 3s 34ms/step - loss: 0.0183 - val_loss: 0.0185\
Epoch 6/40\
88/88 [==============================] - 3s 35ms/step - loss: 0.0168 - val_loss: 0.0180\
Epoch 7/40\
88/88 [==============================] - 3s 33ms/step - loss: 0.0154 - val_loss: 0.0180\
Epoch 8/40\
88/88 [==============================] - 3s 36ms/step - loss: 0.0139 - val_loss: 0.0180\
Epoch 9/40\
88/88 [==============================] - 3s 34ms/step - loss: 0.0116 - val_loss: 0.0179\
Epoch 10/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.0105 - val_loss: 0.0179\
Epoch 11/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.0098 - val_loss: 0.0180\
Epoch 12/40\
88/88 [==============================] - 3s 32ms/step - loss: 0.0092 - val_loss: 0.0180\
Fold 5: 0.017996132 loss, 0.76519734 auc\
[0.018131414, 0.018183993, 0.017955532, 0.018152233, 0.017996132] [0.7604493, 0.75616205, 0.76939553, 0.77772236, 0.76519734]\
\cf10 [I 2020-11-05 19:49:23,710]\cf2  Trial 3 finished with value: 0.018083861097693443 and parameters: \{'layers': 4, 'neurons': 640\}. Best is trial 0 with value: 0.017449426278471947.\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=1 as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/model_selection/_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\
  warnings.warn(\
5\
Epoch 1/40\
88/88 [==============================] - 6s 69ms/step - loss: 0.3339 - val_loss: 0.0572\
Epoch 2/40\
88/88 [==============================] - 5s 61ms/step - loss: 0.0320 - val_loss: 0.0247\
Epoch 3/40\
88/88 [==============================] - 5s 60ms/step - loss: 0.0217 - val_loss: 0.0208\
Epoch 4/40\
88/88 [==============================] - 5s 61ms/step - loss: 0.0188 - val_loss: 0.0186\
Epoch 5/40\
88/88 [==============================] - 5s 59ms/step - loss: 0.0170 - val_loss: 0.0181\
Epoch 6/40\
88/88 [==============================] - 5s 59ms/step - loss: 0.0153 - val_loss: 0.0180\
Epoch 7/40\
88/88 [==============================] - 5s 59ms/step - loss: 0.0135 - val_loss: 0.0179\
Epoch 8/40\
88/88 [==============================] - 5s 62ms/step - loss: 0.0114 - val_loss: 0.0187\
Epoch 9/40\
88/88 [==============================] - 5s 61ms/step - loss: 0.0084 - val_loss: 0.0181\
Epoch 10/40\
88/88 [==============================] - 5s 60ms/step - loss: 0.0072 - val_loss: 0.0182\
Fold 1: 0.018220186 loss, 0.76376987 auc\
Epoch 1/40\
88/88 [==============================] - 6s 66ms/step - loss: 0.3343 - val_loss: 0.0511\
Epoch 2/40\
88/88 [==============================] - 5s 61ms/step - loss: 0.0318 - val_loss: 0.0244\
Epoch 3/40\
88/88 [==============================] - 5s 58ms/step - loss: 0.0217 - val_loss: 0.0202\
Epoch 4/40\
88/88 [==============================] - 5s 54ms/step - loss: 0.0189 - val_loss: 0.0189\
Epoch 5/40\
88/88 [==============================] - 5s 59ms/step - loss: 0.0172 - val_loss: 0.0178\
Epoch 6/40\
88/88 [==============================] - 5s 60ms/step - loss: 0.0156 - val_loss: 0.0177\
Epoch 7/40\
88/88 [==============================] - 5s 57ms/step - loss: 0.0138 - val_loss: 0.0184\
Epoch 8/40\
88/88 [==============================] - 5s 56ms/step - loss: 0.0117 - val_loss: 0.0182\
Epoch 9/40\
88/88 [==============================] - 5s 58ms/step - loss: 0.0087 - val_loss: 0.0178\
Fold 2: 0.017813373 loss, 0.7740028 auc\
Epoch 1/40\
88/88 [==============================] - 6s 65ms/step - loss: 0.3325 - val_loss: 0.0546\
Epoch 2/40\
88/88 [==============================] - 5s 58ms/step - loss: 0.0318 - val_loss: 0.0247\
Epoch 3/40\
88/88 [==============================] - 5s 60ms/step - loss: 0.0218 - val_loss: 0.0202\
Epoch 4/40\
88/88 [==============================] - 5s 60ms/step - loss: 0.0190 - val_loss: 0.0188\
Epoch 5/40\
88/88 [==============================] - 5s 60ms/step - loss: 0.0171 - val_loss: 0.0179\
Epoch 6/40\
88/88 [==============================] - 5s 59ms/step - loss: 0.0155 - val_loss: 0.0178\
Epoch 7/40\
88/88 [==============================] - 5s 60ms/step - loss: 0.0137 - val_loss: 0.0178\
Epoch 8/40\
88/88 [==============================] - 5s 62ms/step - loss: 0.0114 - val_loss: 0.0182\
Epoch 9/40\
88/88 [==============================] - 5s 59ms/step - loss: 0.0091 - val_loss: 0.0188\
Epoch 10/40\
88/88 [==============================] - 5s 61ms/step - loss: 0.0064 - val_loss: 0.0183\
Fold 3: 0.01827081 loss, 0.7470142 auc\
Epoch 1/40\
88/88 [==============================] - 6s 68ms/step - loss: 0.3339 - val_loss: 0.0512\
Epoch 2/40\
88/88 [==============================] - 5s 62ms/step - loss: 0.0321 - val_loss: 0.0249\
Epoch 3/40\
88/88 [==============================] - 5s 61ms/step - loss: 0.0218 - val_loss: 0.0209\
Epoch 4/40\
88/88 [==============================] - 5s 60ms/step - loss: 0.0189 - val_loss: 0.0188\
Epoch 5/40\
88/88 [==============================] - 5s 60ms/step - loss: 0.0171 - val_loss: 0.0185\
Epoch 6/40\
88/88 [==============================] - 5s 61ms/step - loss: 0.0156 - val_loss: 0.0179\
Epoch 7/40\
88/88 [==============================] - 5s 59ms/step - loss: 0.0139 - val_loss: 0.0184\
Epoch 8/40\
88/88 [==============================] - 5s 58ms/step - loss: 0.0118 - val_loss: 0.0184\
Epoch 9/40\
88/88 [==============================] - 5s 59ms/step - loss: 0.0089 - val_loss: 0.0180\
Fold 4: 0.018022282 loss, 0.7679673 auc\
Epoch 1/40\
88/88 [==============================] - 6s 69ms/step - loss: 0.3334 - val_loss: 0.0506\
Epoch 2/40\
88/88 [==============================] - 5s 62ms/step - loss: 0.0322 - val_loss: 0.0249\
Epoch 3/40\
88/88 [==============================] - 5s 61ms/step - loss: 0.0219 - val_loss: 0.0206\
Epoch 4/40\
88/88 [==============================] - 5s 62ms/step - loss: 0.0190 - val_loss: 0.0191\
Epoch 5/40\
88/88 [==============================] - 5s 61ms/step - loss: 0.0173 - val_loss: 0.0178\
Epoch 6/40\
88/88 [==============================] - 5s 59ms/step - loss: 0.0157 - val_loss: 0.0176\
Epoch 7/40\
88/88 [==============================] - 5s 58ms/step - loss: 0.0140 - val_loss: 0.0178\
Epoch 8/40\
88/88 [==============================] - 5s 59ms/step - loss: 0.0122 - val_loss: 0.0181\
Epoch 9/40\
88/88 [==============================] - 5s 58ms/step - loss: 0.0092 - val_loss: 0.0178\
Fold 5: 0.017767789 loss, 0.7785039 auc\
[0.018220186, 0.017813373, 0.01827081, 0.018022282, 0.017767789] [0.76376987, 0.7740028, 0.7470142, 0.7679673, 0.7785039]\
\cf10 [I 2020-11-05 19:54:16,892]\cf2  Trial 4 finished with value: 0.01801888793706894 and parameters: \{'layers': 3, 'neurons': 1024\}. Best is trial 0 with value: 0.017449426278471947.\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=1 as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/model_selection/_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\
  warnings.warn(\
5\
Epoch 1/40\
88/88 [==============================] - 7s 85ms/step - loss: 0.3307 - val_loss: 0.0579\
Epoch 2/40\
88/88 [==============================] - 7s 78ms/step - loss: 0.0319 - val_loss: 0.0247\
Epoch 3/40\
88/88 [==============================] - 7s 78ms/step - loss: 0.0218 - val_loss: 0.0205\
Epoch 4/40\
88/88 [==============================] - 7s 76ms/step - loss: 0.0190 - val_loss: 0.0188\
Epoch 5/40\
88/88 [==============================] - 7s 79ms/step - loss: 0.0173 - val_loss: 0.0181\
Epoch 6/40\
88/88 [==============================] - 7s 75ms/step - loss: 0.0159 - val_loss: 0.0180\
Epoch 7/40\
88/88 [==============================] - 7s 79ms/step - loss: 0.0145 - val_loss: 0.0180\
Epoch 8/40\
88/88 [==============================] - 7s 78ms/step - loss: 0.0122 - val_loss: 0.0178\
Epoch 9/40\
88/88 [==============================] - 7s 78ms/step - loss: 0.0109 - val_loss: 0.0179\
Epoch 10/40\
88/88 [==============================] - 7s 77ms/step - loss: 0.0100 - val_loss: 0.0181\
Epoch 11/40\
88/88 [==============================] - 7s 76ms/step - loss: 0.0093 - val_loss: 0.0180\
Fold 1: 0.018033348 loss, 0.7675678 auc\
Epoch 1/40\
88/88 [==============================] - 8s 86ms/step - loss: 0.3332 - val_loss: 0.0531\
Epoch 2/40\
88/88 [==============================] - 7s 77ms/step - loss: 0.0321 - val_loss: 0.0250\
Epoch 3/40\
88/88 [==============================] - 7s 75ms/step - loss: 0.0220 - val_loss: 0.0206\
Epoch 4/40\
88/88 [==============================] - 7s 79ms/step - loss: 0.0192 - val_loss: 0.0189\
Epoch 5/40\
88/88 [==============================] - 7s 76ms/step - loss: 0.0176 - val_loss: 0.0178\
Epoch 6/40\
88/88 [==============================] - 7s 76ms/step - loss: 0.0162 - val_loss: 0.0178\
Epoch 7/40\
88/88 [==============================] - 7s 77ms/step - loss: 0.0150 - val_loss: 0.0182\
Epoch 8/40\
88/88 [==============================] - 7s 78ms/step - loss: 0.0128 - val_loss: 0.0176\
Epoch 9/40\
88/88 [==============================] - 7s 80ms/step - loss: 0.0115 - val_loss: 0.0177\
Epoch 10/40\
88/88 [==============================] - 7s 80ms/step - loss: 0.0105 - val_loss: 0.0179\
Epoch 11/40\
88/88 [==============================] - 7s 78ms/step - loss: 0.0097 - val_loss: 0.0178\
\cf10 [I 2020-11-05 19:57:13,305]\cf2  Trial 5 pruned. \
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=1 as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/model_selection/_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\
  warnings.warn(\
5\
Epoch 1/40\
88/88 [==============================] - 5s 60ms/step - loss: 0.3271 - val_loss: 0.0478\
Epoch 2/40\
88/88 [==============================] - 5s 52ms/step - loss: 0.0303 - val_loss: 0.0240\
Epoch 3/40\
88/88 [==============================] - 4s 48ms/step - loss: 0.0211 - val_loss: 0.0202\
Epoch 4/40\
88/88 [==============================] - 4s 50ms/step - loss: 0.0184 - val_loss: 0.0185\
Epoch 5/40\
88/88 [==============================] - 4s 51ms/step - loss: 0.0166 - val_loss: 0.0181\
Epoch 6/40\
88/88 [==============================] - 5s 52ms/step - loss: 0.0150 - val_loss: 0.0177\
Epoch 7/40\
88/88 [==============================] - 5s 51ms/step - loss: 0.0130 - val_loss: 0.0178\
Epoch 8/40\
88/88 [==============================] - 4s 50ms/step - loss: 0.0107 - val_loss: 0.0185\
Epoch 9/40\
88/88 [==============================] - 4s 51ms/step - loss: 0.0076 - val_loss: 0.0178\
Fold 1: 0.017840337 loss, 0.7781474 auc\
Epoch 1/40\
88/88 [==============================] - 5s 58ms/step - loss: 0.3253 - val_loss: 0.0435\
Epoch 2/40\
88/88 [==============================] - 4s 49ms/step - loss: 0.0304 - val_loss: 0.0236\
Epoch 3/40\
88/88 [==============================] - 4s 47ms/step - loss: 0.0213 - val_loss: 0.0204\
Epoch 4/40\
88/88 [==============================] - 4s 50ms/step - loss: 0.0186 - val_loss: 0.0188\
Epoch 5/40\
88/88 [==============================] - 4s 49ms/step - loss: 0.0169 - val_loss: 0.0180\
Epoch 6/40\
88/88 [==============================] - 4s 50ms/step - loss: 0.0152 - val_loss: 0.0177\
Epoch 7/40\
88/88 [==============================] - 5s 51ms/step - loss: 0.0134 - val_loss: 0.0182\
Epoch 8/40\
88/88 [==============================] - 4s 50ms/step - loss: 0.0114 - val_loss: 0.0180\
Epoch 9/40\
88/88 [==============================] - 4s 50ms/step - loss: 0.0081 - val_loss: 0.0177\
Epoch 10/40\
88/88 [==============================] - 4s 51ms/step - loss: 0.0070 - val_loss: 0.0177\
Epoch 11/40\
88/88 [==============================] - 4s 49ms/step - loss: 0.0064 - val_loss: 0.0177\
Epoch 12/40\
88/88 [==============================] - 4s 51ms/step - loss: 0.0064 - val_loss: 0.0177\
Epoch 13/40\
88/88 [==============================] - 4s 50ms/step - loss: 0.0063 - val_loss: 0.0177\
Fold 2: 0.017707814 loss, 0.7703699 auc\
Epoch 1/40\
88/88 [==============================] - 6s 64ms/step - loss: 0.3267 - val_loss: 0.0463\
Epoch 2/40\
88/88 [==============================] - 4s 51ms/step - loss: 0.0305 - val_loss: 0.0239\
Epoch 3/40\
88/88 [==============================] - 5s 51ms/step - loss: 0.0213 - val_loss: 0.0200\
Epoch 4/40\
88/88 [==============================] - 4s 50ms/step - loss: 0.0186 - val_loss: 0.0185\
Epoch 5/40\
88/88 [==============================] - 4s 49ms/step - loss: 0.0168 - val_loss: 0.0177\
Epoch 6/40\
88/88 [==============================] - 4s 51ms/step - loss: 0.0151 - val_loss: 0.0177\
Epoch 7/40\
88/88 [==============================] - 5s 52ms/step - loss: 0.0131 - val_loss: 0.0177\
Epoch 8/40\
88/88 [==============================] - 5s 53ms/step - loss: 0.0100 - val_loss: 0.0174\
Epoch 9/40\
88/88 [==============================] - 5s 52ms/step - loss: 0.0088 - val_loss: 0.0174\
Epoch 10/40\
88/88 [==============================] - 5s 52ms/step - loss: 0.0080 - val_loss: 0.0175\
Epoch 11/40\
88/88 [==============================] - 5s 52ms/step - loss: 0.0074 - val_loss: 0.0175\
Fold 3: 0.017457062 loss, 0.78596306 auc\
Epoch 1/40\
88/88 [==============================] - 5s 57ms/step - loss: 0.3285 - val_loss: 0.0464\
Epoch 2/40\
88/88 [==============================] - 4s 51ms/step - loss: 0.0304 - val_loss: 0.0234\
Epoch 3/40\
88/88 [==============================] - 4s 49ms/step - loss: 0.0211 - val_loss: 0.0207\
Epoch 4/40\
88/88 [==============================] - 4s 51ms/step - loss: 0.0184 - val_loss: 0.0188\
Epoch 5/40\
88/88 [==============================] - 5s 51ms/step - loss: 0.0166 - val_loss: 0.0180\
Epoch 6/40\
88/88 [==============================] - 4s 51ms/step - loss: 0.0149 - val_loss: 0.0178\
Epoch 7/40\
88/88 [==============================] - 4s 51ms/step - loss: 0.0127 - val_loss: 0.0181\
Epoch 8/40\
88/88 [==============================] - 4s 50ms/step - loss: 0.0104 - val_loss: 0.0184\
Epoch 9/40\
88/88 [==============================] - 4s 50ms/step - loss: 0.0074 - val_loss: 0.0180\
Fold 4: 0.01802913 loss, 0.7733922 auc\
Epoch 1/40\
88/88 [==============================] - 5s 57ms/step - loss: 0.3272 - val_loss: 0.0463\
Epoch 2/40\
88/88 [==============================] - 4s 50ms/step - loss: 0.0306 - val_loss: 0.0236\
Epoch 3/40\
88/88 [==============================] - 4s 51ms/step - loss: 0.0212 - val_loss: 0.0202\
Epoch 4/40\
88/88 [==============================] - 4s 50ms/step - loss: 0.0185 - val_loss: 0.0189\
Epoch 5/40\
88/88 [==============================] - 4s 51ms/step - loss: 0.0168 - val_loss: 0.0178\
Epoch 6/40\
88/88 [==============================] - 5s 53ms/step - loss: 0.0150 - val_loss: 0.0177\
Epoch 7/40\
88/88 [==============================] - 5s 52ms/step - loss: 0.0131 - val_loss: 0.0178\
Epoch 8/40\
88/88 [==============================] - 5s 51ms/step - loss: 0.0101 - val_loss: 0.0177\
Epoch 9/40\
88/88 [==============================] - 5s 51ms/step - loss: 0.0090 - val_loss: 0.0176\
Epoch 10/40\
88/88 [==============================] - 4s 50ms/step - loss: 0.0081 - val_loss: 0.0177\
Epoch 11/40\
88/88 [==============================] - 4s 50ms/step - loss: 0.0075 - val_loss: 0.0176\
Epoch 12/40\
88/88 [==============================] - 5s 51ms/step - loss: 0.0075 - val_loss: 0.0176\
Fold 5: 0.017642211 loss, 0.7765412 auc\
[0.017840337, 0.017707814, 0.017457062, 0.01802913, 0.017642211] [0.7781474, 0.7703699, 0.78596306, 0.7733922, 0.7765412]\
\cf10 [I 2020-11-05 20:01:57,277]\cf2  Trial 6 finished with value: 0.017735310643911362 and parameters: \{'layers': 2, 'neurons': 1152\}. Best is trial 0 with value: 0.017449426278471947.\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=1 as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/model_selection/_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\
  warnings.warn(\
5\
Epoch 1/40\
88/88 [==============================] - 13s 144ms/step - loss: 0.2987 - val_loss: 0.0367\
Epoch 2/40\
88/88 [==============================] - 11s 130ms/step - loss: 0.0261 - val_loss: 0.0223\
Epoch 3/40\
88/88 [==============================] - 11s 130ms/step - loss: 0.0198 - val_loss: 0.0197\
Epoch 4/40\
88/88 [==============================] - 11s 130ms/step - loss: 0.0177 - val_loss: 0.0181\
Epoch 5/40\
88/88 [==============================] - 11s 130ms/step - loss: 0.0160 - val_loss: 0.0177\
Epoch 6/40\
88/88 [==============================] - 12s 131ms/step - loss: 0.0143 - val_loss: 0.0182\
Epoch 7/40\
88/88 [==============================] - 12s 131ms/step - loss: 0.0122 - val_loss: 0.0181\
Epoch 8/40\
88/88 [==============================] - 12s 131ms/step - loss: 0.0087 - val_loss: 0.0177\
Fold 1: 0.017745856 loss, 0.7832615 auc\
Epoch 1/40\
88/88 [==============================] - 12s 138ms/step - loss: 0.2984 - val_loss: 0.0390\
Epoch 2/40\
88/88 [==============================] - 11s 130ms/step - loss: 0.0263 - val_loss: 0.0221\
Epoch 3/40\
88/88 [==============================] - 12s 131ms/step - loss: 0.0200 - val_loss: 0.0192\
Epoch 4/40\
88/88 [==============================] - 11s 130ms/step - loss: 0.0179 - val_loss: 0.0178\
Epoch 5/40\
88/88 [==============================] - 11s 129ms/step - loss: 0.0162 - val_loss: 0.0175\
Epoch 6/40\
88/88 [==============================] - 11s 131ms/step - loss: 0.0144 - val_loss: 0.0176\
Epoch 7/40\
88/88 [==============================] - 11s 129ms/step - loss: 0.0121 - val_loss: 0.0190\
Epoch 8/40\
88/88 [==============================] - 11s 130ms/step - loss: 0.0088 - val_loss: 0.0178\
Fold 2: 0.017789831 loss, 0.77476853 auc\
Epoch 1/40\
88/88 [==============================] - 13s 149ms/step - loss: 0.2982 - val_loss: 0.0410\
Epoch 2/40\
88/88 [==============================] - 13s 143ms/step - loss: 0.0262 - val_loss: 0.0220\
Epoch 3/40\
88/88 [==============================] - 12s 135ms/step - loss: 0.0200 - val_loss: 0.0193\
Epoch 4/40\
88/88 [==============================] - 12s 142ms/step - loss: 0.0180 - val_loss: 0.0180\
Epoch 5/40\
88/88 [==============================] - 13s 144ms/step - loss: 0.0163 - val_loss: 0.0176\
Epoch 6/40\
88/88 [==============================] - 12s 136ms/step - loss: 0.0147 - val_loss: 0.0173\
Epoch 7/40\
88/88 [==============================] - 13s 148ms/step - loss: 0.0125 - val_loss: 0.0181\
Epoch 8/40\
88/88 [==============================] - 14s 158ms/step - loss: 0.0100 - val_loss: 0.0186\
Epoch 9/40\
88/88 [==============================] - 18s 203ms/step - loss: 0.0067 - val_loss: 0.0181\
Fold 3: 0.018142676 loss, 0.7488004 auc\
Epoch 1/40\
88/88 [==============================] - 19s 213ms/step - loss: 0.2984 - val_loss: 0.0405\
Epoch 2/40\
88/88 [==============================] - 15s 168ms/step - loss: 0.0264 - val_loss: 0.0226\
Epoch 3/40\
88/88 [==============================] - 14s 160ms/step - loss: 0.0200 - val_loss: 0.0193\
Epoch 4/40\
88/88 [==============================] - 17s 191ms/step - loss: 0.0178 - val_loss: 0.0181\
Epoch 5/40\
88/88 [==============================] - 17s 197ms/step - loss: 0.0161 - val_loss: 0.0184\
Epoch 6/40\
88/88 [==============================] - 16s 185ms/step - loss: 0.0144 - val_loss: 0.0179\
Epoch 7/40\
88/88 [==============================] - 16s 186ms/step - loss: 0.0122 - val_loss: 0.0187\
Epoch 8/40\
88/88 [==============================] - 15s 172ms/step - loss: 0.0096 - val_loss: 0.0190\
Epoch 9/40\
88/88 [==============================] - 14s 156ms/step - loss: 0.0063 - val_loss: 0.0184\
Fold 4: 0.01839916 loss, 0.75146085 auc\
Epoch 1/40\
88/88 [==============================] - 16s 181ms/step - loss: 0.2981 - val_loss: 0.0384\
Epoch 2/40\
88/88 [==============================] - 15s 166ms/step - loss: 0.0264 - val_loss: 0.0226\
Epoch 3/40\
88/88 [==============================] - 15s 167ms/step - loss: 0.0199 - val_loss: 0.0190\
Epoch 4/40\
88/88 [==============================] - 14s 161ms/step - loss: 0.0177 - val_loss: 0.0181\
Epoch 5/40\
88/88 [==============================] - 13s 149ms/step - loss: 0.0162 - val_loss: 0.0175\
Epoch 6/40\
88/88 [==============================] - 15s 165ms/step - loss: 0.0144 - val_loss: 0.0178\
Epoch 7/40\
88/88 [==============================] - 15s 169ms/step - loss: 0.0124 - val_loss: 0.0179\
Epoch 8/40\
88/88 [==============================] - 15s 168ms/step - loss: 0.0091 - val_loss: 0.0177\
Fold 5: 0.017689163 loss, 0.7766327 auc\
[0.017745856, 0.017789831, 0.018142676, 0.01839916, 0.017689163] [0.7832615, 0.77476853, 0.7488004, 0.75146085, 0.7766327]\
\cf10 [I 2020-11-05 20:12:42,777]\cf2  Trial 7 finished with value: 0.017953337356448173 and parameters: \{'layers': 3, 'neurons': 1792\}. Best is trial 0 with value: 0.017449426278471947.\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=1 as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/model_selection/_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\
  warnings.warn(\
5\
Epoch 1/40\
88/88 [==============================] - 5s 53ms/step - loss: 0.3555 - val_loss: 0.0627\
Epoch 2/40\
88/88 [==============================] - 3s 40ms/step - loss: 0.0357 - val_loss: 0.0261\
Epoch 3/40\
88/88 [==============================] - 3s 39ms/step - loss: 0.0228 - val_loss: 0.0212\
Epoch 4/40\
88/88 [==============================] - 3s 36ms/step - loss: 0.0194 - val_loss: 0.0192\
Epoch 5/40\
88/88 [==============================] - 3s 37ms/step - loss: 0.0176 - val_loss: 0.0185\
Epoch 6/40\
88/88 [==============================] - 3s 37ms/step - loss: 0.0161 - val_loss: 0.0178\
Epoch 7/40\
88/88 [==============================] - 3s 38ms/step - loss: 0.0145 - val_loss: 0.0177\
Epoch 8/40\
88/88 [==============================] - 4s 40ms/step - loss: 0.0126 - val_loss: 0.0182\
Epoch 9/40\
88/88 [==============================] - 4s 40ms/step - loss: 0.0098 - val_loss: 0.0177\
Epoch 10/40\
88/88 [==============================] - 4s 40ms/step - loss: 0.0088 - val_loss: 0.0178\
Epoch 11/40\
88/88 [==============================] - 4s 44ms/step - loss: 0.0081 - val_loss: 0.0179\
Epoch 12/40\
88/88 [==============================] - 4s 41ms/step - loss: 0.0076 - val_loss: 0.0178\
Fold 1: 0.017835464 loss, 0.78288794 auc\
Epoch 1/40\
88/88 [==============================] - 5s 56ms/step - loss: 0.3563 - val_loss: 0.0540\
Epoch 2/40\
88/88 [==============================] - 3s 36ms/step - loss: 0.0361 - val_loss: 0.0260\
Epoch 3/40\
88/88 [==============================] - 3s 39ms/step - loss: 0.0231 - val_loss: 0.0215\
Epoch 4/40\
88/88 [==============================] - 3s 34ms/step - loss: 0.0196 - val_loss: 0.0197\
Epoch 5/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0178 - val_loss: 0.0183\
Epoch 6/40\
88/88 [==============================] - 3s 31ms/step - loss: 0.0162 - val_loss: 0.0180\
Epoch 7/40\
88/88 [==============================] - 4s 43ms/step - loss: 0.0145 - val_loss: 0.0176\
Epoch 8/40\
88/88 [==============================] - 3s 38ms/step - loss: 0.0126 - val_loss: 0.0180\
Epoch 9/40\
88/88 [==============================] - 4s 47ms/step - loss: 0.0105 - val_loss: 0.0180\
Epoch 10/40\
88/88 [==============================] - 4s 44ms/step - loss: 0.0078 - val_loss: 0.0178\
Fold 2: 0.01778286 loss, 0.7773488 auc\
Epoch 1/40\
88/88 [==============================] - 5s 58ms/step - loss: 0.3565 - val_loss: 0.0584\
Epoch 2/40\
88/88 [==============================] - 4s 45ms/step - loss: 0.0359 - val_loss: 0.0263\
Epoch 3/40\
88/88 [==============================] - 4s 45ms/step - loss: 0.0230 - val_loss: 0.0214\
Epoch 4/40\
88/88 [==============================] - 4s 46ms/step - loss: 0.0196 - val_loss: 0.0192\
Epoch 5/40\
88/88 [==============================] - 4s 45ms/step - loss: 0.0177 - val_loss: 0.0182\
Epoch 6/40\
88/88 [==============================] - 4s 48ms/step - loss: 0.0163 - val_loss: 0.0179\
Epoch 7/40\
88/88 [==============================] - 4s 46ms/step - loss: 0.0146 - val_loss: 0.0176\
Epoch 8/40\
88/88 [==============================] - 3s 34ms/step - loss: 0.0127 - val_loss: 0.0177\
Epoch 9/40\
88/88 [==============================] - 3s 40ms/step - loss: 0.0105 - val_loss: 0.0180\
Epoch 10/40\
88/88 [==============================] - 3s 29ms/step - loss: 0.0078 - val_loss: 0.0177\
Fold 3: 0.017738793 loss, 0.77898675 auc\
Epoch 1/40\
88/88 [==============================] - 5s 62ms/step - loss: 0.3562 - val_loss: 0.0632\
Epoch 2/40\
88/88 [==============================] - 4s 48ms/step - loss: 0.0362 - val_loss: 0.0261\
Epoch 3/40\
88/88 [==============================] - 4s 49ms/step - loss: 0.0230 - val_loss: 0.0218\
Epoch 4/40\
88/88 [==============================] - 4s 49ms/step - loss: 0.0196 - val_loss: 0.0193\
Epoch 5/40\
88/88 [==============================] - 4s 45ms/step - loss: 0.0176 - val_loss: 0.0185\
Epoch 6/40\
88/88 [==============================] - 4s 47ms/step - loss: 0.0160 - val_loss: 0.0181\
Epoch 7/40\
88/88 [==============================] - 4s 47ms/step - loss: 0.0144 - val_loss: 0.0179\
Epoch 8/40\
88/88 [==============================] - 4s 47ms/step - loss: 0.0124 - val_loss: 0.0181\
Epoch 9/40\
88/88 [==============================] - 4s 49ms/step - loss: 0.0102 - val_loss: 0.0183\
Epoch 10/40\
88/88 [==============================] - 4s 40ms/step - loss: 0.0075 - val_loss: 0.0180\
Fold 4: 0.01800692 loss, 0.7743373 auc\
Epoch 1/40\
88/88 [==============================] - 6s 69ms/step - loss: 0.3549 - val_loss: 0.0691\
Epoch 2/40\
88/88 [==============================] - 5s 58ms/step - loss: 0.0359 - val_loss: 0.0266\
Epoch 3/40\
88/88 [==============================] - 4s 50ms/step - loss: 0.0230 - val_loss: 0.0214\
Epoch 4/40\
88/88 [==============================] - 4s 47ms/step - loss: 0.0196 - val_loss: 0.0196\
Epoch 5/40\
88/88 [==============================] - 5s 53ms/step - loss: 0.0178 - val_loss: 0.0183\
Epoch 6/40\
88/88 [==============================] - 5s 57ms/step - loss: 0.0162 - val_loss: 0.0178\
Epoch 7/40\
88/88 [==============================] - 5s 54ms/step - loss: 0.0146 - val_loss: 0.0179\
Epoch 8/40\
88/88 [==============================] - 5s 52ms/step - loss: 0.0127 - val_loss: 0.0178\
Epoch 9/40\
88/88 [==============================] - 5s 52ms/step - loss: 0.0099 - val_loss: 0.0176\
Epoch 10/40\
88/88 [==============================] - 4s 49ms/step - loss: 0.0089 - val_loss: 0.0176\
Epoch 11/40\
88/88 [==============================] - 5s 54ms/step - loss: 0.0082 - val_loss: 0.0177\
Epoch 12/40\
88/88 [==============================] - 3s 39ms/step - loss: 0.0077 - val_loss: 0.0176\
Epoch 13/40\
88/88 [==============================] - 4s 46ms/step - loss: 0.0077 - val_loss: 0.0176\
Fold 5: 0.017636118 loss, 0.77623594 auc\
[0.017835464, 0.01778286, 0.017738793, 0.01800692, 0.017636118] [0.78288794, 0.7773488, 0.77898675, 0.7743373, 0.77623594]\
\cf10 [I 2020-11-05 20:17:15,998]\cf2  Trial 8 finished with value: 0.01780003122985363 and parameters: \{'layers': 2, 'neurons': 768\}. Best is trial 0 with value: 0.017449426278471947.\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=1 as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/model_selection/_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\
  warnings.warn(\
5\
Epoch 1/40\
88/88 [==============================] - 13s 144ms/step - loss: 0.2971 - val_loss: 0.0345\
Epoch 2/40\
88/88 [==============================] - 11s 131ms/step - loss: 0.0252 - val_loss: 0.0213\
Epoch 3/40\
88/88 [==============================] - 11s 123ms/step - loss: 0.0194 - val_loss: 0.0194\
Epoch 4/40\
88/88 [==============================] - 9s 106ms/step - loss: 0.0173 - val_loss: 0.0182\
Epoch 5/40\
88/88 [==============================] - 9s 97ms/step - loss: 0.0157 - val_loss: 0.0178\
Epoch 6/40\
88/88 [==============================] - 10s 112ms/step - loss: 0.0139 - val_loss: 0.0177\
Epoch 7/40\
88/88 [==============================] - 11s 119ms/step - loss: 0.0123 - val_loss: 0.0176\
Epoch 8/40\
88/88 [==============================] - 10s 113ms/step - loss: 0.0095 - val_loss: 0.0181\
Epoch 9/40\
88/88 [==============================] - 10s 119ms/step - loss: 0.0068 - val_loss: 0.0182\
Epoch 10/40\
88/88 [==============================] - 11s 120ms/step - loss: 0.0044 - val_loss: 0.0180\
\cf10 [I 2020-11-05 20:19:29,021]\cf2  Trial 9 pruned. \
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=1 as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/model_selection/_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\
  warnings.warn(\
5\
Epoch 1/40\
88/88 [==============================] - 6s 71ms/step - loss: 0.3068 - val_loss: 0.0375\
Epoch 2/40\
88/88 [==============================] - 7s 74ms/step - loss: 0.0266 - val_loss: 0.0219\
Epoch 3/40\
88/88 [==============================] - 6s 72ms/step - loss: 0.0198 - val_loss: 0.0195\
Epoch 4/40\
88/88 [==============================] - 8s 94ms/step - loss: 0.0175 - val_loss: 0.0184\
Epoch 5/40\
88/88 [==============================] - 8s 90ms/step - loss: 0.0158 - val_loss: 0.0180\
Epoch 6/40\
88/88 [==============================] - 8s 90ms/step - loss: 0.0142 - val_loss: 0.0175\
Epoch 7/40\
88/88 [==============================] - 8s 90ms/step - loss: 0.0122 - val_loss: 0.0176\
Epoch 8/40\
88/88 [==============================] - 8s 93ms/step - loss: 0.0100 - val_loss: 0.0182\
Epoch 9/40\
88/88 [==============================] - 7s 82ms/step - loss: 0.0068 - val_loss: 0.0176\
Fold 1: 0.017621443 loss, 0.7834736 auc\
Epoch 1/40\
88/88 [==============================] - 9s 100ms/step - loss: 0.3062 - val_loss: 0.0409\
Epoch 2/40\
88/88 [==============================] - 8s 91ms/step - loss: 0.0268 - val_loss: 0.0220\
Epoch 3/40\
88/88 [==============================] - 8s 87ms/step - loss: 0.0200 - val_loss: 0.0196\
Epoch 4/40\
88/88 [==============================] - 7s 85ms/step - loss: 0.0177 - val_loss: 0.0184\
Epoch 5/40\
88/88 [==============================] - 7s 83ms/step - loss: 0.0161 - val_loss: 0.0180\
Epoch 6/40\
88/88 [==============================] - 7s 84ms/step - loss: 0.0144 - val_loss: 0.0175\
Epoch 7/40\
88/88 [==============================] - 8s 86ms/step - loss: 0.0127 - val_loss: 0.0176\
Epoch 8/40\
88/88 [==============================] - 8s 87ms/step - loss: 0.0124 - val_loss: 0.0176\
Epoch 9/40\
88/88 [==============================] - 8s 86ms/step - loss: 0.0088 - val_loss: 0.0172\
Epoch 10/40\
88/88 [==============================] - 7s 76ms/step - loss: 0.0079 - val_loss: 0.0172\
Epoch 11/40\
88/88 [==============================] - 5s 61ms/step - loss: 0.0072 - val_loss: 0.0172\
Epoch 12/40\
88/88 [==============================] - 7s 77ms/step - loss: 0.0067 - val_loss: 0.0173\
Fold 2: 0.01725882 loss, 0.77981395 auc\
Epoch 1/40\
88/88 [==============================] - 9s 100ms/step - loss: 0.3051 - val_loss: 0.0381\
Epoch 2/40\
88/88 [==============================] - 8s 85ms/step - loss: 0.0265 - val_loss: 0.0218\
Epoch 3/40\
88/88 [==============================] - 8s 86ms/step - loss: 0.0200 - val_loss: 0.0193\
Epoch 4/40\
88/88 [==============================] - 8s 89ms/step - loss: 0.0178 - val_loss: 0.0183\
Epoch 5/40\
88/88 [==============================] - 8s 96ms/step - loss: 0.0162 - val_loss: 0.0176\
Epoch 6/40\
88/88 [==============================] - 8s 91ms/step - loss: 0.0148 - val_loss: 0.0175\
Epoch 7/40\
88/88 [==============================] - 8s 86ms/step - loss: 0.0126 - val_loss: 0.0174\
Epoch 8/40\
88/88 [==============================] - 6s 67ms/step - loss: 0.0111 - val_loss: 0.0174\
Epoch 9/40\
88/88 [==============================] - 6s 73ms/step - loss: 0.0088 - val_loss: 0.0277\
Epoch 10/40\
88/88 [==============================] - 7s 75ms/step - loss: 0.0080 - val_loss: 0.0177\
Epoch 11/40\
88/88 [==============================] - 7s 79ms/step - loss: 0.0062 - val_loss: 0.0176\
Fold 3: 0.017581642 loss, 0.7636749 auc\
Epoch 1/40\
88/88 [==============================] - 8s 95ms/step - loss: 0.3060 - val_loss: 0.0370\
Epoch 2/40\
88/88 [==============================] - 8s 87ms/step - loss: 0.0267 - val_loss: 0.0220\
Epoch 3/40\
88/88 [==============================] - 8s 86ms/step - loss: 0.0199 - val_loss: 0.0199\
Epoch 4/40\
88/88 [==============================] - 8s 86ms/step - loss: 0.0176 - val_loss: 0.0182\
Epoch 5/40\
88/88 [==============================] - 8s 85ms/step - loss: 0.0158 - val_loss: 0.0180\
Epoch 6/40\
88/88 [==============================] - 7s 77ms/step - loss: 0.0143 - val_loss: 0.0176\
Epoch 7/40\
88/88 [==============================] - 6s 71ms/step - loss: 0.0123 - val_loss: 0.0183\
Epoch 8/40\
88/88 [==============================] - 5s 62ms/step - loss: 0.0102 - val_loss: 0.0180\
Epoch 9/40\
88/88 [==============================] - 7s 79ms/step - loss: 0.0069 - val_loss: 0.0176\
Epoch 10/40\
88/88 [==============================] - 8s 89ms/step - loss: 0.0060 - val_loss: 0.0176\
Epoch 11/40\
88/88 [==============================] - 7s 78ms/step - loss: 0.0054 - val_loss: 0.0176\
Epoch 12/40\
88/88 [==============================] - 7s 83ms/step - loss: 0.0054 - val_loss: 0.0177\
Fold 4: 0.01766676 loss, 0.7673252 auc\
Epoch 1/40\
88/88 [==============================] - 9s 101ms/step - loss: 0.3040 - val_loss: 0.0365\
Epoch 2/40\
88/88 [==============================] - 8s 87ms/step - loss: 0.0267 - val_loss: 0.0223\
Epoch 3/40\
88/88 [==============================] - 8s 91ms/step - loss: 0.0200 - val_loss: 0.0195\
Epoch 4/40\
88/88 [==============================] - 7s 80ms/step - loss: 0.0177 - val_loss: 0.0181\
Epoch 5/40\
88/88 [==============================] - 7s 76ms/step - loss: 0.0161 - val_loss: 0.0177\
Epoch 6/40\
88/88 [==============================] - 6s 63ms/step - loss: 0.0144 - val_loss: 0.0177\
Epoch 7/40\
88/88 [==============================] - 6s 65ms/step - loss: 0.0128 - val_loss: 0.0175\
Epoch 8/40\
88/88 [==============================] - 6s 65ms/step - loss: 0.0108 - val_loss: 0.0176\
Epoch 9/40\
88/88 [==============================] - 5s 61ms/step - loss: 0.0079 - val_loss: 0.0180\
Epoch 10/40\
88/88 [==============================] - 5s 59ms/step - loss: 0.0051 - val_loss: 0.0176\
Fold 5: 0.01755797 loss, 0.76269305 auc\
[0.017621443, 0.01725882, 0.017581642, 0.01766676, 0.01755797] [0.7834736, 0.77981395, 0.7636749, 0.7673252, 0.76269305]\
\cf10 [I 2020-11-05 20:27:04,749]\cf2  Trial 10 finished with value: 0.017537326738238334 and parameters: \{'layers': 1, 'neurons': 1664\}. Best is trial 0 with value: 0.017449426278471947.\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=1 as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/model_selection/_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\
  warnings.warn(\
5\
Epoch 1/40\
88/88 [==============================] - 6s 68ms/step - loss: 0.3069 - val_loss: 0.0358\
Epoch 2/40\
88/88 [==============================] - 6s 63ms/step - loss: 0.0266 - val_loss: 0.0220\
Epoch 3/40\
88/88 [==============================] - 6s 63ms/step - loss: 0.0198 - val_loss: 0.0196\
Epoch 4/40\
88/88 [==============================] - 6s 63ms/step - loss: 0.0176 - val_loss: 0.0184\
Epoch 5/40\
88/88 [==============================] - 5s 62ms/step - loss: 0.0160 - val_loss: 0.0179\
Epoch 6/40\
88/88 [==============================] - 5s 60ms/step - loss: 0.0146 - val_loss: 0.0179\
Epoch 7/40\
88/88 [==============================] - 5s 60ms/step - loss: 0.0127 - val_loss: 0.0176\
Epoch 8/40\
88/88 [==============================] - 5s 60ms/step - loss: 0.0104 - val_loss: 0.0179\
Epoch 9/40\
88/88 [==============================] - 6s 65ms/step - loss: 0.0095 - val_loss: 0.0181\
Epoch 10/40\
88/88 [==============================] - 6s 65ms/step - loss: 0.0058 - val_loss: 0.0177\
Fold 1: 0.017710865 loss, 0.7706137 auc\
Epoch 1/40\
88/88 [==============================] - 6s 67ms/step - loss: 0.3047 - val_loss: 0.0380\
Epoch 2/40\
88/88 [==============================] - 5s 59ms/step - loss: 0.0268 - val_loss: 0.0221\
Epoch 3/40\
88/88 [==============================] - 5s 62ms/step - loss: 0.0201 - val_loss: 0.0195\
Epoch 4/40\
88/88 [==============================] - 5s 61ms/step - loss: 0.0178 - val_loss: 0.0185\
Epoch 5/40\
88/88 [==============================] - 5s 60ms/step - loss: 0.0161 - val_loss: 0.0179\
Epoch 6/40\
88/88 [==============================] - 5s 61ms/step - loss: 0.0145 - val_loss: 0.0174\
Epoch 7/40\
88/88 [==============================] - 5s 60ms/step - loss: 0.0126 - val_loss: 0.0175\
Epoch 8/40\
88/88 [==============================] - 6s 63ms/step - loss: 0.0142 - val_loss: 0.0178\
Epoch 9/40\
88/88 [==============================] - 6s 63ms/step - loss: 0.0104 - val_loss: 0.0171\
Epoch 10/40\
88/88 [==============================] - 5s 61ms/step - loss: 0.0094 - val_loss: 0.0171\
Epoch 11/40\
88/88 [==============================] - 6s 64ms/step - loss: 0.0087 - val_loss: 0.0172\
Epoch 12/40\
88/88 [==============================] - 6s 63ms/step - loss: 0.0082 - val_loss: 0.0173\
Fold 2: 0.017250597 loss, 0.78749144 auc\
Epoch 1/40\
88/88 [==============================] - 6s 67ms/step - loss: 0.3069 - val_loss: 0.0380\
Epoch 2/40\
88/88 [==============================] - 6s 63ms/step - loss: 0.0268 - val_loss: 0.0219\
Epoch 3/40\
88/88 [==============================] - 5s 62ms/step - loss: 0.0201 - val_loss: 0.0194\
Epoch 4/40\
88/88 [==============================] - 5s 61ms/step - loss: 0.0178 - val_loss: 0.0183\
Epoch 5/40\
88/88 [==============================] - 6s 63ms/step - loss: 0.0162 - val_loss: 0.0176\
Epoch 6/40\
88/88 [==============================] - 6s 64ms/step - loss: 0.0149 - val_loss: 0.0175\
Epoch 7/40\
88/88 [==============================] - 5s 61ms/step - loss: 0.0127 - val_loss: 0.0175\
Epoch 8/40\
88/88 [==============================] - 5s 58ms/step - loss: 0.0107 - val_loss: 0.0174\
Epoch 9/40\
88/88 [==============================] - 5s 58ms/step - loss: 0.0081 - val_loss: 0.0185\
Epoch 10/40\
88/88 [==============================] - 5s 61ms/step - loss: 0.0064 - val_loss: 0.0179\
Epoch 11/40\
88/88 [==============================] - 5s 61ms/step - loss: 0.0040 - val_loss: 0.0177\
Fold 3: 0.017693318 loss, 0.7538887 auc\
Epoch 1/40\
88/88 [==============================] - 6s 66ms/step - loss: 0.3062 - val_loss: 0.0366\
Epoch 2/40\
88/88 [==============================] - 5s 60ms/step - loss: 0.0266 - val_loss: 0.0218\
Epoch 3/40\
88/88 [==============================] - 5s 58ms/step - loss: 0.0199 - val_loss: 0.0195\
Epoch 4/40\
88/88 [==============================] - 5s 61ms/step - loss: 0.0177 - val_loss: 0.0183\
Epoch 5/40\
88/88 [==============================] - 5s 60ms/step - loss: 0.0160 - val_loss: 0.0180\
Epoch 6/40\
88/88 [==============================] - 5s 61ms/step - loss: 0.0144 - val_loss: 0.0177\
Epoch 7/40\
88/88 [==============================] - 5s 58ms/step - loss: 0.0127 - val_loss: 0.0177\
Epoch 8/40\
88/88 [==============================] - 5s 58ms/step - loss: 0.0099 - val_loss: 0.0179\
Epoch 9/40\
88/88 [==============================] - 5s 59ms/step - loss: 0.0069 - val_loss: 0.0175\
Epoch 10/40\
88/88 [==============================] - 5s 62ms/step - loss: 0.0059 - val_loss: 0.0175\
Epoch 11/40\
88/88 [==============================] - 6s 63ms/step - loss: 0.0052 - val_loss: 0.0176\
Epoch 12/40\
88/88 [==============================] - 5s 61ms/step - loss: 0.0048 - val_loss: 0.0176\
Fold 4: 0.017640091 loss, 0.76582104 auc\
Epoch 1/40\
88/88 [==============================] - 6s 71ms/step - loss: 0.3055 - val_loss: 0.0375\
Epoch 2/40\
88/88 [==============================] - 5s 61ms/step - loss: 0.0267 - val_loss: 0.0221\
Epoch 3/40\
88/88 [==============================] - 5s 60ms/step - loss: 0.0199 - val_loss: 0.0195\
Epoch 4/40\
88/88 [==============================] - 6s 63ms/step - loss: 0.0177 - val_loss: 0.0182\
Epoch 5/40\
88/88 [==============================] - 6s 63ms/step - loss: 0.0160 - val_loss: 0.0176\
Epoch 6/40\
88/88 [==============================] - 5s 62ms/step - loss: 0.0143 - val_loss: 0.0175\
Epoch 7/40\
88/88 [==============================] - 5s 61ms/step - loss: 0.0127 - val_loss: 0.0174\
Epoch 8/40\
88/88 [==============================] - 6s 63ms/step - loss: 0.0101 - val_loss: 0.0177\
Epoch 9/40\
88/88 [==============================] - 6s 67ms/step - loss: 0.0069 - val_loss: 0.0173\
Epoch 10/40\
88/88 [==============================] - 5s 60ms/step - loss: 0.0059 - val_loss: 0.0174\
Epoch 11/40\
88/88 [==============================] - 5s 61ms/step - loss: 0.0053 - val_loss: 0.0175\
Epoch 12/40\
88/88 [==============================] - 6s 63ms/step - loss: 0.0048 - val_loss: 0.0175\
Fold 5: 0.017493464 loss, 0.7639091 auc\
[0.017710865, 0.017250597, 0.017693318, 0.017640091, 0.017493464] [0.7706137, 0.78749144, 0.7538887, 0.76582104, 0.7639091]\
\cf10 [I 2020-11-05 20:32:57,787]\cf2  Trial 11 finished with value: 0.017557667195796968 and parameters: \{'layers': 1, 'neurons': 1664\}. Best is trial 0 with value: 0.017449426278471947.\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py:67: FutureWarning: Pass shuffle=False, random_state=1 as keyword args. From version 0.25 passing these as positional arguments will result in an error\
  warnings.warn("Pass \{\} as keyword args. From version 0.25 "\
/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/model_selection/_split.py:293: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\
  warnings.warn(\
5\
Epoch 1/40\
88/88 [==============================] - 6s 69ms/step - loss: 0.3114 - val_loss: 0.0381\
Epoch 2/40\
88/88 [==============================] - 5s 56ms/step - loss: 0.0270 - val_loss: 0.0222\
Epoch 3/40\
88/88 [==============================] - 5s 58ms/step - loss: 0.0199 - val_loss: 0.0197\
Epoch 4/40\
88/88 [==============================] - 5s 56ms/step - loss: 0.0177 - val_loss: 0.0183\
Epoch 5/40\
88/88 [==============================] - 5s 57ms/step - loss: 0.0159 - val_loss: 0.0178\
Epoch 6/40\
88/88 [==============================] - 5s 56ms/step - loss: 0.0144 - val_loss: 0.0177\
Epoch 7/40\
88/88 [==============================] - 5s 58ms/step - loss: 0.0125 - val_loss: 0.0176\
Epoch 8/40\
88/88 [==============================] - 5s 60ms/step - loss: 0.0101 - val_loss: 0.0180\
Epoch 9/40\
88/88 [==============================] - 5s 57ms/step - loss: 0.0071 - val_loss: 0.0175\
Epoch 10/40\
88/88 [==============================] - 5s 58ms/step - loss: 0.0061 - val_loss: 0.0176\
Epoch 11/40\
88/88 [==============================] - 5s 61ms/step - loss: 0.0055 - val_loss: 0.0177\
Epoch 12/40\
88/88 [==============================] - 5s 59ms/step - loss: 0.0051 - val_loss: 0.0177\
Fold 1: 0.017678905 loss, 0.7650315 auc\
Epoch 1/40\
88/88 [==============================] - 6s 67ms/step - loss: 0.3111 - val_loss: 0.0384\
Epoch 2/40\
88/88 [==============================] - 5s 58ms/step - loss: 0.0275 - val_loss: 0.0223\
Epoch 3/40\
88/88 [==============================] - 5s 60ms/step - loss: 0.0202 - val_loss: 0.0197\
Epoch 4/40\
88/88 [==============================] - 5s 57ms/step - loss: 0.0178 - val_loss: 0.0186\
Epoch 5/40\
88/88 [==============================] - 5s 58ms/step - loss: 0.0161 - val_loss: 0.0177\
Epoch 6/40\
88/88 [==============================] - 5s 57ms/step - loss: 0.0145 - val_loss: 0.0175\
Epoch 7/40\
88/88 [==============================] - 5s 57ms/step - loss: 0.0126 - val_loss: 0.0177\
Epoch 8/40\
88/88 [==============================] - 5s 58ms/step - loss: 0.0115 - val_loss: 0.0175\
Epoch 9/40\
88/88 [==============================] - 5s 59ms/step - loss: 0.0080 - val_loss: 0.0172\
Epoch 10/40\
88/88 [==============================] - 5s 55ms/step - loss: 0.0071 - val_loss: 0.0173\
Epoch 11/40\
88/88 [==============================] - 5s 55ms/step - loss: 0.0065 - val_loss: 0.0173\
Epoch 12/40\
88/88 [==============================] - 5s 56ms/step - loss: 0.0060 - val_loss: 0.0174\
Fold 2: 0.017359588 loss, 0.778738 auc\
Epoch 1/40\
88/88 [==============================] - 6s 66ms/step - loss: 0.3111 - val_loss: 0.0381\
Epoch 2/40\
88/88 [==============================] - 5s 57ms/step - loss: 0.0276 - val_loss: 0.0226\
Epoch 3/40\
88/88 [==============================] - 5s 56ms/step - loss: 0.0203 - val_loss: 0.0195\
Epoch 4/40\
88/88 [==============================] - 5s 56ms/step - loss: 0.0179 - val_loss: 0.0183\
Epoch 5/40\
88/88 [==============================] - 5s 59ms/step - loss: 0.0163 - val_loss: 0.0177\
Epoch 6/40\
88/88 [==============================] - 5s 56ms/step - loss: 0.0152 - val_loss: 0.0173\
Epoch 7/40\
88/88 [==============================] - 5s 55ms/step - loss: 0.0133 - val_loss: 0.0187\
Epoch 8/40\
88/88 [==============================] - 5s 55ms/step - loss: 0.0112 - val_loss: 0.0175\
Epoch 9/40\
88/88 [==============================] - 5s 61ms/step - loss: 0.0079 - val_loss: 0.0171\
Epoch 10/40\
88/88 [==============================] - 5s 58ms/step - loss: 0.0070 - val_loss: 0.0171\
Epoch 11/40\
88/88 [==============================] - 5s 59ms/step - loss: 0.0064 - val_loss: 0.0172\
Epoch 12/40\
88/88 [==============================] - 5s 61ms/step - loss: 0.0060 - val_loss: 0.0172\
Fold 3: 0.017244529 loss, 0.77868026 auc\
Epoch 1/40\
40/88 [============>.................] - ETA: 2s - loss: 0.5659^CTraceback (most recent call last):\
  File "tuning.py", line 67, in <module>\
    param_tuning()\
  File "tuning.py", line 62, in param_tuning\
    study.optimize(tuning_objective, n_trials=50)\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/study.py", line 338, in optimize\
    self._optimize_sequential(\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/study.py", line 747, in _optimize_sequential\
    self._run_trial_and_callbacks(func, catch, callbacks, gc_after_trial)\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/study.py", line 776, in _run_trial_and_callbacks\
    trial = self._run_trial(func, catch, gc_after_trial)\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/optuna/study.py", line 799, in _run_trial\
    result = func(trial)\
  File "tuning.py", line 47, in tuning_objective\
    myModel.run_training(train_x, train_y, test_x, test_y)\
  File "../models/arch_base.py", line 47, in run_training\
    history = self.model.fit(\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py", line 108, in _method_wrapper\
    return method(self, *args, **kwargs)\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py", line 1098, in fit\
    tmp_logs = train_function(iterator)\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py", line 780, in __call__\
    result = self._call(*args, **kwds)\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py", line 807, in _call\
    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 2829, in __call__\
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 1843, in _filtered_call\
    return self._call_flat(\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 1923, in _call_flat\
    return self._build_call_outputs(self._inference_function.call(\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 545, in call\
    outputs = execute.execute(\
  File "/Users/baconbaker/anaconda3/envs/ml/lib/python3.8/site-packages/tensorflow/python/eager/execute.py", line 59, in quick_execute\
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\
KeyboardInterrupt\
\
(ml) baconbaker@MacBook-Pro-2 experimentation % \
}